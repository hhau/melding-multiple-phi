---
title: "Multiple models, multiple shared quantities"
author: "Andrew Manderson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontfamily: tgpagella
fontsize: 10pt
papersize: a4
geometry: margin=2.25cm
bibliography: bibliography/multi-phi-bib.bib
csl: bibliography/american-statistical-association.csl
output: 
  pdf_document:
    includes:
      in_header:
        tex-input/pre.tex
    fig_caption: true
    number_sections: true
    keep_tex: true
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, out.width = "95%", fig.align = "center", auto_pdf = TRUE)
```

# Introduction

- Considering multiple, diverse data presents a substantial challenge for statisticians.
    - If the data differ in structure, contain different types of measurements, or target related but non-identical populations of interest, then specifying a coherent joint model for each source of data presents a substantial challenge.
    - A more feasible approach may be to specify, and validate, individual submodels for each source of data, then combine 

- Computation for multiple models is also challenging
    - The computational effort required to fit a complex, multi-response model may prove prohibitive.
    - If instead we consider the submodel as the minimum computational unit, we may be able to parallelise certain aspects of the computation, and uses less expensive computational techniques for some submodels.
    - The melding framework also allows us to reuse the submodel computation when targeting the bigger joint model.
    - In a sense we would like to do Sequential importance sampling / refine our estimates, without needing to re-evaluate the submodels.
    - Also like to better understand each submodel, and each submodels contribution to the join model.

- Markov melding introduction
    - however, it considers $\Nm$ submodels that have the _same_ quantity $\phi$ in common. 
    - In several applied settings, we will have a number of submodels that share quantities in a pair wise manner.
    - For example we may have 3 submodels $\pd_{\modelindex}, \modelindex = 1 \ldots, 3$. In the simplest case, our interest is where submodel 1 and 2 share a common quantity $\phi_{1 \cap 2}$, and submodel 2 and 3 share a common quantity $\phi_{2 \cap 3}$, see Figure \ref{fig:intro-dag}.

    \input{tex-input/owls-example/0001-intro-dag.tex}

- Applied researches often collect multiple disparate data sets, and wish to combine them after the fact
    - Describe why and what they do
    - We are presenting a more formal, general form of the things that are done already

- Other advantages to modularised inference
    - understand submodels
    - use multiple software packages, which means we can reuse other computation
    - reuse posterior samples

# Example introduction

- We will now outline some applied analyses that will hopefully convince you that this is way of thinking that is already being employed, and practitioners are finding useful.

## An Integrated population model for little owls

- data background
    - Three forms of data are collected about a population of little owls (_Athene noctua_).
    - Capture recapture, count (census), and fecundity (nest-record) data are collected.
    - Researchers are interested in estimating several properties of the population including the immigration rate, survival rate, and the reproduction rate.
    - No single source of data is sufficient to estimate these quantities, they must be combined.
    - Standard models exist in the ecology literature for these kinds of data; it would be prudent to reuse them.
    - Ecologists multiply these individual models together to form an _integrated population model_.
- DAG of simplified case

## other-tba

# Model specification

- Let us now formalise the kinds of submodels, and their relations, that are of interest.
- First we introduce some notation:
    - notation goes here
- For pedagogical purposes we choose to study the $\Nm = 3$ case
- The appendix contains the more general $\Nm$-model case.

## Melded model

The melded model for the $\Nm = 3$ case is
\input{tex-input/multiple-phi/0010-melded-model.tex}
Note that we cannot necessarily factorise $\pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3}) = \pd_{2}(\phi_{1 \cap 2})\pd_{2}(\phi_{2 \cap 3})$, as this falsely assumes independence.
The corresponding melded posteioris proportional to the melded model defined in Equation \eqref{eqn:melded-model}
\input{tex-input/multi-stage-sampler/0010-melded-posterior.tex}

## Pooled prior

# Posterior estimation

We now present two multi-stage MCMC methods for generating samples from the melded posterior.
By employing a multi-stage strategy we can avoid evaluating all models simultaneously.
The first sampler operates sequentially, accruing and refining samples whilst considering one submodel at a time.
The second parallelises parts of the sampling process, and has the potential to produce a sample, usable for inference, from the melded posterior in less time than the sequential method.

## Sequential sampler

This sampler is sequential; it first targets terms from  submodel $\pd_{1}$, then the target is expanded to also include $\pd_{2}$ terms, finally to include the $\pd_{3}$ terms and $\pd_\text{pool}$.
An overview of this target broadening process is displayed in Figure \ref{fig:seq-sampler-dag}
\input{tex-input/multi-stage-sampler/0001-seq-sampler-dag.tex}

#### Stage one

Mathematically, stage one of the sequential sampler targets
\input{tex-input/multi-stage-sampler/0020-stage-one-target.tex}
using a generic proposal kernel $\q$ for both $\phi_{1 \cap 2}$ and $\psi_{1}$. 
The corresponding acceptance probability for an update from $(\phi_{1 \cap 2}, \psi_{1})$ to $(\phi_{1 \cap 2}^{*}, \psi_{1}^{*})$ is
\input{tex-input/multi-stage-sampler/0021-stage-one-acceptance-probability.tex}

#### Stage two

The stage two target augments the stage one target by including the second submodel $\pd_{2}$ and corresponding prior marginal distribution,
\input{tex-input/multi-stage-sampler/0030-stage-two-target.tex}
A Metropolis-within-Gibbs strategy is employed, where the stage one samples are used as a proposal for $\phi_{1 \cap 2}$, whilst a generic proposal kernel is used for $\psi_{2}$ and $\phi_{2 \cap 3}$.
Hence, $\phi_{1 \cap 2}^{*}$ and $(\phi_{2 \cap 3}^{*}, \psi_{2}^{*})$ are distributed according to
\input{tex-input/multi-stage-sampler/0031-stage-two-gibbs-updates.tex}
The acceptance probability for this proposal strategy is
\input{tex-input/multi-stage-sampler/0032-stage-two-acceptance-probabilities.tex}
Due to our judicious choice of proposal distribution all terms pertaining to $\pd_{1}$ have cancelled in Equation \eqref{eqn:stage-two-acceptance-probabilities-one}, and are constant -- hence cancel -- in Equation \eqref{eqn:stage-two-acceptance-probabilities-two}.

#### Stage three

In stage three we target the full melded posterior
\input{tex-input/multi-stage-sampler/0044-stage-three-target.tex}
Compared to stage two the target now includes terms from the third submodel, and the pooled prior.
Again, we employ a Metropolis-within-Gibbs sampler, with proposals drawn such that
\input{tex-input/multi-stage-sampler/0045-stage-three-gibbs-updates.tex} 
which leads to acceptance probabilities of
\input{tex-input/multi-stage-sampler/0046-stage-three-acceptance-probabilities.tex}
The informed choice of proposal distribution for ($\phi_{1 \cap 2}, \phi_{2 \cap 3}, \psi_{1}, \psi_{2}$) has allowed us to target the full melded posterior without needing to evaluate all submodels simultaneously.  


## Parallel sampler

- Here, we build a MH-within-gibbs sampler so that we can sample models 1 and 3 in parallel.

- Overview in Figure \ref{fig:parallel-dag}.

\input{tex-input/dc-sampler/0001-parallel-dag.tex}

# Examples

## Owls

## other-tba

# Discussion

## Advantages

- better understand submodels, and components thereof
    - priors / prior predictive distributions / posterior predictive distributions / posterior pathologies therein
- mostly re use existing implementations
- refines other components of submodels, where as using parametric approximation to subposterior as a prior in latter models does not.

## Disadvantages

- Submodels may conflict with each other
    - Though we wouldn't have detected this if we could directly evaluate the joint.
- submodels may be practically unidentifiable
- Not all intermediary distributions/outputs are meaningful, depends on what we choose to sample at what stage. 

## Differentiate from applying melding twice

- Only the same if phis are independent in p2 
    - > see appendix

# Conclusion 




<!-- -------------------- END OF MAIN BODY OF DOCUMENT -------------------- -->
\newpage

<!-- The {-} tag here suppresses the section numbering. -->
# Bibliography {-}

<!-- This makes pandoc-citeproc put the references before the end of document. -->
<div id="refs"></div>

\newpage

<!-- Now switch to alphabetical numbering for the appendix, and reset the counter. -->
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}

# Appendix

---
title: "Summary / questions"
author: "Andrew Manderson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontfamily: tgpagella
fontsize: 10pt
papersize: a4
geometry: margin=2.25cm
bibliography: ../bibliography/multi-phi-bib.bib
csl: ../bibliography/journal-of-the-royal-statistical-society.csl
output: 
  html_document:
    code_folding: hide
---

```{r setup, include = FALSE, cache = FALSE, message = FALSE, warning = FALSE, comment = NA}
options(width = 9999)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```


# Comparison to melding hybrid estimates

- Compared chained melding to the point estimate approximation and the hybrids
- Seems like the bulk of the uncertainty comes from the uncertain event time? But there is still more uncertainty when the event times are fixed and the other things vary? I am missing something here.

```{r prop, fig.cap = "Posterior of $\\psi_{2}$ under melding and propagating the subposterior medians"}
knitr::include_graphics("../plots/surv-example/psi-2-method-comparison-no-time-dep.png")
```

# Making the longitudinal model linear in time again

- Whilst investigating the above I realised I could make the 'longitudinal model' actually longitudinal model again.
- Which is to say, the third submodel is now 

\begin{equation}
\begin{gathered}
  x_{i, k} = \eta_{0, i} + \eta_{1, i} \tau_{i, k} + \zeta_{i, k}  \\
  \eta_{0, i} \sim \text{N}(\mu_{\eta, 0}, \sigma^{2}_{\eta, 0}), \,\,
  \eta_{0, i} \sim \text{N}(\mu_{\eta, 1}, \sigma^{2}_{\eta, 1}), \,\,
  \zeta_{i, k} \sim \text{N}(0, \sigma^{2}_{x}), \\
  \mu_{\eta, 0} \sim \text{N}(1, 1^2), \,\,
  \mu_{\eta, 1} \sim \text{N}(1, 1^2), \\
  \sigma^{2}_{\eta, 0} \sim \text{LogNormal}(0, 1^2), \,\,
  \sigma^{2}_{\eta, 1} \sim \text{LogNormal}(0, 1^2), \,\,
  \sigma^{2}_{x} \sim \text{LogNormal}(0, 1^2),
\end{gathered}
\label{eqn:submodel-three-model}
\end{equation}

- The combination of censoring and parametrising the integrand stabilises the computation.
- The down side is that the mixing is terrible at the time/iteration increased such that stage two now takes 1.75 hours, up from 10 mins. (The whole process now takes about 6 hours because we have to fit stage 2 four times.)
- There is poor mixing of the event times in stage two

```{r stage_two_mixing_phi_12}
knitr::include_graphics("../plots/surv-example/stage-two-phi-12-diags.png")
```

- which leads to very poor mixing in of $\psi_{2}$

```{r stage_two_mixing_psi_2}
knitr::include_graphics("../plots/surv-example/stage-two-psi-2-diags.png")
```

- I could probably run this over the weekend / put on long jobs queue on the HPC?
- I think you also mentioned that mixing might be helped if we update the event times one at a time? 
    - I would need to cut out a lot of redundant computation for this to work.

# coherency maths / theorem

I think the general thing I want to say is

<style>
  .theorem {
  display: block;
  font-style: italic;
  }
  .theorem:before {
  content: "Theorem. ";
  font-weight: bold;
  font-style: normal;
  }
  .theorem[text]:before {
  content: "Theorem (" attr(text) ") ";
  }
</style>

<div class="theorem" text='Prior independence'>
There exists a choice of non-negative pooling weights $\boldsymbol{\lambda}$ and pooling functions $g_{\text{pool}}, g^{12}, g^{(12)3}, \ldots$, such that the joint density produced by sequentially melding $M$ submodels is identical to the joint density of the chained melded model, for the same $M$ submodels, for all $\phi_{1 \cap 2}$ and $\phi_{2 \cap 3}$ if and only if
\[
    p_{m}(\phi_{m - 1 \cap m}, \phi_{m \cap m + 1}) =
    p_{m}(\phi_{m - 1 \cap m})
    p_{m}(\phi_{m \cap m + 1})
\]
for $m \in {2, \ldots, M - 1}$ and $M > 3$.
</div>

This is pretty difficult to prove. It either has to start from $M = 4$ for so that we get $\pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3})$ and $\pd_{3}(\phi_{2 \cap 3}, \phi_{3 \cap 4})$ terms, or it doesn't hold in general and there some funky exceptions. But the idea is that I want to be able to state the 'necessary and sufficient' condition is prior independence, with the implication being that without it they differ.

<div class="theorem" text='M = 3 case'>
The above theorem holds in the $M = 3$ case except for following special case not requiring independence
\begin{gather*}
    g_{\text{pool}}(\phi_{1 \cap 2}, \phi_{2 \cap 3}) = p_{1}(\phi_{1 \cap 2}) p_{3}(\phi_{2 \cap 3}), \\
    g^{12}(p_{1}(\phi_{1 \cap 2}), p_{2}(\phi_{1 \cap 2})) = p_{1}(\phi_{1 \cap 2}), \\
    g^{(12)3}(p_{\text{meld}}^{12}(\phi_{2 \cap 3}), p_{3}(\phi_{2 \cap 3})) = p_{3}(\phi_{2 \cap 3})
\end{gather*}
which is a specific choice of dictatorial pooling.
</div>
---
title: "Response to Reviewers"
author: "Andrew A. Manderson and Robert J. B. Goudie"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontfamily: tgpagella
fontsize: 10pt
papersize: a4
geometry: margin=2.25cm
bibliography: ../bibliography/multi-phi-bib.bib
csl: ../bibliography/journal-of-the-royal-statistical-society.csl
link-citations: true
hyperrefoptions:
  - backref
  - colorlinks=true
output:
  pdf_document:
    includes:
      in_header:
        ../tex-input/pre.tex
    fig_caption: true
    number_sections: true
    keep_tex: true
---

<!--
Rscript -e 'rmarkdown::render("ba-reviews/2022-03_response.rmd")'
-->

# Reviewer 1 {-}

> $\hookrightarrow$**R1**: _My main suggestion is for the authors to shorten the manuscript by moving one of the examples to an appendix and instead add a bit more detail about how the melding actually works for readers who may not have familiarity with the previous literature on the topic (including the authors' prior paper on it).  I appreciate both examples in the manuscript, but it might be better to include details about when the pooling results in inference that is exact versus approximate, for example.  There are many choices one must make to implement these methods for fitting models and I think it would help to provide some additional details about the approximation that occurs and the actual algorithms that are used (multi-stage) to fit the models using this approach which don't currently appear until the appendix._

We thank the reviewer for the kind comments.

To address Reviewer 1's first comment, we appreciate the inclination towards a shorter manuscript and contemplated whether we could follow the reviewerâ€™s suggestion, but ultimately feel it is preferable to include both examples in the main text.
As Reviewer 2 notes, the first example is intentionally illustrative -- we have split an existing joint model to make clear, in a simpler setting, the chained Markov melding steps and proposed computational tools, whilst possessing a baseline result for comparison.
This example is also intended to reassure the reader that our proposed multi-stage sampler can produce accurate results.
Although we find this example informative, it is ultimately insufficiently compelling to justify the methodology.
The second example is much more compelling for the methodology, and as Reviewer 2 notes it is the example's complexity that motivates the methodology.
If such complex submodels were not necessary we might be able to immediately conceive of a joint model for all sources of data.
Without the second example we feel some readers will be left wondering what use, if any, there might be for our method.
Finally, moving the first example to the appendix would save only 4 pages, so information currently in the appendix would remain there in either case.

We are unsure of the meaning of the comment describing pooling as producing inference that is exact or approximate.
When all prior marginal densities are known analytically, the pooling function produces an analytic expression proportional to $\pd_{\text{pool}}(\boldsymbol{\phi})$ -- there is no approximation error here.
If one or more of the prior marginal densities is unknown, then we suggest approximating it with a standard distribution or using kernel density estimation. Regardless, the pooling function does not introduce any approximation in addition to that of the prior marginal density estimate.
If by 'exact' the reviewer is instead referring to situations where the chained Markov melded model is the same as the original joint model, then we wish to point our that a suitable joint model must exist in the first place.
Our first example is one such situation, and we have added the following to the start of Section 4:

- "_This example is particularly interesting to us as, for a certain choice of pooling function and pooling weights, the chained Markov melded model and the IPM are identical. This coincidence allows us to use the posterior from the IPM as a benchmark for our multi-stage sampler._"

and the following (new in italic) to Section 4.5:

- "We also use logarithmic pooling with $\lambda = (\frac{1}{2}, \frac{1}{2}, \frac{1}{2})$, which is denoted $\pd_{\text{meld, log}}$ _and results in the chained melded model being identical to the IPM._"

In contrast, there is no obvious alternative joint model in the second example (see our response to Reviewer 2 for further details).

We agree that there are many choices to make when implementing chained Markov melding, but we delineate between choices specific to chained Markov melding and those that must be made when fitting any model.
We wish to include in the main text the choices that we feel will _always_ be important in applications of chained Markov melding, with related, but ultimately example-specific, choices included in the appendix.
To this point we have clarified the purposes of the appendices in the text, and explicitly stated the choices specific to chained Markov melding that users must make.
Our changes are:

1. Adding a sentence to the end of paragraph 1 of the conclusion to that reads:

    "_We also present the choices, and their impacts, that users of chained Markov melding must make which include: the choice of pooling function, and where required the pooling weights; the choice of posterior sampler and the design thereof, including the apportionment of the pooled prior over the stages and stage-specific MCMC techniques._"

1. Emphasising in the text and within the appendix that the individual-at-a-time sampler is only possible due to the conditional independence between individuals in the posterior. Appendix K establishes the validity of our sampler, but the argument is fairly routine and specific to the submodels in the second example (though other submodels may have such posterior conditional independences, such as the hierarchical meta-analysis model in @lunn:etal:13), so including it in the main text seems unnecessary. Specifically, we have added a sentence in Section 5.5 that reads:

    "_This is possible due to the conditional independence between individuals in the posterior, and Appendix K contains the details of this scheme._"

    and modified subsection "sub-step 1" of Appendix K to include:

    "_The model, detailed in Section 5.1, uses the conditional independence between individuals to factorise such that \textellipsis_".

1. Noting in the main text that Appendix B is provided only for completeness. We do not use the sequential sampler in either of our examples. The footnote on page 13 now reads:

    "_For completeness, Appendix B describes such a sequential MCMC sampler. We do not use the sequential sampler in this paper._"

Furthermore, we think that Appendices E, G, and H, covering prior specification and prior marginal estimation, are important but ultimately routine aspects of Bayesian modelling.
Appendices D and I are decisions specific to the data we consider, and would be made as part of any analysis of observational data (and would often go unreported), or would be specified in an analysis plan.

We hope that these points, in addition to addressing the reviewers other comments, sufficiently convince the reviewer of the need for both examples.

> $\hookrightarrow$**R1**: _I also have a number of minor typographical suggestions:_

We thank the reviewer for the suggestions and have adopted them in the revised version.

# Reviewer 2 {-}

We thank the reviewer for the kind comments.

> $\hookrightarrow$**R2**: _(2) I would like the authors to explain why one would not simply ensure that all sub-models can use the same prior, so that we are back in the "Markov combination" setting of Equation 1 (in which case no "pooled" priors need to be constructed). It seems to me that a principled Bayesian approach would always seek to formulate a single prior distribution over all parameters in the model._

We agree with this principle -- it does seem desirable to have the same prior for the same quantity in different models.

However, the starting point for our methodology is multiple complete submodels, which may be specified by separate practitioners.
Since each practitioner is considering a separate submodel, it is likely that each will choose a distinct prior.
Indeed there is a tension between having complete and self-contained submodels, meaning they could be specified and understood independently of the chained melded model, and having the same prior for all common quantities under all submodels.
This is particularly the case when one of the common quantities is a complex, noninvertible function of other submodel parameters.
In our second example, this would involve carefully and simultaneously specifying  the prior for the spline coefficients in $\pd_{1}$ and the prior for the piecewise-linear regression parameters in $\pd_{3}$, such that $\pd_{1}(\phi_{1 \cap 2}) = \pd_{2}(\phi_{1 \cap 2})$, $\pd_{2}(\phi_{2 \cap 3}) = \pd_{3}(\phi_{2 \cap 3})$, and $\pd_{1}(\phi_{1 \cap 2})\pd_{3}(\phi_{2 \cap 3}) = \pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3})$.
It is unclear to us how a prior satisfying these conditions could be directly specified without the perspective and methodology that we propose in the manuscript.

If we do not start from a single common prior, one perspective on our manuscript is that it exactly matches the referee's desire to "ensure that all sub-models can use the same prior, so that we are back in the 'Markov combination' setting".
From this viewpoint, our manuscript answers the question "how can we reformulate the problem of chained submodel combination to be as similar as possible to Markov combination?" via the following steps:

1. Acquiring a common prior $\pd_{\text{pool}}(\boldsymbol{\phi})$ via one of the following methods:
    - We could choose exactly one of the submodel priors for use in the melded model.
This is dictatorial pooling, and thus Markov combination is the special case of Markov melding with dictatorial pooling where all submodel prior marginals are identical.
For chained Markov melding this is unappealing for the reasons we discuss in the dictatorial pooling subsection of Section 2.2.
Specifically, that choosing one prior does not "span" all the quantities in $\boldsymbol{\phi}$.
Our dictatorial pooling proposal for chained Markov melding is our proposal for a joint model as similar to Markov combination as possible in the chained model setting.
    - Alternatively, our logarithmic and linear pooling proposals are transparent methods for obtaining a single common prior $\pd_{\text{pool}}(\boldsymbol{\phi})$ in situations where the dictatorially pooled prior is unacceptable.
    - If a common prior were available from an alternative source (not based upon pooling) then clearly this could be a sensible choice.

2. Adapt each submodel to use this common prior, using marginal replacement
3. Form the Markov melding model by using Markov combination on these marginally-replaced submodels

We now emphasise these steps at the start of Section 2.1 by stating:

- "_The proposed chained marginal replacement operation modifies the submodels to enforce a common prior for $\boldsymbol{\phi}$. This consistency allows us to use Markov combination to unite the submodels._"


Finally, it is important to bear in mind that whilst we might aspire to undertake a fully "principled Bayesian approach", such as the referee alludes to, in some applications this can be too lofty a goal.
Often the number of information sources are too numerous, with some having standard models and others not, and the computational/time constraints are so apparent that to start building a suitable joint model is infeasible.
Our aim is to make it possible to be "more Bayesian" in such difficult settings.
That is, we would like to start incorporating uncertainty from many information sources in a more transparent fashion than the alternative of 1) fixing a common quantity to a point estimate or 2) specifying a Gaussian distribution for said quantity [e.g. Section 4.2.4 of @nicholson_interoperability_2021].
We posit that the construction of a complex joint models by combining smaller, easier to understand submodels results in a better quantification of uncertainty (whilst also retaining the ability to specify, possibly fit, and interrogate the submodels before combining them).

\par\noindent\hrulefill\par

We address points (3) and (4) simultaneously.

> $\hookrightarrow$**R2**: _(3) The proposed methodology appears to perform well in the integrated-population-model (IPM) example. But, as the authors mention, this is just for illustration since the joint posterior distribution in this model can be easily and cheaply approximated using a single MCMC chain via standard software packages._
> _The model in Section 5 appears to be realistically complex. It is great to see such non-toy examples in a methodological paper. However, the example is almost too complex: I am finding it difficult to understand how well (or poorly) the proposed methodology performs here. It would be really good to have benchmark results for the joint model without the bias induced by Markov melding (i.e. in the same way as these are shown for the IPM example). I would expect that such results are be attainable using a sequential Monte Carlo sampler._

> $\hookrightarrow$**R2**: _(4) If benchmark results cannot be obtained for the model from Section 5, this would also be interesting and mentioning this would make the paper much stronger. Because when reading the paper, I did not see an explanation of why the model could not be estimated without Markov melding (and without the bias that this can introduce)._

We do not think there is any reasonable benchmark for the model in Section 5. To obtain benchmark results there must exist

1. a comparable joint model 
2. a "default" method for sampling/approximating the posterior of this model. 

We do not believe that anything exists that reasonably satisfies these conditions.

With respect to the first prerequisite, the premise for the respiratory failure (RF) example, and indeed the methodology, is that there are situations in which it is prohibitively difficult to specify a suitable joint model.
Without such a joint model there is no baseline against which we could declare a model biased, or compare our results.
In particular, the joint model formed by multiplying the individuals submodels is a form of chained Markov melding (using product-of-experts pooling); to use it as a baseline would be circular.

Even if a comparable target/joint model could nevertheless be decided upon, then we are skeptical of the refereeâ€™s expectation that an alternative sampler could easily be implemented.
In particular regarding the refereeâ€™s suggestion of an SMC sampler, we wish to highlight a number of implementation difficulties that would be faced, as well as reasons why it is not an immediate perfect fit for our problem setting:

1. There are no sufficiently flexible off the shelf SMC packages. `NIMBLE` [@michaud_sequential_2020] does not support either the B-spline basis function nor multiple-root finder, and whilst one could in theory recode both in `Rcpp` to make use of `RcppSMC` [@eddelbuettel_rcppsmc_2021; @johansen_smctc_2009], this would be an enormous undertaking. We also point out the difficultly in implementing an SMC sampler for the considerably simpler first example encountered by @finke_efficient_2019 (with details discussed in the online appendix). Tuning these types of samplers is also nontrivial [@buchholz_adaptive_2021].
1. Simultaneously evaluating all the terms in the chained melded model is necessary and computationally challenging in a SMC/MCMC environment. This is a secondary motivation for the multi-stage sampler, as complex evidence synthesis / joint models often cannot be readily evaluated in this manner [see, for example, the models in @nicholson_interoperability_2021].

    Avoiding concurrently evaluating all submodels also enables the reuse of existing software, minimising the need for custom submodel and/or sampler implementations (and the effort the construction/tuning of them entails). We see this as an advantage of our multi-stage estimation method, as it reduces the effort required to be "more Bayesian" and better quantify uncertainty. We state this in both the introduction and in Section 4.6, and have added additional text to the first paragraph of Section 3.

    One complication of this variety in the second example is the calculation of $\phi_{1 \cap 2} = (f(\chi_{1 \cap 2, 1}), \ldots, f(\chi_{1 \cap 2, N}))$. The mathematics belies much complexity, specifically because we rely on a multiple root finder. Implementing this within an HMC, or SMC with HMC refreshment, setting involves massively increasing the number of times this problem must be solved. Using our multi-stage sampler turns this into an embarrassingly parallel, post-stage-one processing problem.

1. Our preference for HMC is because the example depends on a relatively high number ($5 \times 10^{4}$) of high-quality samples from $\pd_{1}(\phi_{1 \cap 2}, \psi_{1} \mid Y_{1})$, which is $N(2 + 10) = 444$-dimensional and highly correlated within each of the 12-dimensional individual subposteriors (see Figure \ref{fig:coef_pairs} at the end of this document). This is made possible by the efficient dynamic HMC implementation within `Stan`.

For the reasons above we do believe there is a reasonable and implementable comparator for the results in Section 5.
We feel this complex example is forms an ideal complement to the simple IPM example, in which a direct comparator is easily available and via which we demonstrate that our methodology can reproduce a joint model in a more straightforward setting.

Finally, we are not sure we understand the referee's discussion of "bias" in this context.
We infer from their comments that the referee may mean a difference between between the results of a directly-specified joint model and the Markov melding model.
As noted in the text for the IPM example, and in response to Reviewer 1, the IPM and the chained model using log-pooling with $\lambda = (\frac{1}{2}, \frac{1}{2}, \frac{1}{2})$ are precisely the same joint model, so no bias of this type is introduced by chained Markov melding in that example.
In the RF example, as we have explained, no obvious comparator joint model is available, so we are unclear what "bias" the referee thinks may be induced.

With the above points in mind, we have made the following modifications to the manuscript to emphasise the key aspects of this example, including:

1. To the end of Section 1.1 "_It is in examples such as this one that we foresee the most use for chained Markov melding; a fully Bayesian approach is desired and the submodels are nontrivial in complexity, with no previously existing or obvious joint model._"
1. To the end of the first paragraph of Section 3.: "_Avoiding concurrently evaluating all submodels also enables the reuse of existing software, minimising the need for custom submodel and/or sampler implementations._"
1. To the end of the first paragraph of Section 5.1 "_Specifically, event times and indicators are a noninvertible function of other parameters in the first submodel, and are an uncertain response in the survival submodel. Chained Markov melding enables us to specify a suitable joint model despite these complications._"
1. As the new third and forth sentences of paragraph 2 in Section 5.5 "_Targeting_ $\pd_{1}(\phi_{1 \cap 2}, \psi_{1} \mid Y_{1})$ _in stage one alleviates the need to solve Equation (46) within an MCMC iteration, instead turning the production of_ $\phi_{1 \cap 2}$ _into an embarrassingly parallel, post-stage-one processing step. Attempting to sample the melded posterior directly would involve solving (46) many times within each iteration, presenting a sizeable computational hurdle which we avoid._"

# Handling Editor {-}

> $\hookrightarrow$**HE**: _On equation (42) what happens when_ $x_{t - 1} = 0$? _Shouldn't the notation accommodate this case?_

Strictly yes, assuming the issue is that the $\text{Poisson}(\lambda)$ distribution is not typically defined when $\lambda = 0$.
We note that this model is ubiquitous and this issue is ubiquitously ignored [Section 17.3 of @king_statistical_2011; the "Materials and methods" section in @abadi_estimation_2010; Section 5.2.1 of @finke_efficient_2019].
We have added the following sentence to the end of Section 4.2 reading

- "_If_ $x_{t - 1} = 0$ _then we assume that the Poisson and binomial distributions become point masses at zero._"

This assumption is ecologically reasonable -- if there are no owls alive at time $t - 1$, i.e. $x_{t - 1} = 0$, then the local population has gone extinct and there will no more owls at times $t, \ldots, T$.
It also aligns with the behaviour of `NIMBLE`/`rjags`, which we use for this part of the example.
Both will sample a node `x` defined as `x ~ dpois(0.0)` with all samples being $0$:

```{r zero_pois_demo, cache = TRUE}
suppressPackageStartupMessages(library(nimble))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(rjags))

samples_nimble <- nimbleCode({
  x ~ dpois(0.0)
}) %>%
  nimbleModel() %>%
  nimbleMCMC(niter = 10) %>%
  suppressMessages() %>%
  as.numeric()

samples_nimble

tf <- tempfile()
mod <- "model {x ~ dpois(0.0)}" %>%
  writeLines(con = tf)

samples_jags <- jags.model(tf, quiet = TRUE) %>%
  jags.samples(variable.names = "x", n.iter = 10) %>%
  magrittr::extract2("x") %>%
  magrittr::extract(, 1 : 10, )

samples_jags
```

# Bibliography {-}

<div id="refs"></div>

\newpage

# Appendix {-}

```{r coef_pairs, echo = FALSE, fig.cap = "Pairs plot for individual $i = 18$ in Example 2. The displayed are parameters are $\\beta_{0, i}$ and $\\xi_{i, 1}, \\ldots, \\xi_{i, 3}$."}
knitr::include_graphics("plots/indiv-18-subset.png")
```

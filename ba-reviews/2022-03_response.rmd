---
title: "Response to Reviewers"
author: "Andrew A. Manderson and Robert J. B. Goudie"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontfamily: tgpagella
fontsize: 10pt
papersize: a4
geometry: margin=2.25cm
bibliography: ../bibliography/multi-phi-bib.bib
csl: ../bibliography/journal-of-the-royal-statistical-society.csl
link-citations: true
hyperrefoptions:
  - backref
  - colorlinks=true
output:
  pdf_document:
    includes:
      in_header:
        ../tex-input/pre.tex
    fig_caption: true
    number_sections: true
    keep_tex: true
---

<!--
Rscript -e 'rmarkdown::render("ba-reviews/2022-03_response.rmd")'
-->

# Reviewer 1


> _My main suggestion is for the authors to shorten the manuscript by moving one of the examples to an appendix and instead add a bit more detail about how the melding actually works for readers who may not have familiarity with the previous literature on the topic (including the authors' prior paper on it).  I appreciate both examples in the manuscript, but it might be better to include details about when the pooling results in inference that is exact versus approximate, for example.  There are many choices one must make to implement these methods for fitting models and I think it would help to provide some additional details about the approximation that occurs and the actual algorithms that are used (multi-stage) to fit the models using this approach which don't currently appear until the appendix._

We thank the reviewer for the kind comments.
We will respond in reverse order to the comments raised by Reviewer 1.

We agree that there are many choices to make when implementing chained Markov melding, but we delineate between choices specific to chained Markov melding and those that must be made in fitting any model.
We wish to include in the main text the choices that we feel will _always_ be relevant to applications of chained Markov melding, with important, but ultimately example-specific, choices included in the appendix.
To this point, we have clarified the purposes of the appendices in the text, and have hopefully assured the reviewer that material in the appendices is overly specific to our second example.
Our changes include:

1. Clarifying in the main text that Appendix B is provided only for completeness. We do not use the sequential sampler in either of our examples.
1. Emphasising in the text and within the appendix that Appendix K is only possible due to the conditional independence between individuals in the posterior, which is very specific to the models under consideration in the second example.

Furthermore, we think that Appendices E, G, and H, which cover prior specification and prior marginal estimation, are important but ultimately routine aspects of Bayesian modelling. Specifying appropriate priors is a universal task when using the Bayesian paradigm, and performing density estimation from Monte Carlo samples should be familiar to most statistical practitioners. We have also introduced a more sophisticated methodology for density estimation in this setting in @manderson_numerically_2022 for the particularly interested reader.
Appendices D and I are decisions specific to the data we consider, which is more specific than the models, and would be made as part of any analysis of observational data (and would often go unreported), or would be specified in an analysis plan.

We are unsure of the meaning of the comment describing pooling as producing inference that is exact or approximate.
When all prior marginal densities are known analytically, the pooling function produces an analytic expression proportional to $\pd_{\text{pool}}(\boldsymbol{\phi})$ -- there is no approximation error here.
If one or more of the prior marginal densities is unknown, then we suggest approximating it (or a transformation of it) with a standard distribution, or using kernel density estimation. Regardless, the pooling function does not introduce any approximation in addition to that of the prior marginal density estimate.

To address Reviewer 1's first comment, we appreciate (and agree with) the inclination towards a shorter manuscript, but ultimately feel tied to including both examples in the main text.
As Reviewer 2 notes, the first example is intentionally illustrative -- we have split an existing joint model to illustrate, in a simpler setting, the melding steps and computational tools whilst possessing a baseline set of result for comparison.
This example is also intended to reassure the reader that our proposed multi-stage sampler can produce accurate results.
Whilst we find this example informative, it is ultimately insufficiently compelling to justify the methodology.
Example 2 is much more compelling for the methodology, and as Reviewer 2 notes it is this complexity that is ultimately the motivating aspect of the methodology. Without such complex submodels we might be able to directly conceive of a joint model for all sources of data.
Without the second example we feel some readers will be left wondering what use, if any, there might be for our method.
Finally, cutting the first example would save only 4 pages, so information currently in the appendix would remain there in either case.

We hope that these points, in addition to addressing the reviewers other comments, sufficiently convince the reviewer of the need for both examples.

> _I also have a number of minor typographical suggestions:_

We thank the reviewer for the suggestions and have adopted them in the revised version.

# Reviewer 2

- We thank the reviewer for the kind comments.

> _(2) I would like the authors to explain why one would not simply ensure that all sub-models can use the same prior, so that we are back in the "Markov combination" setting of Equation 1 (in which case no "pooled" priors need to be constructed). It seems to me that a principled Bayesian approach would always seek to formulate a single prior distribution over all parameters in the model._

- We agree in principle with this sentiment -- it seems desirable to have the same prior for the same quantity in different models.
- But there is a tension between having the individual submodels be complete and contained, meaning that they could be produced independently of the submodels, and having the same prior for all common quantities under all submodels.
- Perhaps our biggest struggle with this idea is visible in example two, where the common quantity $\phi_{1 \cap 2}$ is the response in the second submodel $\pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3}, \psi_{2}, Y_{2})$ and $\pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3})$ contains dependence between the common quantities.
- There are two approaches to ensuring $\pd_{1}(\phi_{1 \cap 2}) = \pd_{2}(\phi_{1 \cap 2})$, $\pd_{2}(\phi_{2 \cap 3}) = \pd_{3}(\phi_{2 \cap 3})$, and $\pd_{1}(\phi_{1 \cap 2})\pd_{3}(\phi_{2 \cap 3}) = \pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3})$ in example two.
  1. Carefully and simultaneously select the prior for the spline coefficients in $\pd_{1}$ and the prior for the piecewise-linear regression parameters in $\pd_{3}$, such that all the aforementioned equalities are satisfied.
    - There are no guarantees that such a set of priors exists[^prior-by-ppd], and even if they did it's unclear how we might actually specify them.
  1. Externally define $\pd_{\text{pool}}(\boldsymbol{\phi})$ and replace $\pd_{1}(\phi_{1 \cap 2})$ with $\pd_{\text{pool}}(\phi_{1 \cap 2})$, $\pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3})$ with $\pd_{\text{pool}}(\boldsymbol{\phi})$, and $\pd_{3}(\phi_{2 \cap 3})$ with $\pd_{\text{pool}}(\phi_{2 \cap 3})$. Alternatively we could expand $\pd_{1}$ to be aware of $\pd_{3}$ by replacing $\pd_{1}(\phi_{1 \cap 2})$ with $\pd_{\text{pool}}(\boldsymbol{\phi})$, doing likewise for $\pd_{3}$.
    - Both proposals require some kind of pooling to account for applying the prior twice when combining the submodels, should the user wish to correct for this information.
    - The second proposal requires expanding the first and third submodels to include a quantity unrelated to the submodel or the submodel's data. There is no information about e.g. $\phi_{2 \cap 3}$ in $\pd_{1}$. This submodel expansion makes it more difficult to understand the submodels in isolation.
    - One aim of our methodology is to enable the construction of complex joint models by combining smaller, easier to understand submodels. The ability to specify, and possibly fit and interrogate a submodel before combining it is key -- we'd like to understand the submodels themselves before combining them. In particular, either proposal yields a very strange model for $\pd_{2}$, which would contain an prior distribution directly on the observational quantity in addition to the likelihood (as well as the prior on the other parameters). Whilst many of these properties are also true of the chained melded model, the key difference is that the submodels remain unmodified and are understandable in isolation.
    - There are no guarantees that a prior that "works" (appropriately represents known information and facilitates the computation of the subposterior) for the submodel will be suitable for the chained melded model with its higher dimensional parameter space. Pooling allows us to take known priors and modify them in a transparent way to ensure they are suitable for the larger chained melded model.

[^prior-by-ppd]: In our experience there are typically many possible priors that have a specific prior predictive distribution for a quantity that is a non-invertible transformation of the parameters. Locating them requires both specialised methodology and a decision analysis to choose between the possible priors.

> _(3) The proposed methodology appears to perform well in the integrated-population-model (IPM) example. But, as the authors mention, this is just for illustration since the joint posterior distribution in this model can be easily and cheaply approximated using a single MCMC chain via standard software packages._
> _The model in Section 5 appears to be realistically complex. It is great to see such non-toy examples in a methodological paper. However, the example is almost too complex: I am finding it difficult to understand how well (or poorly) the proposed methodology performs here. It would be really good to have benchmark results for the joint model without the bias induced by Markov melding (i.e. in the same way as these are shown for the IPM example). I would expect that such results are be attainable using a sequential Monte Carlo sampler._

- We find find the meaning of the word 'bias' difficult to determine in this comment. Does it refer to the hypothetical 'bias' introduced by the chained Markov melding process (which produces a joint model), or the bias introduced by the computational multi-stage algorithm we propose to sample the chained melded posterior?
- Chained Markov melding does not necessarily introduce any 'bias'. As noted in the text for the first example, the IPM and the log-pooling with $\lambda = (\frac{1}{2}, \frac{1}{2}, \frac{1}{2})$ are precisely the same joint model, so no possible 'bias' has been introduced by chained Markov melding in that example.
- If it is this type of bias the reviewer has in mind, then the question is what joint model the reviewer wishes to target? The product of the individual submodels is already a form of chained Markov melding (with product-of-experts pooling), and thus would likewise possess any possible 'bias' present in the PoE case presented in Section 5.6.
- Additionally, in the PoE case, there is no bias due to marginal density approximation as these terms all cancel in the second stage acceptance probability (so we do not use the marginal approximations).

- If it is the bias in the multi-stage algorithm then:
  - Any finite sample, finite time approximation method will posses some form of bias.
  - The joint model parameter space is quite high dimensional (648). The HMC within Stan is key to getting reliable posterior samples from the first and third subposteriors.
  - The proposed multi-stage parallel sampler is a crude SMC sampler (with no particle refreshment step and an immediate tempering schedule).
  - there genuinely are no generic tools for sampling the joint (melded) model in one step (Stan and a TMB-based/`RcppSmc` SMC [@eddelbuettel_rcppsmc_2021; @johansen_smctc_2009] lack access to a multiple root finder, as does the BUGS language (in addition to its insufficient flexibility) rendering the SMC inside `Nimble` [@michaud_sequential_2020] unsuitable).
    - We'd have to build it ourselves, and I'm not convinced I could code one that would converge.
    - Any SMC sampler I would build would simply sample $\pd_{1}(\theta_{1} \mid Y_{1})$ and $\pd_{3}(\theta_{3} \mid Y_{3})$ first using Stan/HMC, then temper in the chained melded posterior as a target. Determining refreshment moves for the $370$ spline coefficients would probably require a sufficiently novel SMC+HMC refreshment scheme that I would also want to include it in Section 3.
  - Computing the event times and indicators is not entirely trivial computationally. Embedding this inside an SMC sampler is expensive (it is prohibitively expensive for HMC as we need to autodiff through a loop over a root-finder, which in turn repeatedly calls the b-spline basis function). Computing the event times as a processing step after stage one but before stage tow makes this step embarrassingly parallel (making the multistage sampler feasible).

> _(4) If benchmark results cannot be obtained for the model from Section 5, this would also be interesting and mentioning this would make the paper much stronger. Because when reading the paper, I did not see an explanation of why the model could not be estimated without Markov melding (and without the bias that this can introduce)._

- This comment again makes us ask the question about what target or joint model the reviewer has in mind.

# Handling Editor

> _On equation (42) what happens when_ $x_{t - 1} = 0$? _Shouldn't the notation accommodate this case?_

- Strictly yes, but this model is ubiquitous and this issue is ubiquitously ignored [Section 17.3 of @king_statistical_2011; @abadi_estimation_2010; Section 5.2.1 of @finke_efficient_2019].
- One suspects that an additional model component is required for $x_{t - 1} = 0$, as this presumably corresponds to extinction.


# Bibliography


@article{abadi_estimation_2010,
  title = {Estimation of Immigration Rate Using Integrated Population Models},
  author = {Abadi, Fitsum and Gimenez, Olivier and Ullrich, Bruno and Arlettaz, Rapha{\"e}l and Schaub, Michael},
  year = {2010},
  volume = {47},
  pages = {393--400},
  doi = {10.1111/j.1365-2664.2010.01789.x},
  annotation = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2664.2010.01789.x},
  file = {/Users/amanderson/Zotero/storage/6BF7EPE3/Abadi et al. - 2010 - Estimation of immigration rate using integrated po.pdf},
  journal = {Journal of Applied Ecology},
  number = {2}
}

@article{alvares_bayesian_2020,
  title = {Bayesian Survival Analysis with {{BUGS}}},
  author = {Alvares, Danilo and L{\'a}zaro, Elena and {G{\'o}mez-Rubio}, Virgilio and Armero, Carmen},
  year = {2020},
  month = jul,
  abstract = {Survival analysis is one of the most important fields of statistics in medicine and the biological sciences. In addition, the computational advances in the last decades have favoured the use of Bayesian methods in this context, providing a flexible and powerful alternative to the traditional frequentist approach. The objective of this paper is to summarise some of the most popular Bayesian survival models, such as accelerated failure time, proportional hazards, mixture cure, competing risks, frailty, and joint models of longitudinal and survival data. Moreover, an implementation of each presented model is provided using a BUGS syntax that can be run with JAGS from the R programming language. Reference to other Bayesian R-packages are also discussed.},
  archivePrefix = {arXiv},
  eprint = {2005.05952},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/JYAEVDNP/Alvares et al. - 2020 - Bayesian survival analysis with BUGS.pdf;/Users/amanderson/Zotero/storage/L7JJMTEX/2005.html},
  journal = {arXiv:2005.05952 [stat]},
  keywords = {Statistics - Applications},
  primaryClass = {stat}
}

@article{austin_generating_2012,
  title = {Generating Survival Times to Simulate {{Cox}} Proportional Hazards Models with Time-Varying Covariates},
  author = {Austin, Peter C.},
  year = {2012},
  volume = {31},
  pages = {3946--3958},
  issn = {1097-0258},
  doi = {10.1002/sim.5452},
  abstract = {Simulations and Monte Carlo methods serve an important role in modern statistical research. They allow for an examination of the performance of statistical procedures in settings in which analytic and mathematical derivations may not be feasible. A key element in any statistical simulation is the existence of an appropriate data-generating process: one must be able to simulate data from a specified statistical model. We describe data-generating processes for the Cox proportional hazards model with time-varying covariates when event times follow an exponential, Weibull, or Gompertz distribution. We consider three types of time-varying covariates: first, a dichotomous time-varying covariate that can change at most once from untreated to treated (e.g., organ transplant); second, a continuous time-varying covariate such as cumulative exposure at a constant dose to radiation or to a pharmaceutical agent used for a chronic condition; third, a dichotomous time-varying covariate with a subject being able to move repeatedly between treatment states (e.g., current compliance or use of a medication). In each setting, we derive closed-form expressions that allow one to simulate survival times so that survival times are related to a vector of fixed or time-invariant covariates and to a single time-varying covariate. We illustrate the utility of our closed-form expressions for simulating event times by using Monte Carlo simulations to estimate the statistical power to detect as statistically significant the effect of different types of binary time-varying covariates. This is compared with the statistical power to detect as statistically significant a binary time-invariant covariate. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5452},
  file = {/Users/amanderson/Zotero/storage/CNYJXAZ3/Austin - 2012 - Generating survival times to simulate Cox proporti.pdf;/Users/amanderson/Zotero/storage/XNULSHBT/sim.html},
  journal = {Statistics in Medicine},
  keywords = {exponential distribution,Gompertz distribution,power and sample size calculation,proportional hazards model,simulations,survival analysis,time-dependent covariate,time-varying covariates,Weibull distribution},
  language = {en},
  number = {29}
}

@book{betancourt_towards_2020,
  title = {Towards {{A Principled Bayesian Workflow}} ({{RStan}})},
  author = {Betancourt, Michael},
  year = {2020}
}

@article{bissiri_general_2016,
  title = {A General Framework for Updating Belief Distributions},
  author = {Bissiri, P. G. and Holmes, C. C. and Walker, S. G.},
  year = {2016},
  month = nov,
  volume = {78},
  pages = {1103--1130},
  issn = {13697412},
  doi = {10.1111/rssb.12158},
  abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered as a special case. Modern application areas make it increasingly challenging for Bayesians to attempt to model the true data-generating mechanism. For instance, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our framework uses loss functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.},
  file = {/Users/amanderson/Zotero/storage/M3RBG5D8/Bissiri et al. - 2016 - A general framework for updating belief distributi.pdf;/Users/amanderson/Zotero/storage/A3W23SND/rssb.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Decision theory,General Bayesian updating,Generalized estimating equations,Gibbs posteriors,Information,Loss function,Maximum entropy,Provably approximately correct Bayes methods,Self-information loss function},
  language = {en},
  number = {5}
}

@article{brilleman_bayesian_2020,
  title = {Bayesian Survival Analysis Using the Rstanarm {{R}} Package},
  author = {Brilleman, Samuel L. and Elci, Eren M. and Novik, Jacqueline Buros and Wolfe, Rory},
  year = {2020},
  month = feb,
  abstract = {Survival data is encountered in a range of disciplines, most notably health and medical research. Although Bayesian approaches to the analysis of survival data can provide a number of benefits, they are less widely used than classical (e.g. likelihood-based) approaches. This may be in part due to a relative absence of user-friendly implementations of Bayesian survival models. In this article we describe how the rstanarm R package can be used to fit a wide range of Bayesian survival models. The rstanarm package facilitates Bayesian regression modelling by providing a user-friendly interface (users specify their model using customary R formula syntax and data frames) and using the Stan software (a C++ library for Bayesian inference) for the back-end estimation. The suite of models that can be estimated using rstanarm is broad and includes generalised linear models (GLMs), generalised linear mixed models (GLMMs), generalised additive models (GAMs) and more. In this article we focus only on the survival modelling functionality. This includes standard parametric (exponential, Weibull, Gompertz) and flexible parametric (spline-based) hazard models, as well as standard parametric accelerated failure time (AFT) models. All types of censoring (left, right, interval) are allowed, as is delayed entry (left truncation), time-varying covariates, time-varying effects, and frailty effects. We demonstrate the functionality through worked examples. We anticipate these implementations will increase the uptake of Bayesian survival analysis in applied research.},
  archivePrefix = {arXiv},
  eprint = {2002.09633},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/IPYBBCQA/Brilleman et al. - 2020 - Bayesian Survival Analysis Using the rstanarm R Pa.pdf;/Users/amanderson/Zotero/storage/U6PAEYNU/2002.html},
  journal = {arXiv:2002.09633 [stat]},
  keywords = {Statistics - Computation,Statistics - Methodology},
  note = {Comment: 50 pages, 5 figures},
  primaryClass = {stat}
}

@misc{brilleman_simsurv_2019,
  title = {Simsurv: {{Simulate}} Survival Data},
  shorttitle = {Simsurv},
  author = {Brilleman, Sam and Gasparini, Alessandro},
  year = {2019},
  month = feb,
  abstract = {Simulate survival times from standard parametric survival distributions (exponential, Weibull, Gompertz), 2-component mixture distributions, or a user-defined hazard, log hazard, cumulative hazard, or log cumulative hazard function. Baseline covariates can be included under a proportional hazards assumption. Time dependent effects (i.e. non-proportional hazards) can be included by interacting covariates with linear time or a user-defined function of time. Clustered event times are also accommodated. The 2-component mixture distributions can allow for a variety of flexible baseline hazard functions reflecting those seen in practice. If the user wishes to provide a user-defined hazard or log hazard function then this is possible, and the resulting cumulative hazard function does not need to have a closed-form solution. Note that this package is modelled on the 'survsim' package available in the 'Stata' software (see Crowther and Lambert (2012) {$<$}http://www.stata-journal.com/sjpdf.html?articlenum=st0275{$>$} or Crowther and Lambert (2013) {$<$}doi:10.1002/sim.5823{$>$}).},
  annotation = {R package version 0.2.3},
  copyright = {GPL ({$\geq$} 3) | file LICENSE},
  keywords = {Survival}
}

@article{bromiley_products_2003,
  title = {Products and Convolutions of {{Gaussian}} Probability Density Functions},
  author = {Bromiley, Paul},
  year = {2003},
  volume = {3},
  pages = {1},
  abstract = {It is well known that the product and the convolution of Gaussian probability density functions (PDFs) are also Gaussian functions. This document provides proofs of this for several cases; the product of two univariate Gaussian PDFs, the product of an arbitrary number of univariate Gaussian PDFs, the product of an arbitrary number of multivariate Gaussian PDFs, and the convolution of two univariate Gaussian PDFs. These results are useful in calculating the effects of smoothing applied as an intermediate step in various algorithms.},
  file = {/Users/amanderson/Zotero/storage/47G9IXHQ/Bromiley - Products and Convolutions of Gaussian Probability .pdf},
  journal = {Tina-Vision Memo},
  language = {en},
  number = {4}
}

@article{carpenter_stan_2017,
  title = {Stan: {{A}} Probabilistic Programming Language},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  volume = {76},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  journal = {Journal of Statistical Software},
  number = {1}
}

@article{clemen_combining_1999,
  title = {Combining Probability Distributions from Experts in Risk Analysis},
  author = {Clemen, Robert T. and Winkler, Robert L.},
  year = {1999},
  volume = {19},
  pages = {187--203},
  issn = {1539-6924},
  doi = {10.1111/j.1539-6924.1999.tb00399.x},
  abstract = {This paper concerns the combination of experts' probability distributions in risk analysis, discussing a variety of combination methods and attempting to highlight the important conceptual and practical issues to be considered in designing a combination process in practice. The role of experts is important because their judgments can provide valuable information, particularly in view of the limited availability of ``hard data'' regarding many important uncertainties in risk analysis. Because uncertainties are represented in terms of probability distributions in probabilistic risk analysis (PRA), we consider expert information in terms of probability distributions. The motivation for the use of multiple experts is simply the desire to obtain as much information as possible. Combining experts' probability distributions summarizes the accumulated information for risk analysts and decision-makers. Procedures for combining probability distributions are often compartmentalized as mathematical aggregation methods or behavioral approaches, and we discuss both categories. However, an overall aggregation process could involve both mathematical and behavioral aspects, and no single process is best in all circumstances. An understanding of the pros and cons of different methods and the key issues to consider is valuable in the design of acombination process for a specific PRA. The output, a ''combined probabilitydistribution,'' can ideally be viewed as representing a summary of the current state of expert opinion regarding the uncertainty of interest.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1539-6924.1999.tb00399.x},
  file = {/Users/amanderson/Zotero/storage/WLTS8H25/Clemen and Winkler - 1999 - Combining Probability Distributions From Experts i.pdf;/Users/amanderson/Zotero/storage/6PJY5DKL/j.1539-6924.1999.tb00399.html;/Users/amanderson/Zotero/storage/MEMNQZMM/j.1539-6924.1999.tb00399.html},
  journal = {Risk Analysis},
  keywords = {Combining probabilities,expert judgment,probability assessment},
  language = {en},
  number = {2}
}

@article{crowther_simulating_2013,
  title = {Simulating Biologically Plausible Complex Survival Data},
  author = {Crowther, Michael J. and Lambert, Paul C.},
  year = {2013},
  volume = {32},
  pages = {4118--4134},
  issn = {1097-0258},
  doi = {10.1002/sim.5823},
  abstract = {Simulation studies are conducted to assess the performance of current and novel statistical models in pre-defined scenarios. It is often desirable that chosen simulation scenarios accurately reflect a biologically plausible underlying distribution. This is particularly important in the framework of survival analysis, where simulated distributions are chosen for both the event time and the censoring time. This paper develops methods for using complex distributions when generating survival times to assess methods in practice. We describe a general algorithm involving numerical integration and root-finding techniques to generate survival times from a variety of complex parametric distributions, incorporating any combination of time-dependent effects, time-varying covariates, delayed entry, random effects and covariates measured with error. User-friendly Stata software is provided. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5823},
  copyright = {Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/amanderson/Zotero/storage/6U2NSKRE/Crowther and Lambert - 2013 - Simulating biologically plausible complex survival.pdf;/Users/amanderson/Zotero/storage/U5RYACZ7/sim.html},
  journal = {Statistics in Medicine},
  keywords = {delayed entry,measurement error,simulation,survival,time-dependent effects,time-varying covariates},
  language = {en},
  number = {23}
}

@article{de_valpine_programming_2017,
  title = {Programming with Models: Writing Statistical Algorithms for General Model Structures with {{NIMBLE}}},
  shorttitle = {Programming with Models},
  author = {{de Valpine}, Perry and Turek, Daniel and Paciorek, Christopher J. and {Anderson-Bergman}, Clifford and Lang, Duncan Temple and Bodik, Rastislav},
  year = {2017},
  month = apr,
  volume = {26},
  pages = {403--413},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2016.1172487},
  abstract = {We describe NIMBLE, a system for programming statistical algorithms for general model structures within R. NIMBLE is designed to meet three challenges: flexible model specification, a language for programming algorithms that can use different models, and a balance between high-level programmability and execution efficiency. For model specification, NIMBLE extends the BUGS language and creates model objects, which can manipulate variables, calculate log probability values, generate simulations, and query the relationships among variables. For algorithm programming, NIMBLE provides functions that operate with model objects using two stages of evaluation. The first stage allows specialization of a function to a particular model and/or nodes, such as creating a Metropolis-Hastings sampler for a particular block of nodes. The second stage allows repeated execution of computations using the results of the first stage. To achieve efficient second-stage computation, NIMBLE compiles models and functions via C++, using the Eigen library for linear algebra, and provides the user with an interface to compiled objects. The NIMBLE language represents a compilable domain-specific language (DSL) embedded within R. This article provides an overview of the design and rationale for NIMBLE along with illustrative examples including importance sampling, Markov chain Monte Carlo (MCMC) and Monte Carlo expectation maximization (MCEM). Supplementary materials for this article are available online.},
  file = {/Users/amanderson/Zotero/storage/ZYMTAZF3/de Valpine et al. - 2017 - Programming With Models Writing Statistical Algor.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {2}
}

@article{donnat_bayesian_2020,
  title = {A {{Bayesian}} Hierarchical Network for Combining Heterogeneous Data Sources in Medical Diagnoses},
  author = {Donnat, Claire and Miolane, Nina and Bunbury, Freddy and Kreindler, Jack},
  year = {2020},
  month = oct,
  abstract = {Computer-Aided Diagnosis has shown stellar performance in providing accurate medical diagnoses across multiple testing modalities (medical images, electrophysiological signals, etc.). While this field has typically focused on fully harvesting the signal provided by a single (and generally extremely reliable) modality, fewer efforts have utilized imprecise data lacking reliable ground truth labels. In this unsupervised, noisy setting, the robustification and quantification of the diagnosis uncertainty become paramount, thus posing a new challenge: how can we combine multiple sources of information -- often themselves with vastly varying levels of precision and uncertainty -- to provide a diagnosis estimate with confidence bounds? Motivated by a concrete application in antibody testing, we devise a Stochastic Expectation-Maximization algorithm that allows the principled integration of heterogeneous, and potentially unreliable, data types. Our Bayesian formalism is essential in (a) flexibly combining these heterogeneous data sources and their corresponding levels of uncertainty, (b) quantifying the degree of confidence associated with a given diagnostic, and (c) dealing with the missing values that typically plague medical data. We quantify the potential of this approach on simulated data, and showcase its practicality by deploying it on a real COVID-19 immunity study.},
  archivePrefix = {arXiv},
  eprint = {2007.13847},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/53RRDA6G/Donnat et al. - 2020 - A Bayesian Hierarchical Network for Combining Hete.pdf;/Users/amanderson/Zotero/storage/JJPBDPT7/Donnat et al. - 2020 - A Bayesian Hierarchical Network for Combining Hete.pdf;/Users/amanderson/Zotero/storage/8982MAK5/2007.html;/Users/amanderson/Zotero/storage/BCASEUDV/2007.html},
  journal = {arXiv:2007.13847 [stat]},
  keywords = {Statistics - Applications},
  primaryClass = {stat}
}

@book{everitt_finite_1981,
  title = {Finite Mixture Distributions},
  author = {Everitt, Brian and Hand, David J.},
  year = {1981},
  publisher = {{Chapman and Hall}},
  address = {{London New York}},
  isbn = {0-412-22420-8}
}

@article{finke_efficient_2019,
  title = {Efficient Sequential {{Monte Carlo}} Algorithms for Integrated Population Models},
  author = {Finke, Axel and King, Ruth and Beskos, Alexandros and Dellaportas, Petros},
  year = {2019},
  month = jun,
  volume = {24},
  pages = {204--224},
  issn = {1085-7117, 1537-2693},
  doi = {10.1007/s13253-018-00349-9},
  file = {/Users/amanderson/Zotero/storage/AYUCXSYC/Finke et al. - 2019 - Efficient Sequential Monte Carlo Algorithms for In.pdf},
  journal = {Journal of Agricultural, Biological and Environmental Statistics},
  language = {en},
  number = {2}
}

@misc{gabry_bayesplot_2021,
  title = {Bayesplot: Plotting for {{Bayesian}} Models},
  shorttitle = {Bayesplot},
  author = {Gabry, Jonah and Mahr, Tristan and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin and Barrett, Malcolm and Weber, Frank and Sroka, Eduardo Coronado and Vehtari, Aki},
  year = {2021},
  month = jan,
  abstract = {Plotting functions for posterior analysis, MCMC diagnostics, prior and posterior predictive checks, and other visualizations to support the applied Bayesian workflow advocated in Gabry, Simpson, Vehtari, Betancourt, and Gelman (2019) {$<$}doi:10.1111/rssa.12378{$>$}. The package is designed not only to provide convenient functionality for users, but also a common set of functions that can be easily used by developers working on a variety of R packages for Bayesian modeling, particularly (but not exclusively) packages interfacing with 'Stan'.},
  annotation = {R package version 1.8.0},
  copyright = {GPL ({$\geq$} 3)}
}

@article{gabry_visualization_2019,
  ids = {gabry\_visualization\_2019-1},
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  volume = {182},
  pages = {389--402},
  doi = {10.1111/rssa.12378},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12378},
  file = {/Users/amanderson/Zotero/storage/35VTLRD6/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf;/Users/amanderson/Zotero/storage/AWC6XBUB/rssa.html},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  keywords = {Bayesian data analysis,Statistical graphics,Statistical workflow},
  number = {2}
}

@article{gelman_bayesian_2020,
  title = {Bayesian Workflow},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archivePrefix = {arXiv},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/DAWRLSZG/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/amanderson/Zotero/storage/UYYUCBBU/2011.html},
  journal = {arXiv:2011.01808 [stat]},
  keywords = {Statistics - Methodology},
  note = {Comment: 77 pages, 35 figures},
  primaryClass = {stat}
}

@article{genest_allocating_1990,
  title = {Allocating the Weights in the Linear Opinion Pool},
  author = {Genest, Christian and McConway, Kevin J.},
  year = {1990},
  volume = {9},
  pages = {53--73},
  doi = {10.1002/for.3980090106},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.3980090106},
  file = {/Users/amanderson/Zotero/storage/HD539YPQ/Genest and McConway - 1990 - Allocating the weights in the linear opinion pool.pdf},
  journal = {Journal of Forecasting},
  number = {1}
}

@article{goudie_joining_2019,
  title = {Joining and Splitting Models with {{Markov}} Melding},
  author = {Goudie, Robert J. B. and Presanis, Anne M. and Lunn, David and De Angelis, Daniela and Wernisch, Lorenz},
  year = {2019},
  month = mar,
  volume = {14},
  pages = {81--109},
  issn = {1936-0975},
  doi = {10.1214/18-BA1104},
  abstract = {Analysing multiple evidence sources is often feasible only via a modular approach, with separate submodels specified for smaller components of the available evidence. Here we introduce a generic framework that enables fully Bayesian analysis in this setting. We propose a generic method for forming a suitable joint model when joining submodels, and a convenient computational algorithm for fitting this joint model in stages, rather than as a single, monolithic model. The approach also enables splitting of large joint models into smaller submodels, allowing inference for the original joint model to be conducted via our multi-stage algorithm. We motivate and demonstrate our approach through two examples: joining components of an evidence synthesis of A/H1N1 influenza, and splitting a large ecology model.},
  file = {/Users/amanderson/Zotero/storage/YNN4BGDS/Goudie et al. - 2019 - Joining and Splitting Models with Markov Melding.pdf},
  journal = {Bayesian Analysis},
  language = {en},
  number = {1}
}

@article{jacob_better_2017-1,
  title = {Better Together? {{Statistical}} Learning in Models Made of Modules},
  shorttitle = {Better Together?},
  author = {Jacob, Pierre E. and Murray, Lawrence M. and Holmes, Chris C. and Robert, Christian P.},
  year = {2017},
  month = aug,
  abstract = {In modern applications, statisticians are faced with integrating heterogeneous data modalities relevant for an inference, prediction, or decision problem. In such circumstances, it is convenient to use a graphical model to represent the statistical dependencies, via a set of connected "modules", each relating to a specific data modality, and drawing on specific domain expertise in their development. In principle, given data, the conventional statistical update then allows for coherent uncertainty quantification and information propagation through and across the modules. However, misspecification of any module can contaminate the estimate and update of others, often in unpredictable ways. In various settings, particularly when certain modules are trusted more than others, practitioners have preferred to avoid learning with the full model in favor of approaches that restrict the information propagation between modules, for example by restricting propagation to only particular directions along the edges of the graph. In this article, we investigate why these modular approaches might be preferable to the full model in misspecified settings. We propose principled criteria to choose between modular and full-model approaches. The question arises in many applied settings, including large stochastic dynamical systems, meta-analysis, epidemiological models, air pollution models, pharmacokinetics-pharmacodynamics, and causal inference with propensity scores.},
  archivePrefix = {arXiv},
  eprint = {1708.08719},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/X2U8ZX22/Jacob et al. - 2017 - Better together Statistical learning in models ma.pdf;/Users/amanderson/Zotero/storage/HEMHXPPF/1708.html},
  journal = {arXiv:1708.08719 [stat]},
  keywords = {Statistics - Methodology},
  note = {Comment: 31 pages, 10 figures, 3 tables},
  primaryClass = {stat}
}

@misc{kay_tidybayes_2020,
  title = {Tidybayes: Tidy Data and Geoms for {{Bayesian}} Models},
  author = {Kay, Matthew},
  year = {2020},
  doi = {10.5281/zenodo.1308151},
  note = {R package version 2.0.2}
}

@article{lauritzen_chain_2002,
  title = {Chain Graph Models and Their Causal Interpretations},
  author = {Lauritzen, Steffen L. and Richardson, Thomas S.},
  year = {2002},
  volume = {64},
  pages = {321--348},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00340},
  abstract = {Chain graphs are a natural generalization of directed acyclic graphs and undirected graphs. However, the apparent simplicity of chain graphs belies the subtlety of the conditional independence hypotheses that they represent. There are many simple and apparently plausible, but ultimately fallacious, interpretations of chain graphs that are often invoked, implicitly or explicitly. These interpretations also lead to flawed methods for applying background knowledge to model selection. We present a valid interpretation by showing how the distribution corresponding to a chain graph may be generated from the equilibrium distributions of dynamic models with feed-back. These dynamic interpretations lead to a simple theory of intervention, extending the theory developed for directed acyclic graphs. Finally, we contrast chain graph models under this interpretation with simultaneous equation models which have traditionally been used to model feed-back in econometrics.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00340},
  file = {/Users/amanderson/Zotero/storage/73NK848T/Lauritzen and Richardson - 2002 - Chain graph models and their causal interpretation.pdf;/Users/amanderson/Zotero/storage/HV4A7QIK/Lauritzen and Richardson - 2002 - Chain graph models and their causal interpretation.pdf;/Users/amanderson/Zotero/storage/XMNL77TT/Lauritzen and Richardson - 2002 - Chain graph models and their causal interpretation.pdf;/Users/amanderson/Zotero/storage/4INWW3JH/1467-9868.html;/Users/amanderson/Zotero/storage/5DR6FZVP/1467-9868.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Causal model,Chain graph,Feed-back system,Gibbs sampler,Intervention theory,Structural equation model},
  language = {en},
  number = {3}
}

@article{lee_bayesian_2020,
  title = {Bayesian Joint Inference for Multiple Directed Acyclic Graphs},
  author = {Lee, Kyoungjae and Cao, Xuan},
  year = {2020},
  month = aug,
  abstract = {In many applications, data often arise from multiple groups that may share similar characteristics. A joint estimation method that models several groups simultaneously can be more efficient than estimating parameters in each group separately. We focus on unraveling the dependence structures of data based on directed acyclic graphs and propose a Bayesian joint inference method for multiple graphs. To encourage similar dependence structures across all groups, a Markov random field prior is adopted. We establish the joint selection consistency of the fractional posterior in high dimensions, and benefits of the joint inference are shown under the common support assumption. This is the first Bayesian method for joint estimation of multiple directed acyclic graphs. The performance of the proposed method is demonstrated using simulation studies, and it is shown that our joint inference outperforms other competitors. We apply our method to an fMRI data for simultaneously inferring multiple brain functional networks.},
  archivePrefix = {arXiv},
  eprint = {2008.06190},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/YN5ME8EN/Lee and Cao - 2020 - Bayesian joint inference for multiple directed acy.pdf;/Users/amanderson/Zotero/storage/XGGRR3I6/2008.html},
  journal = {arXiv:2008.06190 [stat]},
  keywords = {Statistics - Methodology},
  primaryClass = {stat}
}

@article{li_generalized_2020,
  title = {Generalized Liquid Association Analysis for Multimodal Data Integration},
  author = {Li, Lexin and Zeng, Jing and Zhang, Xin},
  year = {2020},
  month = aug,
  abstract = {Multimodal data are now prevailing in scientific research. A central question in multimodal integrative analysis is to understand how two data modalities associate and interact with each other given another modality or demographic covariates. The problem can be formulated as studying the associations among three sets of random variables, a question that has received relatively less attention in the literature. In this article, we propose a novel generalized liquid association analysis method, which offers a new and unique angle to this important class of problem of studying three-way associations. We extend the notion of liquid association of Li (2002) from the univariate setting to the multivariate and high-dimensional setting. We establish a population dimension reduction model, transform the problem to sparse Tucker decomposition of a three-way tensor, and develop a higher-order singular value decomposition estimation algorithm. We derive the non-asymptotic error bound and asymptotic consistency of the proposed estimator, while allowing the variable dimensions to be larger than and diverge with the sample size. We demonstrate the efficacy of the method through both simulations and a multimodal neuroimaging application for Alzheimer's disease research.},
  archivePrefix = {arXiv},
  eprint = {2008.03733},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/ICGDSL4B/Li et al. - 2020 - Generalized Liquid Association Analysis for Multim.pdf;/Users/amanderson/Zotero/storage/3FEV58QX/2008.html},
  journal = {arXiv:2008.03733 [stat]},
  keywords = {Statistics - Methodology},
  primaryClass = {stat}
}

@article{lin_recent_2014,
  title = {Recent Developments on the Construction of Bivariate Distributions with Fixed Marginals},
  author = {Lin, Gwo and Dou, Xiaoling and Kuriki, Satoshi and Huang, Jin-Sheng},
  year = {2014},
  volume = {1},
  pages = {14},
  issn = {2195-5832},
  doi = {10.1186/2195-5832-1-14},
  file = {/Users/amanderson/Zotero/storage/6B9NYZX6/Lin et al. - 2014 - Recent developments on the construction of bivaria.pdf;/Users/amanderson/Zotero/storage/FHVWL3LW/Lin et al. - 2014 - Recent developments on the construction of bivaria.pdf},
  journal = {Journal of Statistical Distributions and Applications},
  language = {en},
  number = {1}
}

@article{lunn_bugs_2009,
  title = {The {{BUGS}} Project: Evolution, Critique and Future Directions},
  author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
  year = {2009},
  volume = {28},
  pages = {3049--3067},
  publisher = {{Wiley}},
  doi = {10.1002/sim.3680},
  journal = {Statistics in Medicine},
  number = {25}
}

@article{manderson_numerically_2020,
  title = {A Numerically Stable Algorithm for Integrating {{Bayesian}} Models Using {{Markov}} Melding},
  author = {Manderson, Andrew A. and Goudie, Robert J. B.},
  year = {2020},
  pages = {arXiv:2001.08038},
  annotation = {\_eprint: 2001.08038},
  journal = {arXiv e-prints}
}

@incollection{massa_combining_2010,
  title = {Combining Statistical Models},
  booktitle = {Algebraic Methods in Statistics and Probability {{II}}},
  author = {Massa, M. Sofia and Lauritzen, Steffen L.},
  year = {2010},
  volume = {516},
  pages = {239--259},
  publisher = {{Amer. Math. Soc., Providence, RI}},
  doi = {10.1090/conm/516/10179},
  mrnumber = {2730753},
  series = {Contemp. {{Math}}.}
}

@article{mauff_joint_2020,
  title = {Joint Models with Multiple Longitudinal Outcomes and a Time-to-Event Outcome: A Corrected Two-Stage Approach},
  shorttitle = {Joint Models with Multiple Longitudinal Outcomes and a Time-to-Event Outcome},
  author = {Mauff, Katya and Steyerberg, Ewout and Kardys, Isabella and Boersma, Eric and Rizopoulos, Dimitris},
  year = {2020},
  month = jul,
  volume = {30},
  pages = {999--1014},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-020-09927-9},
  abstract = {Joint models for longitudinal and survival data have gained a lot of attention in recent years, with the development of myriad extensions to the basic model, including those which allow for multivariate longitudinal data, competing risks and recurrent events. Several software packages are now also available for their implementation. Although mathematically straightforward, the inclusion of multiple longitudinal outcomes in the joint model remains computationally difficult due to the large number of random effects required, which hampers the practical application of this extension. We present a novel approach that enables the fitting of such models with more realistic computational times. The idea behind the approach is to split the estimation of the joint model in two steps; estimating a multivariate mixed model for the longitudinal outcomes, and then using the output from this model to fit the survival submodel. So called two-stage approaches have previously been proposed, and shown to be biased. Our approach differs from the standard version, in that we additionally propose the application of a correction factor, adjusting the estimates obtained such that they more closely resemble those we would expect to find with the multivariate joint model. This correction is based on importance sampling ideas. Simulation studies show that this corrected-two-stage approach works satisfactorily, eliminating the bias while maintaining substantial improvement in computational time, even in more difficult settings.},
  archivePrefix = {arXiv},
  eprint = {1808.07719},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/VXCHKAR6/Mauff et al. - 2020 - Joint models with multiple longitudinal outcomes a.pdf;/Users/amanderson/Zotero/storage/YWYA6VVZ/Mauff et al. - 2020 - Joint Models with Multiple Longitudinal Outcomes a.pdf;/Users/amanderson/Zotero/storage/BK8DARLX/1808.html},
  journal = {Statistics and Computing},
  keywords = {read,Statistics - Methodology},
  note = {Comment: 33 pages, 7 figures and 7 tables including appendices. Accepted in Statistics and Computing},
  number = {4}
}

@article{moller_efficient_2006,
  title = {An Efficient {{Markov}} Chain {{Monte Carlo}} Method for Distributions with Intractable Normalising Constants},
  author = {M{\o}ller, J. and Pettitt, A. N. and Reeves, R. and Berthelsen, K. K.},
  year = {2006},
  volume = {93},
  pages = {451--458},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  abstract = {Maximum likelihood parameter estimation and sampling from Bayesian posterior distributions are problematic when the probability density for the parameter of interest involves an intractable normalising constant which is also a function of that parameter. In this paper, an auxiliary variable method is presented which requires only that independent samples can be drawn from the unnormalised density at any particular parameter value. The proposal distribution is constructed so that the normalising constant cancels from the Metropolis-Hastings ratio. The method is illustrated by producing posterior samples for parameters of the Ising model given a particular lattice realisation.},
  file = {/Users/amanderson/Zotero/storage/XN3V45KY/Møller et al. - 2006 - An efficient Markov chain Monte Carlo method for d.pdf;/Users/amanderson/Zotero/storage/9VN6E8KG/221054.html},
  journal = {Biometrika},
  number = {2}
}

@article{murray_mcmc_nodate,
  title = {{{MCMC}} for Doubly-Intractable Distributions},
  author = {Murray, Iain and Ghahramani, Zoubin and MacKay, David J C},
  pages = {8},
  abstract = {Markov Chain Monte Carlo (MCMC) algorithms are routinely used to draw samples from distributions with intractable normalization constants. However, standard MCMC algorithms do not apply to doublyintractable distributions in which there are additional parameter-dependent normalization terms; for example, the posterior over parameters of an undirected graphical model. An ingenious auxiliary-variable scheme (M\o ller et al., 2004) offers a solution: exact sampling (Propp and Wilson, 1996) is used to sample from a Metropolis\textendash Hastings proposal for which the acceptance probability is tractable. Unfortunately the acceptance probability of these expensive updates can be low. This paper provides a generalization of M\o ller et al. (2004) and a new MCMC algorithm, which obtains better acceptance probabilities for the same amount of exact sampling, and removes the need to estimate model parameters before sampling begins.},
  file = {/Users/amanderson/Zotero/storage/32PE3XU7/Murray et al. - MCMC for doubly-intractable distributions.pdf;/Users/amanderson/Zotero/storage/TU5V94QP/Murray et al. - MCMC for doubly-intractable distributions.pdf},
  language = {en}
}

@book{nelsen_introduction_2006,
  title = {An Introduction to Copulas},
  author = {Nelsen, Roger B.},
  year = {2006},
  edition = {Second},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28678-0},
  file = {/Users/amanderson/Zotero/storage/C83TRFIW/Nelsen - 2006 - An introduction to copulas.pdf},
  keywords = {Copulas (Mathematical statistics)}
}

@misc{nimble_development_team_nimble_2019,
  title = {{{NIMBLE}}: {{MCMC}}, Particle Filtering, and Programmable Hierarchical Modeling},
  author = {{NIMBLE Development Team}},
  year = {2019},
  doi = {10.5281/zenodo.1211190},
  note = {R package manual version 0.9.0}
}

@article{oh_considerations_2018,
  title = {Considerations for Analysis of Time-to-Event Outcomes Measured with Error: Bias and Correction with {{SIMEX}}},
  shorttitle = {Considerations for Analysis of Time-to-Event Outcomes Measured with Error},
  author = {Oh, Eric J. and Shepherd, Bryan E. and Lumley, Thomas and Shaw, Pamela A.},
  year = {2018},
  month = apr,
  volume = {37},
  pages = {1276--1289},
  issn = {0277-6715},
  doi = {10.1002/sim.7554},
  abstract = {For time-to-event outcomes, a rich literature exists on the bias introduced by covariate measurement error in regression models, such as the Cox model, and methods of analysis to address this bias. By comparison, less attention has been given to understanding the impact or addressing errors in the failure time outcome. For many diseases, the timing of an event of interest (such as progression-free survival or time to AIDS progression) can be difficult to assess or reliant on self-report and therefore prone to measurement error. For linear models, it is well known that random errors in the outcome variable do not bias regression estimates. With non-linear models, however, even random error or misclassification can introduce bias into estimated parameters. We compare the performance of two common regression models, the Cox and Weibull models, in the setting of measurement error in the failure time outcome. We introduce an extension of the SIMEX method to correct for bias in hazard ratio estimates from the Cox model and discuss other analysis options to address measurement error in the response. A formula to estimate the bias induced into the hazard ratio by classical measurement error in the event time for a log-linear survival model is presented. Detailed numerical studies are presented to examine the performance of the proposed SIMEX method under varying levels and parametric forms of the error in the outcome. We further illustrate the method with observational data on HIV outcomes from the Vanderbilt Comprehensive Care Clinic.},
  file = {/Users/amanderson/Zotero/storage/8TKZG2R2/Oh et al. - 2018 - Considerations for analysis of time-to-event outco.pdf;/Users/amanderson/Zotero/storage/SSEHCERZ/Oh et al. - 2018 - Considerations for Analysis of Time-to-Event Outco.pdf;/Users/amanderson/Zotero/storage/4IQ5GI3G/sim.html},
  journal = {Statistics in medicine},
  keywords = {accelerated failure time,Cox model,measurement error,SIMEX,survival analysis},
  number = {8},
  pmcid = {PMC5810403},
  pmid = {29193180}
}

@book{ohagan_uncertain_2006,
  title = {Uncertain Judgements: Eliciting Experts' Probabilities},
  author = {O'Hagan, A. and Buck, C.E. and Daneshkhah, A. and Eiser, J.R. and Garthwaite, P.H. and Jenkinson, D.J. and Oakley, J.E. and Rakow, T.},
  year = {2006},
  publisher = {{Wiley}},
  doi = {10.1002/0470033312},
  file = {/Users/amanderson/Zotero/storage/C8CJ2XL5/O'Hagan - 2006 - Uncertain judgements eliciting experts' probabili.pdf},
  isbn = {978-0-470-03330-2},
  keywords = {Bayesian statistical decision theory,Distribution (Probability theory),Mathematical statistics,Probabilities,Statistics},
  series = {Statistics in {{Practice}}}
}

@article{parsons_evaluating_2020,
  title = {Evaluating the Relative Contribution of Data Sources in a {{Bayesian}} Analysis with the Application of Estimating the Size of Hard to Reach Populations},
  author = {Parsons, Jacob and Niu, Xiaoyue and Bao, Le},
  year = {2020},
  month = sep,
  abstract = {When using multiple data sources in an analysis, it is important to understand the influence of each data source on the analysis and the consistency of the data sources with each other and the model. We suggest the use of a retrospective value of information framework in order to address such concerns. Value of information methods can be computationally difficult. We illustrate the use of computational methods that allow these methods to be applied even in relatively complicated settings. In illustrating the proposed methods, we focus on an application in estimating the size of hard to reach populations. Specifically, we consider estimating the number of injection drug users in Ukraine by combining all available data sources spanning over half a decade and numerous sub-national areas in the Ukraine. This application is of interest to public health researchers as this hard to reach population that plays a large role in the spread of HIV. We apply a Bayesian hierarchical model and evaluate the contribution of each data source in terms of absolute influence, expected influence, and level of surprise. Finally we apply value of information methods to inform suggestions on future data collection.},
  archivePrefix = {arXiv},
  eprint = {2009.02372},
  eprinttype = {arxiv},
  file = {/Users/amanderson/Zotero/storage/3H3BFRK2/Parsons et al. - 2020 - Evaluating the relative contribution of data sourc.pdf;/Users/amanderson/Zotero/storage/NC8EAYR9/2009.html},
  journal = {arXiv:2009.02372 [stat]},
  keywords = {Statistics - Applications},
  note = {Comment: 24 pages, 7 figures, 2 tables},
  primaryClass = {stat}
}

@misc{plummer_rjags_2018,
  title = {Rjags: {{Bayesian}} Graphical Models Using {{MCMC}}},
  author = {Plummer, Martyn},
  year = {2018},
  annotation = {R package version 4-10},
  note = {R package version 4-10}
}

@article{richardson_statistical_2016,
  title = {Statistical Methods in Integrative Genomics},
  author = {Richardson, Sylvia and Tseng, George C. and Sun, Wei},
  year = {2016},
  month = jun,
  volume = {3},
  pages = {181--209},
  issn = {2326-8298},
  doi = {10.1146/annurev-statistics-041715-033506},
  abstract = {Statistical methods in integrative genomics aim to answer important biology questions by jointly analyzing multiple types of genomic data (vertical integration) or aggregating the same type of data across multiple studies (horizontal integration). In this article, we introduce different types of genomic data and data resources, and then review statistical methods of integrative genomics, with emphasis on the motivation and rationale of these methods. We conclude with some summary points and future research directions.},
  file = {/Users/amanderson/Zotero/storage/PM7Y4PJ7/Richardson et al. - 2016 - Statistical Methods in Integrative Genomics.pdf},
  journal = {Annual Review of Statistics and Its Application},
  keywords = {genomics,horizontal data integration,integrative genomics,vertical data integration},
  language = {eng},
  pmcid = {PMC4963036},
  pmid = {27482531}
}

@article{schaub_local_2006,
  title = {Local Population Dynamics and the Impact of Scale and Isolation: A Study on Different Little Owl Populations},
  shorttitle = {Local Population Dynamics and the Impact of Scale and Isolation},
  author = {Schaub, Michael and Ullrich, Bruno and Kn{\"o}tzsch, Gerhard and Albrecht, Patrick and Meisser, Christian},
  year = {2006},
  volume = {115},
  pages = {389--400},
  issn = {1600-0706},
  doi = {10.1111/j.2006.0030-1299.15374.x},
  abstract = {The understanding of how variation of demographic rates translates into variation of population growth is a central aim in population ecology. Besides stochastic and deterministic factors, the spatial extent and the isolation of a local population may have an impact on the contribution of the different demographic components. Using long-term demographic data we performed retrospective population analyses of four little owl (Athene noctua) populations with differential spatial extent and degree of isolation to assess the contribution of demographic rates to the variation of the growth rate ({$\lambda$}) of each local population and to the difference of {$\lambda$} among populations. In all populations variation of fecundity contributed least to variation of {$\lambda$}, and variation of adult survival contributed most to variation of {$\lambda$} in three of four populations. Between population comparisons revealed that differences mainly stem from differences of immigration and juvenile local survival. The relative importance of immigration to {$\lambda$} tended to decrease with increasing spatial extent and isolation of the local populations. None of the four local populations was self-sustainable. Because the local populations export and import individuals, they can be considered as open recruitment systems in which part of the recruited breeding birds are not produced locally. The spatial extent and the degree of isolation of a local population have an impact on local population dynamics; hence these factors need to be considered in studies about local population dynamics and for deriving conservation measures.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2006.0030-1299.15374.x},
  file = {/Users/amanderson/Zotero/storage/8XVKMBMY/Schaub et al. - 2006 - Local population dynamics and the impact of scale .pdf;/Users/amanderson/Zotero/storage/PNG6H748/j.2006.0030-1299.15374.html;/Users/amanderson/Zotero/storage/RWRAPQJ7/j.2006.0030-1299.15374.html},
  journal = {Oikos},
  language = {en},
  number = {3}
}

@misc{stan_development_team_rstan_2020,
  title = {{{RStan}}: The {{R}} Interface to {{Stan}}},
  author = {{Stan Development Team}},
  year = {2020},
  annotation = {R package version 2.21.3},
  note = {R package version 2.21.3}
}

@article{vehtari_rank-normalization_2020-1,
  title = {Rank-Normalization, Folding, and Localization: An Improved \$\textbackslash widehat\{\vphantom\}{{R}}\vphantom\{\}\$ for Assessing Convergence of {{MCMC}}},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2020},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic R\textasciicircum R\textasciicircum{$<$}math alttext="\$\textbackslash widehat\{R\}\$" overflow="scroll"{$>$} {$<$}mover accent="true"{$>$} {$<$}mrow{$>$} {$<$}mi{$>$}R{$<$}/mi{$>$} {$<$}/mrow{$>$} {$<$}mrow{$>$} {$<$}mo{$>$}\textasciicircum{$<$}/mo{$>$} {$<$}/mrow{$>$} {$<$}/mover{$>$} {$<$}/math{$>$} of Gelman and Rubin (1992) has serious flaws. Traditional R\textasciicircum R\textasciicircum{$<$}math alttext="\$\textbackslash widehat\{R\}\$" overflow="scroll"{$>$} {$<$}mover accent="true"{$>$} {$<$}mrow{$>$} {$<$}mi{$>$}R{$<$}/mi{$>$} {$<$}/mrow{$>$} {$<$}mrow{$>$} {$<$}mo{$>$}\textasciicircum{$<$}/mo{$>$} {$<$}/mrow{$>$} {$<$}/mover{$>$} {$<$}/math{$>$} will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/Users/amanderson/Zotero/storage/3F4IM86A/Vehtari et al. - 2020 - Rank-Normalization, Folding, and Localization An .pdf;/Users/amanderson/Zotero/storage/KS95FEQ4/1593828229.html},
  journal = {Bayesian Analysis},
  language = {EN}
}

@article{wang_integrative_2020,
  title = {Integrative Survival Analysis with Uncertain Event Times in Application to a Suicide Risk Study},
  author = {Wang, Wenjie and Aseltine, Robert and Chen, Kun and Yan, Jun},
  year = {2020},
  month = mar,
  volume = {14},
  pages = {51--73},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/19-AOAS1287},
  abstract = {The concept of integrating data from disparate sources to accelerate scientific discovery has generated tremendous excitement in many fields. The potential benefits from data integration, however, may be compromised by the uncertainty due to incomplete/imperfect record linkage. Motivated by a suicide risk study, we propose an approach for analyzing survival data with uncertain event times arising from data integration. Specifically, in our problem deaths identified from the hospital discharge records together with reported suicidal deaths determined by the Office of Medical Examiner may still not include all the death events of patients, and the missing deaths can be recovered from a complete database of death records. Since the hospital discharge data can only be linked to the death record data by matching basic patient characteristics, a patient with a censored death time from the first dataset could be linked to multiple potential event records in the second dataset. We develop an integrative Cox proportional hazards regression in which the uncertainty in the matched event times is modeled probabilistically. The estimation procedure combines the ideas of profile likelihood and the expectation conditional maximization algorithm (ECM). Simulation studies demonstrate that under realistic settings of imperfect data linkage the proposed method outperforms several competing approaches including multiple imputation. A marginal screening analysis using the proposed integrative Cox model is performed to identify risk factors associated with death following suicide-related hospitalization in Connecticut. The identified diagnostics codes are consistent with existing literature and provide several new insights on suicide risk, prediction and prevention.},
  file = {/Users/amanderson/Zotero/storage/I68DWIUP/Wang et al. - 2020 - Integrative survival analysis with uncertain event.pdf;/Users/amanderson/Zotero/storage/RFQAFZII/1587002664.html},
  journal = {Annals of Applied Statistics},
  keywords = {Cox model,data linkage,ECM algorithm,integrative learning,suicide prevention},
  language = {EN},
  mrnumber = {MR4085083},
  number = {1},
  zmnumber = {07200161}
}

@article{xue_double-parallel_2019,
  title = {Double-Parallel {{Monte Carlo}} for {{Bayesian}} Analysis of Big Data},
  author = {Xue, Jingnan and Liang, Faming},
  year = {2019},
  month = jan,
  volume = {29},
  pages = {23--32},
  issn = {1573-1375},
  doi = {10.1007/s11222-017-9791-1},
  abstract = {This paper proposes a simple, practical, and efficient MCMC algorithm for Bayesian analysis of big data. The proposed algorithm suggests to divide the big dataset into some smaller subsets and provides a simple method to aggregate the subset posteriors to approximate the full data posterior. To further speed up computation, the proposed algorithm employs the population stochastic approximation Monte Carlo algorithm, a parallel MCMC algorithm, to simulate from each subset posterior. Since this algorithm consists of two levels of parallel, data parallel and simulation parallel, it is coined as ``Double-Parallel Monte Carlo.'' The validity of the proposed algorithm is justified mathematically and numerically.},
  file = {/Users/amanderson/Zotero/storage/DFYKBFHP/Xue and Liang - 2019 - Double-Parallel Monte Carlo for Bayesian analysis .pdf},
  journal = {Statistics and Computing},
  language = {en},
  number = {1}
}

@article{zhang_fully_2020,
  title = {Fully {{Bayesian}} Benchmarking of Small Area Estimation Models},
  author = {Zhang, Junni L. and Bryant, John},
  year = {2020},
  volume = {36},
  pages = {197--223},
  publisher = {{Sciendo}},
  address = {{Berlin}},
  doi = {10.2478/jos-2020-0010},
  file = {/Users/amanderson/Zotero/storage/A3SGADXA/jos-2020-0010_SM.pdf;/Users/amanderson/Zotero/storage/YTMZUKAM/Zhang and Bryant - 2020 - Fully Bayesian Benchmarking of Small Area Estimati.pdf},
  journal = {Journal of Official Statistics},
  keywords = {read},
  number = {1}
}

@article{zipkin_synthesizing_2018,
  title = {Synthesizing Multiple Data Types for Biological Conservation Using Integrated Population Models},
  author = {Zipkin, Elise F. and Saunders, Sarah P.},
  year = {2018},
  month = jan,
  volume = {217},
  pages = {240--250},
  issn = {00063207},
  doi = {10.1016/j.biocon.2017.10.017},
  abstract = {Assessing the impacts of ongoing climate and anthropogenic-induced change on wildlife populations requires understanding species distributions and abundances across large spatial and temporal scales. For threatened or declining populations, collecting sufficient broad-scale data is challenging as sample sizes tend to be low because many such species are rare and/or elusive. As a result, demographic data are often piecemeal, leading to difficulties in determining causes of population changes and developing strategies to mitigate the effects of environmental stressors. Thus, the population dynamics of threatened species across spatio-temporal extents is typically inferred through incomplete, independent, local-scale studies. Emerging integrative modeling approaches, such as integrated population models (IPMs), combine multiple data types into a single analysis and provide a foundation for overcoming problems of sparse or fragmentary data. In this paper, we demonstrate how IPMs can be successfully implemented by synthesizing the elements, advantages, and novel insights of this modeling approach. We highlight the latest developments in IPMs that are explicitly relevant to the ecology and conservation of threatened species, including capabilities to quantify the spatial scale of management, sourcesink dynamics, synchrony within metapopulations, and population density effects on demographic rates. Adoption of IPMs has led to improved detection of population declines, adaptation of targeted monitoring schemes, and refined management strategies. Continued methodological advancements of IPMs, such as incorporation of a wider set of data types (e.g., citizen science data) and coupled population-environment models, will allow for broader applicability within ecological and conservation sciences.},
  file = {/Users/amanderson/Zotero/storage/KXSQPIPH/Zipkin and Saunders - 2018 - Synthesizing multiple data types for biological co.pdf},
  journal = {Biological Conservation},
  language = {en}
}




@article{abadi_estimation_2010,
  title = {Estimation of Immigration Rate Using Integrated Population Models},
  author = {Abadi, Fitsum and Gimenez, Olivier and Ullrich, Bruno and Arlettaz, Rapha{\"e}l and Schaub, Michael},
  year = {2010},
  journal = {Journal of Applied Ecology},
  volume = {47},
  number = {2},
  pages = {393--400},
  doi = {10.1111/j.1365-2664.2010.01789.x},
  annotation = {{$\_$}eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2664.2010.01789.x},
  file = {/Users/amanderson/Zotero/storage/6BF7EPE3/Abadi et al. - 2010 - Estimation of immigration rate using integrated po.pdf}
}

@article{abbas_kullback-leibler_2009,
  title = {A {{Kullback-Leibler}} View of Linear and Log-Linear Pools},
  author = {Abbas, Ali E.},
  year = {2009},
  month = feb,
  journal = {Decision Analysis},
  publisher = {{INFORMS}},
  doi = {10.1287/deca.1080.0133},
  abstract = {Linear and log-linear pools are widely used methods for aggregating expert belief. This paper frames the expert aggregation problem as a decision problem with scoring rules. We propose a scoring fu...},
  copyright = {Copyright \textcopyright{} 2009, INFORMS},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/IYZUR6DF/Abbas - 2009 - A Kullback-Leibler View of Linear and Log-Linear P.pdf;/Users/amanderson/Zotero/storage/95BPIP75/deca.1080.html}
}

@article{ades_multiparameter_2006,
  ids = {ades:sutton:06,ades_multiparameter_2006-1},
  title = {Multiparameter Evidence Synthesis in Epidemiology and Medical Decision-Making: Current Approaches},
  shorttitle = {Multiparameter Evidence Synthesis in Epidemiology and Medical Decision-Making},
  author = {Ades, A. E. and Sutton, A. J.},
  year = {2006},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {169},
  number = {1},
  pages = {5--35},
  issn = {1467-985X},
  doi = {10.1111/j.1467-985X.2005.00377.x},
  abstract = {Summary. Alongside the development of meta-analysis as a tool for summarizing research literature, there is renewed interest in broader forms of quantitative synthesis that are aimed at combining evidence from different study designs or evidence on multiple parameters. These have been proposed under various headings: the confidence profile method, cross-design synthesis, hierarchical models and generalized evidence synthesis. Models that are used in health technology assessment are also referred to as representing a synthesis of evidence in a mathematical structure. Here we review alternative approaches to statistical evidence synthesis, and their implications for epidemiology and medical decision-making. The methods include hierarchical models, models informed by evidence on different functions of several parameters and models incorporating both of these features. The need to check for consistency of evidence when using these powerful methods is emphasized. We develop a rationale for evidence synthesis that is based on Bayesian decision modelling and expected value of information theory, which stresses not only the need for a lack of bias in estimates of treatment effects but also a lack of bias in assessments of uncertainty. The increasing reliance of governmental bodies like the UK National Institute for Clinical Excellence on complex evidence synthesis in decision modelling is discussed.},
  langid = {english},
  keywords = {Bayesian methods,Decision-making,Evidence synthesis,Hierarchical models},
  annotation = {{$\_$}eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-985X.2005.00377.x},
  file = {/Users/amanderson/Zotero/storage/PDK9DSMJ/Ades and Sutton - 2006 - Multiparameter evidence synthesis in epidemiology .pdf;/Users/amanderson/Zotero/storage/GS9CP6IB/j.1467-985X.2005.00377.html;/Users/amanderson/Zotero/storage/IDDGVM8M/j.1467-985X.2005.00377.html}
}

@article{alvares_tractable_2021,
  title = {A Tractable {{Bayesian}} Joint Model for Longitudinal and Survival Data},
  author = {Alvares, Danilo and Rubio, Francisco Javier},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.10906 [stat]},
  eprint = {2104.10906},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We introduce a numerically tractable formulation of Bayesian joint models for longitudinal and survival data. The longitudinal process is modelled using generalised linear mixed models, while the survival process is modelled using a parametric general hazard structure. The two processes are linked by sharing fixed and random effects, separating the effects that play a role at the time scale from those that affect the hazard scale. This strategy allows for the inclusion of non-linear and time-dependent effects while avoiding the need for numerical integration, which facilitates the implementation of the proposed joint model. We explore the use of flexible parametric distributions for modelling the baseline hazard function which can capture the basic shapes of interest in practice. We discuss prior elicitation based on the interpretation of the parameters. We present an extensive simulation study, where we analyse the inferential properties of the proposed models, and illustrate the trade-off between flexibility, sample size, and censoring. We also apply our proposal to two real data applications in order to demonstrate the adaptability of our formulation both in univariate time-to-event data and in a competing risks framework. The methodology is implemented in rstan.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  note = {Comment: To appear in Statistics in Medicine. Software available at https://github.com/daniloalvares/Tractable-BJM},
  file = {/Users/amanderson/Zotero/storage/FSHQ6TZR/Alvares and Rubio - 2021 - A tractable Bayesian joint model for longitudinal .pdf;/Users/amanderson/Zotero/storage/2VHZCUFZ/2104.html}
}

@misc{belgorodski_rriskdistributions_2017,
  title = {{{rriskDistributions}}: {{Fitting}} Distributions to given Data or Known Quantiles},
  author = {Belgorodski, Natalia and Greiner, Matthias and Tolksdorf, Kristin and Schueller, Katharina},
  year = {2017},
  note = {R package version 2.1.2}
}

@article{beyersmann_simulating_2009,
  title = {Simulating Competing Risks Data in Survival Analysis},
  author = {Beyersmann, Jan and Latouche, Aur{\'e}lien and Buchholz, Anika and Schumacher, Martin},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {6},
  pages = {956--971},
  issn = {1097-0258},
  doi = {10.1002/sim.3516},
  abstract = {Competing risks analysis considers time-to-first-event (`survival time') and the event type (`cause'), possibly subject to right-censoring. The cause-, i.e. event-specific hazards, completely determine the competing risk process, but simulation studies often fall back on the much criticized latent failure time model. Cause-specific hazard-driven simulation appears to be the exception; if done, usually only constant hazards are considered, which will be unrealistic in many medical situations. We explain simulating competing risks data based on possibly time-dependent cause-specific hazards. The simulation design is as easy as any other, relies on identifiable quantities only and adds to our understanding of the competing risks process. In addition, it immediately generalizes to more complex multistate models. We apply the proposed simulation design to computing the least false parameter of a misspecified proportional subdistribution hazard model, which is a research question of independent interest in competing risks. The simulation specifications have been motivated by data on infectious complications in stem-cell transplanted patients, where results from cause-specific hazards analyses were difficult to interpret in terms of cumulative event probabilities. The simulation illustrates that results from a misspecified proportional subdistribution hazard analysis can be interpreted as a time-averaged effect on the cumulative event probability scale. Copyright \textcopyright{} 2009 John Wiley {$\&$} Sons, Ltd.},
  langid = {english},
  keywords = {cause-specific hazard,latent failure time,model misspecification,multistate model,subdistribution hazard},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3516},
  file = {/Users/amanderson/Zotero/storage/JMR4ZN95/Beyersmann et al. - 2009 - Simulating competing risks data in survival analys.pdf;/Users/amanderson/Zotero/storage/W6AV9RMJ/sim.html}
}

@article{bissiri_general_2016,
  title = {A General Framework for Updating Belief Distributions},
  author = {Bissiri, P. G. and Holmes, C. C. and Walker, S. G.},
  year = {2016},
  month = nov,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {78},
  number = {5},
  pages = {1103--1130},
  issn = {13697412},
  doi = {10.1111/rssb.12158},
  abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered as a special case. Modern application areas make it increasingly challenging for Bayesians to attempt to model the true data-generating mechanism. For instance, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our framework uses loss functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.},
  langid = {english},
  keywords = {Decision theory,General Bayesian updating,Generalized estimating equations,Gibbs posteriors,Information,Loss function,Maximum entropy,Provably approximately correct Bayes methods,Self-information loss function},
  file = {/Users/amanderson/Zotero/storage/M3RBG5D8/Bissiri et al. - 2016 - A general framework for updating belief distributi.pdf;/Users/amanderson/Zotero/storage/A3W23SND/rssb.html}
}

@article{blomstedt:etal:19,
  title = {Meta-Analysis of {{Bayesian}} Analyses},
  author = {Blomstedt, Paul and Mesquita, Diego and Lintusaari, Jarno and Sivula, Tuomas and Corander, Jukka and Kaski, Samuel},
  year = {2019},
  journal = {arXiv e-prints},
  eprint = {1904.04484},
  eprinttype = {arxiv},
  primaryclass = {stat.ME},
  pages = {arXiv:1904.04484},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190404484B},
  archiveprefix = {arXiv},
  eid = {arXiv:1904.04484}
}

@article{brilleman_bayesian_2020,
  title = {Bayesian Survival Analysis Using the Rstanarm {{R}} Package},
  author = {Brilleman, Samuel L. and Elci, Eren M. and Novik, Jacqueline Buros and Wolfe, Rory},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.09633 [stat]},
  eprint = {2002.09633},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Survival data is encountered in a range of disciplines, most notably health and medical research. Although Bayesian approaches to the analysis of survival data can provide a number of benefits, they are less widely used than classical (e.g. likelihood-based) approaches. This may be in part due to a relative absence of user-friendly implementations of Bayesian survival models. In this article we describe how the rstanarm R package can be used to fit a wide range of Bayesian survival models. The rstanarm package facilitates Bayesian regression modelling by providing a user-friendly interface (users specify their model using customary R formula syntax and data frames) and using the Stan software (a C++ library for Bayesian inference) for the back-end estimation. The suite of models that can be estimated using rstanarm is broad and includes generalised linear models (GLMs), generalised linear mixed models (GLMMs), generalised additive models (GAMs) and more. In this article we focus only on the survival modelling functionality. This includes standard parametric (exponential, Weibull, Gompertz) and flexible parametric (spline-based) hazard models, as well as standard parametric accelerated failure time (AFT) models. All types of censoring (left, right, interval) are allowed, as is delayed entry (left truncation), time-varying covariates, time-varying effects, and frailty effects. We demonstrate the functionality through worked examples. We anticipate these implementations will increase the uptake of Bayesian survival analysis in applied research.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/IPYBBCQA/Brilleman et al. - 2020 - Bayesian Survival Analysis Using the rstanarm R Pa.pdf;/Users/amanderson/Zotero/storage/U6PAEYNU/2002.html}
}

@misc{brilleman_simsurv_2021,
  title = {Simsurv: {{Simulate}} Survival Data},
  author = {Brilleman, Sam},
  year = {2021},
  month = jan,
  abstract = {Simulate survival times from standard parametric survival distributions (exponential, Weibull, Gompertz), 2-component mixture distributions, or a user-defined hazard, log hazard, cumulative hazard, or log cumulative hazard function. Baseline covariates can be included under a proportional hazards assumption. Time dependent effects (i.e. non-proportional hazards) can be included by interacting covariates with linear time or a user-defined function of time. Clustered event times are also accommodated. The 2-component mixture distributions can allow for a variety of flexible baseline hazard functions reflecting those seen in practice. If the user wishes to provide a user-defined hazard or log hazard function then this is possible, and the resulting cumulative hazard function does not need to have a closed-form solution. For details see the supporting paper {$\&$}lt;{$<$}a href="https://doi.org/10.18637{$\%$}2Fjss.v097.i03"{$>$}doi:10.18637/jss.v097.i03{$<$}/a{$>\&$}gt;. Note that this package is modelled on the 'survsim' package available in the 'Stata' software (see Crowther and Lambert (2012) {$\&$}lt;{$<$}a href="https://www.stata-journal.com/sjpdf.html?articlenum=st0275"{$>$}https://www.stata-journal.com/sjpdf.html?articlenum=st0275{$<$}/a{$>\&$}gt; or Crowther and Lambert (2013) {$\&$}lt;{$<$}a href="https://doi.org/10.1002{$\%$}2Fsim.5823"{$>$}doi:10.1002/sim.5823{$<$}/a{$>\&$}gt;).},
  annotation = {Note: R package version 1.0.0},
  note = {R package version 1.0.0},
  file = {/Users/amanderson/Zotero/storage/KV9ZQUS3/index.html}
}

@article{bromiley_products_2003,
  title = {Products and Convolutions of {{Gaussian}} Probability Density Functions},
  author = {Bromiley, Paul},
  year = {2003},
  journal = {Tina-Vision Memo},
  volume = {3},
  number = {4},
  pages = {1},
  abstract = {It is well known that the product and the convolution of Gaussian probability density functions (PDFs) are also Gaussian functions. This document provides proofs of this for several cases; the product of two univariate Gaussian PDFs, the product of an arbitrary number of univariate Gaussian PDFs, the product of an arbitrary number of multivariate Gaussian PDFs, and the convolution of two univariate Gaussian PDFs. These results are useful in calculating the effects of smoothing applied as an intermediate step in various algorithms.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/47G9IXHQ/Bromiley - Products and Convolutions of Gaussian Probability .pdf}
}

@article{brown_nonlinear_2016-1,
  ids = {brown_nonlinear_2016},
  title = {Nonlinear {{Imputation}} of {{Pao2}}/{{Fio2 From Spo2}}/{{Fio2 Among Patients With Acute Respiratory Distress Syndrome}}},
  author = {Brown, Samuel M. and Grissom, Colin K. and Moss, Marc and Rice, Todd W. and Schoenfeld, David and Hou, Peter C. and Thompson, B. Taylor and Brower, Roy G.},
  year = {2016},
  month = aug,
  journal = {Chest},
  volume = {150},
  number = {2},
  pages = {307--313},
  issn = {0012-3692},
  doi = {10.1016/j.chest.2016.01.003},
  abstract = {Background ARDS is an important clinical problem. The definition of ARDS requires testing of arterial blood gas to define the ratio of Pao2 to Fio2 (Pao2/Fio2 ratio). However, many patients with ARDS do not undergo blood gas measurement, which may result in underdiagnosis of the condition. As a consequence, a method for estimating Pao2 on the basis of noninvasive measurements is desirable. Methods Using data from three ARDS Network studies, we analyzed the enrollment arterial blood gas measurements to compare nonlinear with linear and log-linear imputation methods of estimating Pao2 from percent saturation of hemoglobin with oxygen as measured by pulse oximetry (Spo2). We compared mortality on the basis of various measured and imputed Pao2/Fio2 ratio cutoffs to ensure clinical equivalence. Results We studied 1,184 patients, in 707 of whom the Spo2{$~\leq$} 96{$\%$}. Nonlinear imputation from the Spo2/Fio2 ratio resulted in lower error than linear or log-linear imputation (P{$~<~$}.001) for patients with Spo2{$~\leq$} 96{$\%~$}but was equivalent to log-linear imputation in all patients. Ninety-day hospital mortality was 26{$\%~$}to 30{$\%$}, depending on the Pao2/Fio2 ratio, whether nonlinearly imputed or measured. On multivariate regression, the association between imputed and measured Pao2 varied by use of vasopressors and Spo2. Conclusions A nonlinear equation more accurately imputes Pao2/Fio2 from Spo2/Fio2 than linear or log-linear equations, with similar observed hospital mortality depending on Spo2/Fio2 ratio vs{$~$}measured Pao2/Fio2 ratios. While further refinement through prospective validation is indicated, a nonlinear imputation appears superior to prior approaches to imputation.},
  langid = {english},
  keywords = {acute respiratory distress syndrome,pulse oximetry,respiratory failure,severity scores},
  file = {/Users/amanderson/Zotero/storage/9Z7N82KE/Brown et al. - 2016 - Nonlinear Imputation of Pao2Fio2 From Spo2Fio2 A.pdf;/Users/amanderson/Zotero/storage/LGYLJ2SP/Brown et al. - 2016 - Nonlinear Imputation of Pa o 2 F io 2 From Sp o 2.pdf;/Users/amanderson/Zotero/storage/QL7W4JPG/S001236921600458X.html}
}

@article{burke_meta-analysis_2017,
  title = {Meta-Analysis Using Individual Participant Data: One-Stage and Two-Stage Approaches, and Why They May Differ},
  shorttitle = {Meta-Analysis Using Individual Participant Data},
  author = {Burke, Danielle L. and Ensor, Joie and Riley, Richard D.},
  year = {2017},
  month = feb,
  journal = {Statistics in Medicine},
  volume = {36},
  number = {5},
  pages = {855--875},
  issn = {02776715},
  doi = {10.1002/sim.7141},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/6NBA2XJA/Burke et al. - 2017 - Meta-analysis using individual participant data o.pdf}
}

@inproceedings{carmona_semi-modular_2020,
  title = {Semi-Modular {{Inference}}: Enhanced Learning in Multi-Modular Models by Tempering the Influence of Components},
  shorttitle = {Semi-{{Modular Inference}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Carmona, Christian and Nicholls, Geoff},
  year = {2020},
  month = jun,
  pages = {4226--4235},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Bayesian statistical inference loses predictive optimality when generative models are misspecified.Working within an existing coherent loss-based generalisation of Bayesian inference, we show exist...},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/BSGBPF78/Carmona and Nicholls - 2020 - Semi-Modular Inference enhanced learning in multi.pdf;/Users/amanderson/Zotero/storage/MTGGVZEE/carmona20a.html}
}

@article{carpenter_stan_2017,
  title = {Stan: {{A}} Probabilistic Programming Language},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01}
}

@article{chan_divide-and-conquer_2021,
  title = {Divide-and-Conquer {{Monte Carlo}} Fusion},
  author = {Chan, Ryan S. Y. and Pollock, Murray and Johansen, Adam M. and Roberts, Gareth O.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07265 [stat]},
  eprint = {2110.07265},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Combining several (sample approximations of) distributions, which we term sub-posteriors, into a single distribution proportional to their product, is a common challenge. For instance, in distributed `big data' problems, or when working under multi-party privacy constraints. Many existing approaches resort to approximating the individual sub-posteriors for practical necessity, then representing the resulting approximate posterior. The quality of the posterior approximation for these approaches is poor when the sub-posteriors fall out-with a narrow range of distributional form. Recently, a Fusion approach has been proposed which finds a direct and exact Monte Carlo approximation of the posterior (as opposed to the sub-posteriors), circumventing the drawbacks of approximate approaches. Unfortunately, existing Fusion approaches have a number of computational limitations, particularly when unifying a large number of sub-posteriors. In this paper, we generalise the theory underpinning existing Fusion approaches, and embed the resulting methodology within a recursive divide-and-conquer sequential Monte Carlo paradigm. This ultimately leads to a competitive Fusion approach, which is robust to increasing numbers of sub-posteriors.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/9SSKP92G/Chan et al. - 2021 - Divide-and-Conquer Monte Carlo Fusion.pdf;/Users/amanderson/Zotero/storage/3J3RT98U/2110.html}
}

@inproceedings{cornish:etal:19,
  ids = {cornish_scalable_2019,cornish_scalable_2019-1},
  title = {Scalable {{Metropolis-Hastings}} for Exact {{Bayesian}} Inference with Large Datasets},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Cornish, Rob and Vanetti, Paul and {Bouchard-Cote}, Alexandre and Deligiannidis, George and Doucet, Arnaud},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {1351--1360},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  issn = {2640-3498},
  pdf = {http://proceedings.mlr.press/v97/cornish19a/cornish19a.pdf},
  file = {/Users/amanderson/Zotero/storage/DK79RR3A/Cornish et al. - 2019 - Scalable Metropolis-Hastings for Exact Bayesian In.pdf;/Users/amanderson/Zotero/storage/E882E37K/Cornish et al. - 2019 - Scalable Metropolis-Hastings for Exact Bayesian In.pdf}
}

@article{crowder_identifiability_1991,
  title = {On the {{Identifiability Crisis}} in {{Competing Risks Analysis}}},
  author = {Crowder, Martin},
  year = {1991},
  journal = {Scandinavian Journal of Statistics},
  volume = {18},
  number = {3},
  pages = {223--233},
  publisher = {{[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]}},
  issn = {0303-6898},
  abstract = {Competing risks analysis, based on fully parametric models, suffers from a well known non-identifiability problem in which the assumed multivariate distribution is not verifiable from observed data. It is shown here that the situation is even worse than previously described. Further, systems which survive one or more failures are investigated and an analogous structure is found.},
  file = {/Users/amanderson/Zotero/storage/YKSYRY7E/Crowder - 1991 - On the Identifiability Crisis in Competing Risks A.pdf}
}

@article{crowther_simulating_2013,
  title = {Simulating Biologically Plausible Complex Survival Data},
  author = {Crowther, Michael J. and Lambert, Paul C.},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {23},
  pages = {4118--4134},
  issn = {1097-0258},
  doi = {10.1002/sim.5823},
  abstract = {Simulation studies are conducted to assess the performance of current and novel statistical models in pre-defined scenarios. It is often desirable that chosen simulation scenarios accurately reflect a biologically plausible underlying distribution. This is particularly important in the framework of survival analysis, where simulated distributions are chosen for both the event time and the censoring time. This paper develops methods for using complex distributions when generating survival times to assess methods in practice. We describe a general algorithm involving numerical integration and root-finding techniques to generate survival times from a variety of complex parametric distributions, incorporating any combination of time-dependent effects, time-varying covariates, delayed entry, random effects and covariates measured with error. User-friendly Stata software is provided. Copyright \textcopyright{} 2013 John Wiley {$\&$} Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2013 John Wiley {$\&$} Sons, Ltd.},
  langid = {english},
  keywords = {delayed entry,measurement error,simulation,survival,time-dependent effects,time-varying covariates},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5823},
  file = {/Users/amanderson/Zotero/storage/6U2NSKRE/Crowther and Lambert - 2013 - Simulating biologically plausible complex survival.pdf;/Users/amanderson/Zotero/storage/U5RYACZ7/sim.html}
}

@article{dai_bayesian_2021-1,
  ids = {dai_bayesian_2021},
  title = {Bayesian Fusion: {{Scalable}} Unification of Distributed Statistical Analyses},
  shorttitle = {Bayesian {{Fusion}}},
  author = {Dai, Hongsheng and Pollock, Murray and Roberts, Gareth},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.02123 [stat]},
  eprint = {2102.02123},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {There has recently been considerable interest in addressing the problem of unifying distributed statistical analyses into a single coherent inference. This problem naturally arises in a number of situations, including in big-data settings, when working under privacy constraints, and in Bayesian model choice. The majority of existing approaches have relied upon convenient approximations of the distributed analyses. Although typically being computationally efficient, and readily scaling with respect to the number of analyses being unified, approximate approaches can have significant shortcomings -- the quality of the inference can degrade rapidly with the number of analyses being unified, and can be substantially biased even when unifying a small number of analyses that do not concur. In contrast, the recent Fusion approach of Dai et al. (2019) is a rejection sampling scheme which is readily parallelisable and is exact (avoiding any form of approximation other than Monte Carlo error), albeit limited in applicability to unifying a small number of low-dimensional analyses. In this paper we introduce a practical Bayesian Fusion approach. We extend the theory underpinning the Fusion methodology and, by embedding it within a sequential Monte Carlo algorithm, we are able to recover the correct target distribution. By means of extensive guidance on the implementation of the approach, we demonstrate theoretically and empirically that Bayesian Fusion is robust to increasing numbers of analyses, and coherently unifying analyses which do not concur. This is achieved while being computationally competitive with approximate schemes.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/5P5C4M5K/Dai et al. - 2021 - Bayesian Fusion Scalable unification of distribut.pdf;/Users/amanderson/Zotero/storage/HHKWRWE4/Dai et al. - 2021 - Bayesian Fusion Scalable unification of distribut.pdf;/Users/amanderson/Zotero/storage/UEMGD5UB/Dai et al. - 2021 - Bayesian Fusion Scalable unification of distribut.pdf;/Users/amanderson/Zotero/storage/HAHCFVPB/2102.html;/Users/amanderson/Zotero/storage/MFQA34E4/2102.html;/Users/amanderson/Zotero/storage/TIPKKKVT/2102.html}
}

@article{dai_monte_2019-1,
  ids = {dai_monte_2019},
  title = {Monte {{Carlo}} Fusion},
  author = {Dai, Hongsheng and Pollock, Murray and Roberts, Gareth},
  year = {2019},
  month = mar,
  journal = {Journal of Applied Probability},
  volume = {56},
  number = {1},
  eprint = {1901.00139},
  eprinttype = {arxiv},
  pages = {174--191},
  publisher = {{Cambridge University Press}},
  issn = {0021-9002, 1475-6072},
  doi = {10.1017/jpr.2019.12},
  abstract = {In this paper we propose a new theory and methodology to tackle the problem of unifying Monte Carlo samples from distributed densities into a single Monte Carlo draw from the target density. This surprisingly challenging problem arises in many settings (for instance, expert elicitation, multiview learning, distributed `big data' problems, etc.), but to date the framework and methodology proposed in this paper (Monte Carlo fusion) is the first general approach which avoids any form of approximation error in obtaining the unified inference. In this paper we focus on the key theoretical underpinnings of this new methodology, and simple (direct) Monte Carlo interpretations of the theory. There is considerable scope to tailor the theory introduced in this paper to particular application settings (such as the big data setting), construct efficient parallelised schemes, understand the approximation and computational efficiencies of other such unification paradigms, and explore new theoretical and methodological directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {65C05; 65C60; 62C10; 65C30,65C30,65C60,Fork-and-join,fusion,Langevin diffusion,Monte Carlo,Primary 65C05,Secondary 62C10,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/9VLR5UV2/Dai et al. - 2019 - Monte Carlo fusion.pdf;/Users/amanderson/Zotero/storage/TUITWQ6V/Dai et al. - 2019 - Monte Carlo Fusion.pdf;/Users/amanderson/Zotero/storage/3IRA4RN7/2C7099791C6073E37F98A7FFD91C331E.html}
}

@article{dawid_hyper_1993-1,
  title = {Hyper {{Markov}} Laws in the Statistical Analysis of Decomposable Graphical Models},
  author = {Dawid, A. P. and Lauritzen, S. L.},
  year = {1993},
  month = sep,
  journal = {The Annals of Statistics},
  volume = {21},
  number = {3},
  pages = {1272--1317},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176349260},
  abstract = {This paper introduces and investigates the notion of a hyper Markov law, which is a probability distribution over the set of probability measures on a multivariate space that (i) is concentrated on the set of Markov probabilities over some decomposable graph, and (ii) satisfies certain conditional independence restrictions related to that graph. A stronger version of this hyper Markov property is also studied. Our analysis starts by reconsidering the properties of Markov probabilities, using an abstract approach which thereafter proves equally applicable to the hyper Markov case. Next, it is shown constructively that hyper Markov laws exist, that they appear as sampling distributions of maximum likelihood estimators in decomposable graphical models, and also that they form natural conjugate prior distributions for a Bayesian analysis of these models. As examples we construct a range of specific hyper Markov laws, including the hyper multinomial, hyper Dirichlet and the hyper Wishart and inverse Wishart laws. These laws occur naturally in connection with the analysis of decomposable log-linear and covariance selection models.},
  keywords = {$\\log$-linear models,62E15,62H99,Bayesian statistics,Collapsibility,Contingency tables,covariance selection,cut,decomposable graphs,Dirichlet distribution,expert systems,graphical models,hyper Dirichlet law,hyper inverse Wishart law,hyper matrix $F$ Law,hyper matrix $t$ law,hyper Multinomial law,hyper Normal law,hyper Wishart law,inverse Wishart distribution,matrix $F$ distribution,matrix $t$ distribution,Multivariate analysis,triangulated graphs,Wishart distribution},
  file = {/Users/amanderson/Zotero/storage/GZ7M2AX4/Dawid and Lauritzen - 1993 - Hyper Markov Laws in the Statistical Analysis of D.pdf;/Users/amanderson/Zotero/storage/GVANNUD2/1176349260.html}
}

@article{de_carvalho_combining_2020,
  title = {Combining Probability Distributions: {{Extending}} the Logarithmic Pooling Approach},
  shorttitle = {Combining Probability Distributions},
  author = {{de Carvalho}, Luiz Max and Villela, Daniel A. M. and Coelho, Flavio Codeco and Bastos, Leonardo Soares},
  year = {2020},
  month = dec,
  journal = {arXiv:1502.04206 [stat]},
  eprint = {1502.04206},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Combining distributions is an important issue in decision theory and Bayesian inference. Logarithmic pooling is a popular method to aggregate expert opinions by using a set of weights that reflect the reliability of each information source. However, the resulting pooled distribution depends heavily on set of weights given to each opinion/prior and thus careful consideration must be given to the choice of weights. In this paper we review and extend the statistical theory of logarithmic pooling, focusing on the assignment of the weights using a hierarchical prior distribution. We explore several statistical applications, such as the estimation of survival probabilities, meta-analysis and Bayesian melding of deterministic models of population growth and epidemics. We show that it is possible learn the weights from data, although identifiability issues may arise for some configurations of priors and data. Furthermore, we show how the hierarchical approach leads to posterior distributions that are able to accommodate prior-data conflict in complex models.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  note = {Comment: Massively updated manuscript; submitted for publication},
  file = {/Users/amanderson/Zotero/storage/3DFK2H7H/de Carvalho et al. - 2020 - Combining probability distributions Extending the.pdf;/Users/amanderson/Zotero/storage/QGWWE6HQ/1502.html}
}

@article{de_valpine_programming_2017,
  title = {Programming with Models: Writing Statistical Algorithms for General Model Structures with {{NIMBLE}}},
  shorttitle = {Programming with Models},
  author = {{de Valpine}, Perry and Turek, Daniel and Paciorek, Christopher J. and {Anderson-Bergman}, Clifford and Lang, Duncan Temple and Bodik, Rastislav},
  year = {2017},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {2},
  pages = {403--413},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2016.1172487},
  abstract = {We describe NIMBLE, a system for programming statistical algorithms for general model structures within R. NIMBLE is designed to meet three challenges: flexible model specification, a language for programming algorithms that can use different models, and a balance between high-level programmability and execution efficiency. For model specification, NIMBLE extends the BUGS language and creates model objects, which can manipulate variables, calculate log probability values, generate simulations, and query the relationships among variables. For algorithm programming, NIMBLE provides functions that operate with model objects using two stages of evaluation. The first stage allows specialization of a function to a particular model and/or nodes, such as creating a Metropolis-Hastings sampler for a particular block of nodes. The second stage allows repeated execution of computations using the results of the first stage. To achieve efficient second-stage computation, NIMBLE compiles models and functions via C++, using the Eigen library for linear algebra, and provides the user with an interface to compiled objects. The NIMBLE language represents a compilable domain-specific language (DSL) embedded within R. This article provides an overview of the design and rationale for NIMBLE along with illustrative examples including importance sampling, Markov chain Monte Carlo (MCMC) and Monte Carlo expectation maximization (MCEM). Supplementary materials for this article are available online.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/ZYMTAZF3/de Valpine et al. - 2017 - Programming With Models Writing Statistical Algor.pdf}
}

@article{diggle_real-time_2015,
  title = {Real-Time Monitoring of Progression towards Renal Failure in Primary Care Patients},
  author = {Diggle, Peter J. and Sousa, In{\^e}s and Asar, {\"O}zg{\"u}r},
  year = {2015},
  month = jul,
  journal = {Biostatistics},
  volume = {16},
  number = {3},
  pages = {522--536},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxu053},
  abstract = {Abstract.  Chronic renal failure is a progressive condition that, typically, is asymptomatic for many years. Early detection of incipient kidney failure enables},
  langid = {english},
  keywords = {read},
  file = {/Users/amanderson/Zotero/storage/VJ7GWLZB/Diggle et al. - 2015 - Real-time monitoring of progression towards renal .pdf;/Users/amanderson/Zotero/storage/8S7WFDTT/269574.html}
}

@inproceedings{donnat_bayesian_2020,
  title = {A {{Bayesian}} Hierarchical Network for Combining Heterogeneous Data Sources in Medical Diagnoses},
  booktitle = {Proceedings of the {{Machine Learning}} for {{Health NeurIPS Workshop}}},
  author = {Donnat, Claire and Miolane, Nina and Bunbury, Freddy and Kreindler, Jack},
  year = {2020},
  month = nov,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {136},
  pages = {53--84},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The increasingly widespread use of affordable, yet often less reliable medical data and diagnostic tools poses a new challenge for the field of ComputerAided Diagnosis: how can we combine multiple ...},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/KJNT7F3K/Donnat et al. - 2020 - A Bayesian Hierarchical Network for Combining Hete.pdf;/Users/amanderson/Zotero/storage/WUFYKUII/donnat20a.html}
}

@article{finke_efficient_2019,
  title = {Efficient Sequential {{Monte Carlo}} Algorithms for Integrated Population Models},
  author = {Finke, Axel and King, Ruth and Beskos, Alexandros and Dellaportas, Petros},
  year = {2019},
  month = jun,
  journal = {Journal of Agricultural, Biological and Environmental Statistics},
  volume = {24},
  number = {2},
  pages = {204--224},
  issn = {1085-7117, 1537-2693},
  doi = {10.1007/s13253-018-00349-9},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/AYUCXSYC/Finke et al. - 2019 - Efficient Sequential Monte Carlo Algorithms for In.pdf;/Users/amanderson/Zotero/storage/IGNFEYUN/13253_2018_349_MOESM1_ESM.pdf}
}

@misc{gabry_bayesplot_2021,
  title = {Bayesplot: Plotting for {{Bayesian}} Models},
  shorttitle = {Bayesplot},
  author = {Gabry, Jonah and Mahr, Tristan and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin and Barrett, Malcolm and Weber, Frank and Sroka, Eduardo Coronado and Vehtari, Aki},
  year = {2021},
  month = jan,
  abstract = {Plotting functions for posterior analysis, MCMC diagnostics, prior and posterior predictive checks, and other visualizations to support the applied Bayesian workflow advocated in Gabry, Simpson, Vehtari, Betancourt, and Gelman (2019) {$<$}doi:10.1111/rssa.12378{$>$}. The package is designed not only to provide convenient functionality for users, but also a common set of functions that can be easily used by developers working on a variety of R packages for Bayesian modeling, particularly (but not exclusively) packages interfacing with 'Stan'.},
  copyright = {GPL ({$\geq$} 3)},
  annotation = {R package version 1.8.0},
  note = {R package version 1.8.0}
}

@article{gabry_visualization_2019,
  ids = {gabry_visualization_2019-1},
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  number = {2},
  pages = {389--402},
  doi = {10.1111/rssa.12378},
  keywords = {Bayesian data analysis,Statistical graphics,Statistical workflow},
  annotation = {{$\_$}eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12378},
  file = {/Users/amanderson/Zotero/storage/35VTLRD6/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf;/Users/amanderson/Zotero/storage/AWC6XBUB/rssa.html}
}

@article{gelman_bayesian_2020,
  title = {Bayesian Workflow},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01808 [stat]},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/DAWRLSZG/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/amanderson/Zotero/storage/UYYUCBBU/2011.html}
}

@article{genest_characterization_1986,
  title = {Characterization of Externally {{Bayesian}} Pooling Operators},
  author = {Genest, Christian and McConway, Kevin J. and Schervish, Mark J.},
  year = {1986},
  journal = {The Annals of Statistics},
  volume = {14},
  number = {2},
  pages = {487--501},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {When a panel of experts is asked to provide some advice in the form of a group probability distribution, the question arises as to whether they should synthesize their opinions before or after they learn the outcome of an experiment. If the group posterior distribution is the same whatever the order in which the pooling and the updating are done, the pooling mechanism is said to be externally Bayesian by Madansky (1964). In this paper, we characterize all externally Bayesian pooling formulas and we give conditions under which the opinion of the group will be proportional to the geometric average of the individual densities.},
  file = {/Users/amanderson/Zotero/storage/4SBH486Z/Genest et al. - 1986 - Characterization of Externally Bayesian Pooling Op.pdf}
}

@article{giganti_accounting_2020,
  title = {Accounting for Dependent Errors in Predictors and Time-to-Event Outcomes Using Electronic Health Records, Validation Samples, and Multiple Imputation},
  author = {Giganti, Mark J. and Shaw, Pamela A. and Chen, Guanhua and Bebawy, Sally S. and Turner, Megan M. and Sterling, Timothy R. and Shepherd, Bryan E.},
  year = {2020},
  month = jun,
  journal = {Annals of Applied Statistics},
  volume = {14},
  number = {2},
  pages = {1045--1061},
  issn = {1932-6157},
  doi = {10.1214/20-aoas1343},
  abstract = {Data from electronic health records (EHR) are prone to errors, which are often correlated across multiple variables. The error structure is further complicated when analysis variables are derived as functions of two or more error-prone variables. Such errors can substantially impact estimates, yet we are unaware of methods that simultaneously account for errors in covariates and time-to-event outcomes. Using EHR data from 4217 patients, the hazard ratio for an AIDS-defining event associated with a 100 cell/mm3 increase in CD4 count at ART initiation was 0.74 (95{$\%$}CI: 0.68\textendash 0.80) using unvalidated data and 0.60 (95{$\%$}CI: 0.53\textendash 0.68) using fully validated data. Our goal is to obtain unbiased and efficient estimates after validating a random subset of records. We propose fitting discrete failure time models to the validated subsample and then multiply imputing values for unvalidated records. We demonstrate how this approach simultaneously addresses dependent errors in predictors, time-to-event outcomes, and inclusion criteria. Using the fully validated dataset as a gold standard, we compare the mean squared error of our estimates with those from the unvalidated dataset and the corresponding subsample-only dataset for various subsample sizes. By incorporating reasonably sized validated subsamples and appropriate imputation models, our approach had improved estimation over both the naive analysis and the analysis using only the validation subsample.},
  pmcid = {PMC7523695},
  pmid = {32999698},
  file = {/Users/amanderson/Zotero/storage/VRH3MRHP/Giganti et al. - 2020 - ACCOUNTING FOR DEPENDENT ERRORS IN PREDICTORS AND .pdf}
}

@article{goudie_joining_2019,
  title = {Joining and Splitting Models with {{Markov}} Melding},
  author = {Goudie, Robert J. B. and Presanis, Anne M. and Lunn, David and De Angelis, Daniela and Wernisch, Lorenz},
  year = {2019},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {1},
  pages = {81--109},
  issn = {1936-0975},
  doi = {10.1214/18-BA1104},
  abstract = {Analysing multiple evidence sources is often feasible only via a modular approach, with separate submodels specified for smaller components of the available evidence. Here we introduce a generic framework that enables fully Bayesian analysis in this setting. We propose a generic method for forming a suitable joint model when joining submodels, and a convenient computational algorithm for fitting this joint model in stages, rather than as a single, monolithic model. The approach also enables splitting of large joint models into smaller submodels, allowing inference for the original joint model to be conducted via our multi-stage algorithm. We motivate and demonstrate our approach through two examples: joining components of an evidence synthesis of A/H1N1 influenza, and splitting a large ecology model.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/FXY8J4JC/Goudie et al. - 2018 - Supplementary Material for ``Joining and splittin.pdf;/Users/amanderson/Zotero/storage/YNN4BGDS/Goudie et al. - 2019 - Joining and Splitting Models with Markov Melding.pdf}
}

@article{gu_semiparametric_2015,
  title = {Semiparametric Time to Event Models in the Presence of Error-Prone, Self-Reported Outcomes\textemdash with Application to the Women's Health Initiative},
  author = {Gu, Xiangdong and Ma, Yunsheng and Balasubramanian, Raji},
  year = {2015},
  month = jun,
  journal = {Annals of Applied Statistics},
  volume = {9},
  number = {2},
  pages = {714--730},
  issn = {1932-6157},
  doi = {10.1214/15-AOAS810},
  abstract = {The onset of several silent, chronic diseases such as diabetes can be detected only through diagnostic tests. Due to cost considerations, self-reported outcomes are routinely collected in lieu of expensive diagnostic tests in large-scale prospective investigations such as the Women's Health Initiative. However, self-reported outcomes are subject to imperfect sensitivity and specificity. Using a semiparametric likelihood-based approach, we present time to event models to estimate the association of one or more covariates with a error-prone, self-reported outcome. We present simulation studies to assess the effect of error in self-reported outcomes with regard to bias in the estimation of the regression parameter of interest. We apply the proposed methods to prospective data from 152,830 women enrolled in the Women's Health Initiative to evaluate the effect of statin use with the risk of incident diabetes mellitus among postmenopausal women. The current analysis is based on follow-up through 2010, with a median duration of follow-up of 12.1 years. The methods proposed in this paper are readily implemented using our freely available R software package icensmis, which is available at the Comprehensive R Archive Network (CRAN) website.},
  pmcid = {PMC4729390},
  pmid = {26834908},
  keywords = {read},
  file = {/Users/amanderson/Zotero/storage/H2HZKDKQ/Gu et al. - 2015 - SEMIPARAMETRIC TIME TO EVENT MODELS IN THE PRESENC.pdf}
}

@book{harron_methodological_2015,
  title = {Methodological Developments in Data Linkage},
  author = {Harron, Katie and Goldstein, Harvey and Dibben, Chris},
  year = {2015},
  publisher = {{John Wiley {$\&$} Sons, Incorporated}},
  address = {{New York, UNITED KINGDOM}},
  abstract = {A comprehensive compilation of new developments in data linkage methodology The increasing availability of large administrative databases has led to a dramatic rise in the use of data linkage, yet the standard texts on linkage are still those which describe the seminal work from the 1950-60s, with some updates. Linkage and analysis of data across sources remains problematic due to lack of discriminatory and accurate identifiers, missing data and regulatory issues. Recent developments in data linkage methodology have concentrated on bias and analysis of linked data, novel approaches to organising relationships between databases and privacy-preserving linkage. Methodological Developments in Data Linkage brings together a collection of contributions from members of the international data linkage community, covering cutting edge methodology in this field. It presents opportunities and challenges provided by linkage of large and often complex datasets, including analysis problems, legal and security aspects, models for data access and the development of novel research areas. New methods for handling uncertainty in analysis of linked data, solutions for anonymised linkage and alternative models for data collection are also discussed. Key Features: Presents cutting edge methods for a topic of increasing importance to a wide range of research areas, with applications to data linkage systems internationally Covers the essential issues associated with data linkage today Includes examples based on real data linkage systems, highlighting the opportunities, successes and challenges that the increasing availability of linkage data provides Novel approach incorporates technical aspects of both linkage, management and analysis of linked data This book will be of core interest to academics, government employees, data holders, data managers, analysts and statisticians who use administrative data. It will also appeal to researchers in a variety of areas, including epidemiology, biostatistics, social statistics, informatics, policy and public health.},
  isbn = {978-1-119-07248-5},
  keywords = {Linked data},
  file = {/Users/amanderson/Zotero/storage/PZTP3VM9/detail.html}
}

@article{hashemi_latent_2003,
  title = {A Latent Process Model for Joint Modeling of Events and Marker},
  author = {Hashemi, R. and {Jacqmin-Gadda}, H. and Commenges, D.},
  year = {2003},
  month = dec,
  journal = {Lifetime Data Analysis},
  volume = {9},
  number = {4},
  pages = {331--343},
  issn = {1572-9249},
  doi = {10.1023/B:LIDA.0000012420.36627.a6},
  abstract = {The paper formulates joint modeling of a counting process and a sequence of longitudinal measurements, governed by a common latent stochastic process. The latent process is modeled as a function of explanatory variables and a Brownian motion process. The conditional likelihood given values of the latent process at the measurement times, has been drawn using Brownian bridge properties; then integrating over all possible values of the latent process at the measurement times leads to the desired joint likelihood. An estimation procedure using joint likelihood and a numerical optimization is described. The method is applied to the study of cognitive decline and Alzheimer's disease.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/N4DT53HI/Hashemi et al. - 2003 - A Latent Process Model for Joint Modeling of Event.pdf}
}

@book{hastie_generalized_1999,
  title = {Generalized Additive Models},
  author = {Hastie, Trevor and Tibshirani, Robert},
  year = {1999},
  publisher = {{Chapman {$\&$} Hall/CRC}},
  address = {{Boca Raton, Fla}},
  isbn = {978-0-412-34390-2},
  langid = {english},
  lccn = {QA278.2 .H39 1999},
  keywords = {Linear models (Statistics),Regression analysis,Smoothing (Statistics)},
  file = {/Users/amanderson/Zotero/storage/6TYV8Y5K/Hastie and Tibshirani - 1999 - Generalized additive models.pdf}
}

@article{hennessey_bayesian_2010,
  title = {A {{Bayesian}} Approach to Dose\textendash Response Assessment and Synergy and Its Application to in Vitro Dose\textendash Response Studies},
  author = {Hennessey, Violeta G. and Rosner, Gary L. and Bast Jr, Robert C. and Chen, Min-Yu},
  year = {2010},
  journal = {Biometrics},
  volume = {66},
  number = {4},
  pages = {1275--1283},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2010.01403.x},
  abstract = {In this article, we propose a Bayesian approach to dose\textendash response assessment and the assessment of synergy between two combined agents. We consider the case of an in vitro ovarian cancer research study aimed at investigating the antiproliferative activities of four agents, alone and paired, in two human ovarian cancer cell lines. In this article, independent dose\textendash response experiments were repeated three times. Each experiment included replicates at investigated dose levels including control (no drug). We have developed a Bayesian hierarchical nonlinear regression model that accounts for variability between experiments, variability within experiments (i.e., replicates), and variability in the observed responses of the controls. We use Markov chain Monte Carlo to fit the model to the data and carry out posterior inference on quantities of interest (e.g., median inhibitory concentration IC 50). In addition, we have developed a method, based on Loewe additivity, that allows one to assess the presence of synergy with honest accounting of uncertainty. Extensive simulation studies show that our proposed approach is more reliable in declaring synergy compared to current standard analyses such as the median-effect principle/combination index method (Chou and Talalay, 1984, Advances in Enzyme Regulation 22, 27\textendash 55), which ignore important sources of variability and uncertainty.},
  langid = {english},
  keywords = {Combination index method,Drug interaction,E max model,Interaction index,Loewe additivity model,Median-effect principle},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2010.01403.x},
  file = {/Users/amanderson/Zotero/storage/MTP42Z62/Hennessey et al. - 2010 - A Bayesian Approach to DoseResponse Assessment an.pdf;/Users/amanderson/Zotero/storage/8KHCWUSZ/j.1541-0420.2010.01403.html}
}

@article{hinton_training_2002,
  title = {Training Products of Experts by Minimizing Contrastive Divergence},
  author = {Hinton, Geoffrey E.},
  year = {2002},
  journal = {Neural Computation},
  volume = {14},
  number = {8},
  pages = {1771--1800},
  doi = {10.1162/089976602760128018},
  annotation = {{$\_$}eprint: https://doi.org/10.1162/089976602760128018}
}

@article{hooten_making_2019,
  title = {Making Recursive {{Bayesian}} Inference Accessible},
  author = {Hooten, Mevin B. and Johnson, Devin S. and Brost, Brian M.},
  year = {2019},
  month = oct,
  journal = {The American Statistician},
  pages = {1--10},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1665584},
  abstract = {Bayesian models provide recursive inference naturally because they can formally reconcile new data and existing scientific information. However, popular use of Bayesian methods often avoids priors that are based on exact posterior distributions resulting from former studies. Two existing Recursive Bayesian methods are: Prior- and Proposal-Recursive Bayes. Prior-Recursive Bayes uses Bayesian updating, fitting models to partitions of data sequentially, and provides a way to accommodate new data as they become available using the posterior from the previous stage as the prior in the new stage based on the latest data. ProposalRecursive Bayes is intended for use with hierarchical Bayesian models and uses a set of transient priors in first stage independent analyses of the data partitions. The second stage of Proposal-Recursive Bayes uses the posteriors from the first stage as proposals in a Markov chain Monte Carlo algorithm to fit the full model. We combine Prior- and Proposal-Recursive concepts to fit any Bayesian model, and often with computational improvements. We demonstrate our method with two case studies. Our approach has implications for big data, streaming data, and optimal adaptive design situations.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/5S4XMM8J/Hooten et al. - 2019 - Making Recursive Bayesian Inference Accessible.pdf}
}

@book{ibrahim_bayesian_2001,
  title = {Bayesian Survival Analysis},
  author = {Ibrahim, Joseph George and Chen, Ming-Hui and Sinha, Debajyoti},
  year = {2001},
  series = {Springer Series in Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-95277-2},
  lccn = {QA276 .I27 2001},
  keywords = {Bayesian statistical decision theory,Failure time data analysis},
  file = {/Users/amanderson/Zotero/storage/U53YJHQG/Ibrahim et al. - Bayesian Survival Analysis.pdf}
}

@article{jackson_when_2018,
  title = {When Should Meta-Analysis Avoid Making Hidden Normality Assumptions?},
  author = {Jackson, Dan and White, Ian R.},
  year = {2018},
  journal = {Biometrical Journal},
  volume = {60},
  number = {6},
  pages = {1040--1058},
  issn = {1521-4036},
  doi = {10.1002/bimj.201800071},
  abstract = {Meta-analysis is a widely used statistical technique. The simplicity of the calculations required when performing conventional meta-analyses belies the parametric nature of the assumptions that justify them. In particular, the normal distribution is extensively, and often implicitly, assumed. Here, we review how the normal distribution is used in meta-analysis. We discuss when the normal distribution is likely to be adequate and also when it should be avoided. We discuss alternative and more advanced methods that make less use of the normal distribution. We conclude that statistical methods that make fewer normality assumptions should be considered more often in practice. In general, statisticians and applied analysts should understand the assumptions made by their statistical analyses. They should also be able to defend these assumptions. Our hope is that this article will foster a greater appreciation of the extent to which assumptions involving the normal distribution are made in statistical methods for meta-analysis. We also hope that this article will stimulate further discussion and methodological work.},
  langid = {english},
  keywords = {central limit theorem,distributional assumptions,normal approximation,random effects models},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201800071},
  file = {/Users/amanderson/Zotero/storage/ZYXV9CHA/Jackson and White - 2018 - When should meta-analysis avoid making hidden norm.pdf;/Users/amanderson/Zotero/storage/L8PL847J/bimj.html}
}

@article{jacob_better_2017-1,
  ids = {jacob_better_2017},
  title = {Better Together? {{Statistical}} Learning in Models Made of Modules},
  shorttitle = {Better Together?},
  author = {Jacob, Pierre E. and Murray, Lawrence M. and Holmes, Chris C. and Robert, Christian P.},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.08719 [stat]},
  eprint = {1708.08719},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In modern applications, statisticians are faced with integrating heterogeneous data modalities relevant for an inference, prediction, or decision problem. In such circumstances, it is convenient to use a graphical model to represent the statistical dependencies, via a set of connected "modules", each relating to a specific data modality, and drawing on specific domain expertise in their development. In principle, given data, the conventional statistical update then allows for coherent uncertainty quantification and information propagation through and across the modules. However, misspecification of any module can contaminate the estimate and update of others, often in unpredictable ways. In various settings, particularly when certain modules are trusted more than others, practitioners have preferred to avoid learning with the full model in favor of approaches that restrict the information propagation between modules, for example by restricting propagation to only particular directions along the edges of the graph. In this article, we investigate why these modular approaches might be preferable to the full model in misspecified settings. We propose principled criteria to choose between modular and full-model approaches. The question arises in many applied settings, including large stochastic dynamical systems, meta-analysis, epidemiological models, air pollution models, pharmacokinetics-pharmacodynamics, and causal inference with propensity scores.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/X2U8ZX22/Jacob et al. - 2017 - Better together Statistical learning in models ma.pdf;/Users/amanderson/Zotero/storage/HEMHXPPF/1708.html}
}

@article{johnson_mimic-iii_2016,
  title = {{{MIMIC-III}}, a Freely Accessible Critical Care Database},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
  year = {2016},
  month = may,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160035},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.35},
  abstract = {MIMIC-III (`Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
  copyright = {2016 The Author(s)},
  langid = {english},
  annotation = {Bandiera{$\_$}abtest: a Cg{$\_$}type: Nature Research Journals Primary{$\_$}atype: Research Subject{$\_$}term: Diagnosis;Health care;Medical research;Outcomes research;Prognosis Subject{$\_$}term{$\_$}id: diagnosis;health-care;medical-research;outcomes-research;prognosis},
  file = {/Users/amanderson/Zotero/storage/7S5QLZVZ/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care datab.pdf;/Users/amanderson/Zotero/storage/9FP5BJGH/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care datab.pdf;/Users/amanderson/Zotero/storage/ZXALWJQB/sdata201635.html}
}

@book{kalbfleisch_statistical_2002,
  title = {The Statistical Analysis of Failure Time Data},
  author = {Kalbfleisch, J. D. and Prentice, Ross L.},
  year = {2002},
  series = {Wiley Series in Probability and Statistics},
  edition = {2nd ed},
  publisher = {{J. Wiley}},
  address = {{Hoboken, N.J}},
  isbn = {978-0-471-36357-6},
  langid = {english},
  lccn = {QA276 .K215 2002},
  keywords = {Failure time data analysis,Regression analysis,Survival analysis (Biometry)},
  file = {/Users/amanderson/Zotero/storage/HIEG27NW/Kalbfleisch and Prentice - 2002 - The statistical analysis of failure time data.pdf}
}

@misc{kay_tidybayes_2020,
  title = {Tidybayes: Tidy Data and Geoms for {{Bayesian}} Models},
  author = {Kay, Matthew},
  year = {2020},
  doi = {10.5281/zenodo.1308151},
  note = {R package version 2.0.2}
}

@book{kedem_statistical_2017,
  title = {Statistical Data Fusion},
  author = {Kedem, Benjamin and De Oliveira, Victor and Sverchkov, Michael},
  year = {2017},
  publisher = {{World Scientific}},
  doi = {10.1142/10282},
  annotation = {{$\_$}eprint: https://www.worldscientific.com/doi/pdf/10.1142/10282}
}

@article{kharratzadeh_splines_2017,
  title = {Splines in {{Stan}}},
  author = {Kharratzadeh, Milad},
  year = {2017},
  journal = {Stan Case Studies},
  volume = {4},
  abstract = {In this document, we discuss the implementation of splines in Stan. We start by providing a brief introduction to splines and then explain how they can be implemented in Stan. We also discuss a novel prior that alleviates some of the practical challenges of spline models.}
}

@article{kuntz_divide-and-conquer_2021,
  title = {The Divide-and-Conquer Sequential {{Monte Carlo}} Algorithm: Theoretical Properties and Limit Theorems},
  shorttitle = {The Divide-and-Conquer Sequential {{Monte Carlo}} Algorithm},
  author = {Kuntz, Juan and Crucinio, Francesca R. and Johansen, Adam M.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.15782 [math, stat]},
  eprint = {2110.15782},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We revisit the divide-and-conquer sequential Monte Carlo (DaC-SMC) algorithm and firmly establish it as a well-founded method by showing that it possesses the same basic properties as conventional sequential Monte Carlo (SMC) algorithms do. In particular, we derive pertinent laws of large numbers, {$\$$}L{$\sphat$}p{$\$$} inequalities, and central limit theorems; and we characterize the bias in the normalized estimates produced by the algorithm and argue the absence thereof in the unnormalized ones. We further consider its practical implementation and several interesting variants; obtain expressions for its globally and locally optimal intermediate targets, auxiliary measures, and proposal kernels; and show that, in comparable conditions, DaC-SMC proves more statistically efficient than its direct SMC analogue. We close the paper with a discussion of our results, open questions, and future research directions.},
  archiveprefix = {arXiv},
  keywords = {65C05 (Primary) 60F05; 60F15; 62F15; 68W15 (Secondary),Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/TYT86EKS/Kuntz et al. - 2021 - The divide-and-conquer sequential Monte Carlo algo.pdf;/Users/amanderson/Zotero/storage/PDW3ER55/2110.html}
}

@book{kurowicka_dependence_2011,
  title = {Dependence Modeling: Vine Copula Handbook},
  shorttitle = {Dependence Modeling},
  editor = {Kurowicka, Dorota and Joe, Harry},
  year = {2011},
  publisher = {{World Scientific}},
  address = {{Singapore}},
  isbn = {978-981-4299-87-9},
  langid = {english}
}

@article{lagakos_general_1979,
  title = {General Right Censoring and Its Impact on the Analysis of Survival Data},
  author = {Lagakos, S. W.},
  year = {1979},
  journal = {Biometrics},
  volume = {35},
  number = {1},
  pages = {139--156},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2529941},
  abstract = {This paper concerns general right censoring and some of the difficulties it creates in the analysis of survival data. A general formulation of censored-survival processes leads to the partition of all models into those based on noninformative and informative censoring. Nearly all statistical methods for censored data assume that censoring is noninformative. Topics considered within this class include: the relationships between three models for noninformative censoring, the use of likelihood methods for inferences about the distribution of survival time, the effects of censoring on the K-sample problem, and the effects of censoring on model testing. Also considered are several topics which relate to informative censoring models. These include: problems of nonidentifiability that can be encountered when attempting to assess a set of data for the type of censoring in effect, the consequences of falsely assuming that censoring is noninformative, and classes of informative censoring models.},
  file = {/Users/amanderson/Zotero/storage/8VJHMNAA/Lagakos - 1979 - General Right Censoring and Its Impact on the Anal.pdf}
}

@article{lahat_multimodal_2015,
  title = {Multimodal Data Fusion: An Overview of Methods, Challenges, and Prospects},
  shorttitle = {Multimodal Data Fusion},
  author = {Lahat, D. and Adali, T. and Jutten, C.},
  year = {2015},
  month = sep,
  journal = {Proceedings of the IEEE},
  volume = {103},
  number = {9},
  pages = {1449--1477},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2015.2460697},
  abstract = {In various disciplines, information about the same phenomenon can be acquired from different types of detectors, at different conditions, in multiple experiments or subjects, among others. We use the term ``modality'' for each such acquisition framework. Due to the rich characteristics of natural phenomena, it is rare that a single modality provides complete knowledge of the phenomenon of interest. The increasing availability of several modalities reporting on the same system introduces new degrees of freedom, which raise questions beyond those related to exploiting each modality separately. As we argue, many of these questions, or ``challenges,'' are common to multiple domains. This paper deals with two key issues: ``why we need data fusion'' and ``how we perform it.'' The first issue is motivated by numerous examples in science and technology, followed by a mathematical framework that showcases some of the benefits that data fusion provides. In order to address the second issue, ``diversity'' is introduced as a key concept, and a number of data-driven solutions based on matrix and tensor decompositions are discussed, emphasizing how they account for diversity across the data sets. The aim of this paper is to provide the reader, regardless of his or her community of origin, with a taste of the vastness of the field, the prospects, and the opportunities that it holds.},
  keywords = {acquisition framework,Blind source separation,data acquisition,data diversity,data fusion,Data integration,data-driven solutions,Electroencephalography,Laser radar,latent variables,matrix decomposition,modality term,multimodal data fusion,Multimodal sensors,multimodality,multiset data analysis,overview,sensor fusion,Sensors,Synthetic aperture radar,tensor decomposition,tensor decompositions,tensors,uncited-but-relevant},
  file = {/Users/amanderson/Zotero/storage/5WVLEVJ2/Lahat et al. - 2015 - Multimodal Data Fusion An Overview of Methods, Ch.pdf;/Users/amanderson/Zotero/storage/CGN7P8DV/Lahat et al. - 2015 - Multimodal Data Fusion An Overview of Methods, Ch.pdf;/Users/amanderson/Zotero/storage/PH5592QE/Lahat et al. - 2015 - Multimodal Data Fusion An Overview of Methods, Ch.pdf;/Users/amanderson/Zotero/storage/XYTF266C/7214350.html;/Users/amanderson/Zotero/storage/YPTNX3WQ/7214350.html}
}

@article{lang_bayesian_2004,
  title = {Bayesian {{P-Splines}}},
  author = {Lang, Stefan and Brezger, Andreas},
  year = {2004},
  month = mar,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {13},
  number = {1},
  pages = {183--212},
  publisher = {{Taylor {$\&$} Francis}},
  issn = {1061-8600},
  doi = {10.1198/1061860043010},
  abstract = {P-splines are an attractive approach for modeling nonlinear smooth effects of covariates within the additive and varying coefficient models framework. In this article, we first develop a Bayesian version for P-splines and generalize in a second step the approach in various ways. First, the assumption of constant smoothing parameters can be replaced by allowing the smoothing parameters to be locally adaptive. This is particularly useful in situations with changing curvature of the underlying smooth function or with highly oscillating functions. In a second extension, one-dimensional P-splines are generalized to two-dimensional surface fitting for modeling interactions between metrical covariates. In a last step, the approach is extended to situations with spatially correlated responses allowing the estimation of geoadditive models. Inference is fully Bayesian and uses recent MCMC techniques for drawing random samples from the posterior. In a couple of simulation studies the performance of Bayesian P-splines is studied and compared to other approaches in the literature. We illustrate the approach by two complex application on rents for flats in Munich and on human brain mapping.},
  keywords = {Geoadditive models,Locally adaptive smoothing parameters,MCMC,Surface fitting,Varying coefficient models},
  annotation = {{$\_$}eprint: https://doi.org/10.1198/1061860043010},
  file = {/Users/amanderson/Zotero/storage/B3G96WZJ/Lang and Brezger - 2004 - Bayesian P-Splines.pdf;/Users/amanderson/Zotero/storage/E7QY35NS/1061860043010.html}
}

@article{lauritzen_chain_2002,
  title = {Chain Graph Models and Their Causal Interpretations},
  author = {Lauritzen, Steffen L. and Richardson, Thomas S.},
  year = {2002},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {64},
  number = {3},
  pages = {321--348},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00340},
  abstract = {Chain graphs are a natural generalization of directed acyclic graphs and undirected graphs. However, the apparent simplicity of chain graphs belies the subtlety of the conditional independence hypotheses that they represent. There are many simple and apparently plausible, but ultimately fallacious, interpretations of chain graphs that are often invoked, implicitly or explicitly. These interpretations also lead to flawed methods for applying background knowledge to model selection. We present a valid interpretation by showing how the distribution corresponding to a chain graph may be generated from the equilibrium distributions of dynamic models with feed-back. These dynamic interpretations lead to a simple theory of intervention, extending the theory developed for directed acyclic graphs. Finally, we contrast chain graph models under this interpretation with simultaneous equation models which have traditionally been used to model feed-back in econometrics.},
  langid = {english},
  keywords = {Causal model,Chain graph,Feed-back system,Gibbs sampler,Intervention theory,Structural equation model},
  annotation = {{$\_$}eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00340},
  file = {/Users/amanderson/Zotero/storage/73NK848T/Lauritzen and Richardson - 2002 - Chain graph models and their causal interpretation.pdf;/Users/amanderson/Zotero/storage/HV4A7QIK/Lauritzen and Richardson - 2002 - Chain graph models and their causal interpretation.pdf;/Users/amanderson/Zotero/storage/XMNL77TT/Lauritzen and Richardson - 2002 - Chain graph models and their causal interpretation.pdf;/Users/amanderson/Zotero/storage/4INWW3JH/1467-9868.html;/Users/amanderson/Zotero/storage/5DR6FZVP/1467-9868.html}
}

@article{lebreton_modeling_1992,
  title = {Modeling Survival and Testing Biological Hypotheses Using Marked Animals: A Unified Approach with Case Studies},
  shorttitle = {Modeling Survival and Testing Biological Hypotheses Using Marked Animals},
  author = {Lebreton, Jean-Dominique and Burnham, Kenneth P. and Clobert, Jean and Anderson, David R.},
  year = {1992},
  journal = {Ecological Monographs},
  volume = {62},
  number = {1},
  pages = {67--118},
  issn = {1557-7015},
  doi = {10.2307/2937171},
  abstract = {The understanding of the dynamics of animal populations and of related ecological and evolutionary issues frequently depends on a direct analysis of life history parameters. For instance, examination of trade\textemdash offs between reproduction and survival usually rely on individually marked animals, for which the exact time of death is most often unknown, because marked individuals cannot be followed closely through time. Thus, the quantitative analysis of survival studies and experiments must be based on capture\textemdash recapture (or resighting) models which consider, besides the parameters of primary interest, recapture or resighting rates that are nuisance parameters. Capture\textemdash recapture models oriented to estimation of survival rates are the result of a recent change in emphasis from earlier approaches in which population size was the most important parameter, survival rates having been first introduced as nuisance parameters. This emphasis on survival rates in capture\textemdash recapture models developed rapidly in the 1980s and used as a basic structure the Cormack\textemdash Jolly\textemdash Seber survival model applied to an homogeneous group of animals, with various kinds of constraints on the model parameters. These approaches are conditional on first captures; hence they do not attempt to model the initial capture of unmarked animals as functions of population abundance in addition to survival and capture probabilities. This paper synthesizes, using a common framework, these recent developments together with new ones, with an emphasis on flexibility in modeling, model selection, and the analysis of multiple data sets. The effects on survival and capture rates of time, age, and categorical variables characterizing the individuals (e.g., sex) can be considered, as well as interactions between such effects. This 'analysis of variance' philosophy emphasizes the structure of the survival and capture process rather than the technical characteristics of any particular model. The flexible array of models encompassed in this synthesis uses a common notation. As a result of the great level of flexibility and relevance achieved, the focus is changed from fitting a particular model to model building and model selection. The following procedure is recommended: (1) start from a global model compatible with the biology of the species studied and with the design of the study, and assess its fit; (2) select a more parsimonious model using Akaike's Information Criterion to limit the number of formal tests; (3) test for the most important biological questions by comparing this model with neighboring ones using likelihood ratio tests; and (4) obtain maximum likelihood estimates of model parameters with estimates of precision. Computer software is critical, as few of the models now available have parameter estimators that are in closed form. A comprehensive table of existing computer software is provided. We used RELEASE for data summary and goodness\textemdash of\textemdash fit tests and SURGE for iterative model fitting and the computation of likelihood ratio tests. Five increasingly complex examples are given to illustrate the theory. The first, using two data sets on the European Dipper (Cinclus cinclus), tests for sex\textemdash specific parameters, explores a model with time\textemdash dependent survival rates, and finally uses a priori information to model survival allowing for an environmental variable. The second uses data on two colonies of the Swift (Apus apus), and shows how interaction terms can be modeled and assessed and how survival and recapture rates sometimes partly counterbalance each other. The third shows complex variation in survival rates across sexes and age classes in the roe deer (Capreolus capreolus), with a test of density dependence in annual survival rates. The fourth is an example of experimental density manipulation using the common lizard (Lacerta vivipara). The last example attempts to examine a large and complex data set on the Greater Flamingo (Phoenicopterus ruber), where parameters are age specific, survival is a function of an environmental variable, and an age {$\times$} year interaction term is important. Heterogeneity seems present in this example and cannot be adequately modeled with existing theory. The discussion presents a summary of the paradigm we recommend and details issues in model selection and design, and foreseeable future developments.},
  copyright = {\textcopyright{} 1992 by the Ecological Society of America},
  langid = {english},
  annotation = {{$\_$}eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.2307/2937171},
  file = {/Users/amanderson/Zotero/storage/FFRLTUGF/Lebreton et al. - 1992 - Modeling Survival and Testing Biological Hypothese.pdf;/Users/amanderson/Zotero/storage/EW3NWK2X/2937171.html}
}

@article{lee_bayesian_2020,
  title = {Bayesian Joint Inference for Multiple Directed Acyclic Graphs},
  author = {Lee, Kyoungjae and Cao, Xuan},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.06190 [stat]},
  eprint = {2008.06190},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In many applications, data often arise from multiple groups that may share similar characteristics. A joint estimation method that models several groups simultaneously can be more efficient than estimating parameters in each group separately. We focus on unraveling the dependence structures of data based on directed acyclic graphs and propose a Bayesian joint inference method for multiple graphs. To encourage similar dependence structures across all groups, a Markov random field prior is adopted. We establish the joint selection consistency of the fractional posterior in high dimensions, and benefits of the joint inference are shown under the common support assumption. This is the first Bayesian method for joint estimation of multiple directed acyclic graphs. The performance of the proposed method is demonstrated using simulation studies, and it is shown that our joint inference outperforms other competitors. We apply our method to an fMRI data for simultaneously inferring multiple brain functional networks.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,uncited-but-relevant},
  file = {/Users/amanderson/Zotero/storage/YN5ME8EN/Lee and Cao - 2020 - Bayesian joint inference for multiple directed acy.pdf;/Users/amanderson/Zotero/storage/XGGRR3I6/2008.html}
}

@article{lee_model_2000,
  title = {A Model for Markers and Latent Health Status},
  author = {Lee, Mel-Ling Ting and DeGruttola, Victor and Schoenfeld, David},
  year = {2000},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {62},
  number = {4},
  pages = {747--762},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00261},
  abstract = {We extend the bivariate Wiener process considered by Whitmore and co-workers and model the joint process of a marker and health status. The health status process is assumed to be latent or unobservable. The time to reach the primary end point or failure (death, onset of disease, etc.) is the time when the latent health status process first crosses a failure threshold level. Inferences for the model are based on two kinds of data: censored survival data and marker measurements. Covariates, such as treatment variables, risk factors and base-line conditions, are related to the model parameters through generalized linear regression functions. The model offers a much richer potential for the study of treatment efficacy than do conventional models. Treatment effects can be assessed in terms of their influence on both the failure threshold and the health status process parameters. We derive an explicit formula for the prediction of residual failure times given the current marker level. Also we discuss model validation. This model does not require the proportional hazards assumption and hence can be widely used. To demonstrate the usefulness of the model, we apply the methods in analysing data from the protocol 116a of the AIDS Clinical Trials Group.},
  copyright = {2000 Royal Statistical Society},
  langid = {english},
  keywords = {Acquired immune deficiency syndrome,Bivariate Wiener process,Disease progression,Health status,Latent process,Marker,Prediction,Proportional hazards,read,Survival analysis,uncited-but-relevant},
  annotation = {{$\_$}eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00261},
  file = {/Users/amanderson/Zotero/storage/XP9QS3YP/1467-9868.html}
}

@article{lee_threshold_2006,
  title = {Threshold Regression for Survival Analysis: Modeling Event Times by a Stochastic Process Reaching a Boundary},
  shorttitle = {Threshold Regression for Survival Analysis},
  author = {Lee, Mei-Ling Ting and Whitmore, G. A.},
  year = {2006},
  journal = {Statistical Science},
  volume = {21},
  number = {4},
  pages = {501--513},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {Many researchers have investigated first hitting times as models for survival data. First hitting times arise naturally in many types of stochastic processes, ranging from Wiener processes to Markov chains. In a survival context, the state of the underlying process represents the strength of an item or the health of an individual. The item fails or the individual experiences a clinical endpoint when the process reaches an adverse threshold state for the first time. The time scale can be calendar time or some other operatioal measure of degradation or disease progression. In many applications, the process is latent (i.e., unobservable). Threshold regression refers to first-hitting-time models with regression structures that accommodate covariate data. The parameters of the process, threshold state and time scale may depend on the covariates. This paper reviews aspects of this topic and discusses fruitful avenues for future research.},
  keywords = {read},
  file = {/Users/amanderson/Zotero/storage/HP88FZQP/Lee and Whitmore - 2006 - Threshold Regression for Survival Analysis Modeli.pdf}
}

@article{lee_threshold_2010,
  ids = {lee_threshold_2010-1},
  title = {Threshold Regression for Survival Data with Time-Varying Covariates},
  author = {Lee, Mei-Ling Ting and Whitmore, G. A. and Rosner, Bernard A.},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {7-8},
  pages = {896--905},
  issn = {1097-0258},
  doi = {10.1002/sim.3808},
  abstract = {Time-to-event data with time-varying covariates pose an interesting challenge for statistical modeling and inference, especially where the data require a regression structure but are not consistent with the proportional hazard assumption. Threshold regression (TR) is a relatively new methodology based on the concept that degradation or deterioration of a subject's health follows a stochastic process and failure occurs when the process first reaches a failure state or threshold (a first-hitting-time). Survival data with time-varying covariates consist of sequential observations on the level of degradation and/or on covariates of the subject, prior to the occurrence of the failure event. Encounters with this type of data structure abound in practical settings for survival analysis and there is a pressing need for simple regression methods to handle the longitudinal aspect of the data. Using a Markov property to decompose a longitudinal record into a series of single records is one strategy for dealing with this type of data. This study looks at the theoretical conditions for which this Markov approach is valid. The approach is called threshold regression with Markov decomposition or Markov TR for short. A number of important special cases, such as data with unevenly spaced time points and competing risks as stopping modes, are discussed. We show that a proportional hazards regression model with time-varying covariates is consistent with the Markov TR model. The Markov TR procedure is illustrated by a case application to a study of lung cancer risk. The procedure is also shown to be consistent with the use of an alternative time scale. Finally, we present the connection of the procedure to the concept of a collapsible survival model. Copyright \textcopyright{} 2010 John Wiley {$\&$} Sons, Ltd.},
  langid = {english},
  keywords = {competing risks,first-hitting-time,latent process,longitudinal data,Markov property,stopping time,unevenly spaced time points,Wiener diffusion process},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3808},
  file = {/Users/amanderson/Zotero/storage/6XCR5ZI7/Lee et al. - 2010 - Threshold regression for survival data with time-v.pdf;/Users/amanderson/Zotero/storage/ABRRR5PH/Lee et al. - 2010 - Threshold regression for survival data with time-v.pdf;/Users/amanderson/Zotero/storage/48FHSNG8/sim.html;/Users/amanderson/Zotero/storage/RNJCBSZP/sim.html}
}

@article{leung_censoring_1997,
  title = {Censoring Issues in Survival Analysis},
  author = {Leung, Kwan-Moon and Elashoff, Robert M. and Afifi, Abdelmonem A.},
  year = {1997},
  month = may,
  journal = {Annual Review of Public Health},
  volume = {18},
  number = {1},
  pages = {83--104},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev.publhealth.18.1.83},
  abstract = {A key characteristic that distinguishes survival analysis from other areas in statistics is that survival data are usually censored. Censoring occurs when incomplete information is available about the survival time of some individuals. We define censoring through some practical examples extracted from the literature in various fields of public health. With few exceptions, the censoring mechanisms in most observational studies are unknown and hence it is necessary to make assumptions about censoring when the common statistical methods are used to analyze censored data. In addition, we present situations in which censoring mechanisms can be ignored. The effects of the censoring assumptions are demonstrated through actual studies.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/TXRIQ3YH/Leung et al. - 1997 - CENSORING ISSUES IN SURVIVAL ANALYSIS.pdf}
}

@article{li_generalized_2020,
  title = {Generalized Liquid Association Analysis for Multimodal Data Integration},
  author = {Li, Lexin and Zeng, Jing and Zhang, Xin},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.03733 [stat]},
  eprint = {2008.03733},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Multimodal data are now prevailing in scientific research. A central question in multimodal integrative analysis is to understand how two data modalities associate and interact with each other given another modality or demographic covariates. The problem can be formulated as studying the associations among three sets of random variables, a question that has received relatively less attention in the literature. In this article, we propose a novel generalized liquid association analysis method, which offers a new and unique angle to this important class of problem of studying three-way associations. We extend the notion of liquid association of Li (2002) from the univariate setting to the multivariate and high-dimensional setting. We establish a population dimension reduction model, transform the problem to sparse Tucker decomposition of a three-way tensor, and develop a higher-order singular value decomposition estimation algorithm. We derive the non-asymptotic error bound and asymptotic consistency of the proposed estimator, while allowing the variable dimensions to be larger than and diverge with the sample size. We demonstrate the efficacy of the method through both simulations and a multimodal neuroimaging application for Alzheimer's disease research.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,uncited-but-relevant},
  file = {/Users/amanderson/Zotero/storage/ICGDSL4B/Li et al. - 2020 - Generalized Liquid Association Analysis for Multim.pdf;/Users/amanderson/Zotero/storage/3FEV58QX/2008.html}
}

@article{lin_recent_2014,
  title = {Recent Developments on the Construction of Bivariate Distributions with Fixed Marginals},
  author = {Lin, Gwo and Dou, Xiaoling and Kuriki, Satoshi and Huang, Jin-Sheng},
  year = {2014},
  journal = {Journal of Statistical Distributions and Applications},
  volume = {1},
  number = {1},
  pages = {14},
  issn = {2195-5832},
  doi = {10.1186/2195-5832-1-14},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/6B9NYZX6/Lin et al. - 2014 - Recent developments on the construction of bivaria.pdf;/Users/amanderson/Zotero/storage/FHVWL3LW/Lin et al. - 2014 - Recent developments on the construction of bivaria.pdf}
}

@book{lin2014past,
  title = {Past, Present, and Future of Statistical Science},
  author = {Lin, X. and Genest, C. and Banks, D.L. and Molenberghs, G. and Scott, D.W. and Wang, J.L.},
  year = {2014},
  publisher = {{CRC Press}},
  isbn = {978-1-4822-0498-8},
  lccn = {2014003735}
}

@article{lindsten_divide-and-conquer_2017,
  title = {Divide-and-Conquer with Sequential {{Monte Carlo}}},
  author = {Lindsten, F. and Johansen, A. M. and Naesseth, C. A. and Kirkpatrick, B. and Sch{\"o}n, T. B. and Aston, J. A. D. and {Bouchard-C{\^o}t{\'e}}, A.},
  year = {2017},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {2},
  pages = {445--458},
  publisher = {{Taylor {$\&$} Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2016.1237363},
  abstract = {We propose a novel class of Sequential Monte Carlo (SMC) algorithms, appropriate for inference in probabilistic graphical models. This class of algorithms adopts a divide-and-conquer approach based upon an auxiliary tree-structured decomposition of the model of interest, turning the overall inferential task into a collection of recursively solved subproblems. The proposed method is applicable to a broad class of probabilistic graphical models, including models with loops. Unlike a standard SMC sampler, the proposed divide-and-conquer SMC employs multiple independent populations of weighted particles, which are resampled, merged, and propagated as the method progresses. We illustrate empirically that this approach can outperform standard methods in terms of the accuracy of the posterior expectation and marginal likelihood approximations. Divide-and-conquer SMC also opens up novel parallel implementation options and the possibility of concentrating the computational effort on the most challenging subproblems. We demonstrate its performance on a Markov random field and on a hierarchical logistic regression problem. Supplementary materials including proofs and additional numerical results are available online.},
  keywords = {Bayesian methods,Graphical models,Hierarchical models,Particle filters},
  annotation = {{$\_$}eprint: https://doi.org/10.1080/10618600.2016.1237363},
  file = {/Users/amanderson/Zotero/storage/3J9JP2VI/Lindsten et al. - 2017 - Divide-and-Conquer With Sequential Monte Carlo.pdf;/Users/amanderson/Zotero/storage/GTBYA2C3/Lindsten et al. - 2017 - Divide-and-Conquer With Sequential Monte Carlo.pdf;/Users/amanderson/Zotero/storage/VPRQXGYW/10618600.2016.html}
}

@article{liu_modularization_2009,
  title = {Modularization in {{Bayesian}} Analysis, with Emphasis on Analysis of Computer Models},
  author = {Liu, F. and Bayarri, M. J. and Berger, J. O.},
  year = {2009},
  journal = {Bayesian Analysis},
  volume = {4},
  number = {1},
  pages = {119--150},
  publisher = {{International Society for Bayesian Analysis}},
  doi = {10.1214/09-BA404},
  abstract = {Bayesian analysis incorporates different sources of information into a single analysis through Bayes theorem. When one or more of the sources of information are suspect (e.g., if the model assumed for the information is viewed as quite possibly being significantly flawed), there can be a concern that Bayes theorem allows this suspect information to overly influence the other sources of information. We consider a variety of situations in which this arises, and give methodological suggestions for dealing with the problem.},
  file = {/Users/amanderson/Zotero/storage/R4HQBPNK/Liu et al. - 2009 - Modularization in Bayesian analysis, with emphasis.pdf}
}

@article{lu_using_1993,
  title = {Using Degradation Measures to Estimate a Time-to-Failure Distribution},
  author = {Lu, C. Joseph and Meeker, William Q.},
  year = {1993},
  journal = {Technometrics},
  volume = {35},
  number = {2},
  pages = {161--174},
  publisher = {{[Taylor {$\&$} Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269661},
  abstract = {Some life tests result in few or no failures. In such cases, it is difficult to assess reliability with traditional life tests that record only time to failure. For some devices, it is possible to obtain degradation measurements over time, and these measurements may contain useful information about product reliability. Even with little or no censoring, there may be important practical advantages to analyzing degradation data. If failure is defined in terms of a specified level of degradation, a degradation model defines a particular time-to-failure distribution. Generally it is not possible to obtain a closed-form expression for this distribution. The purpose of this work is to develop statistical methods for using degradation measures to estimate a time-to-failure distribution for a broad class of degradation models. We use a nonlinear mixed-effects model and develop methods based on Monte Carlo simulation to obtain point estimates and confidence intervals for reliability assessment.},
  file = {/Users/amanderson/Zotero/storage/SEEB7LS4/Lu and Meeker - 1993 - Using Degradation Measures to Estimate a Time-to-F.pdf}
}

@article{lunn_bugs_2009,
  title = {The {{BUGS}} Project: Evolution, Critique and Future Directions},
  author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {25},
  pages = {3049--3067},
  publisher = {{Wiley}},
  doi = {10.1002/sim.3680}
}

@article{lunn:etal:13,
  ids = {lunn_fully_2013,lunn_fully_2013-1},
  title = {Fully {{Bayesian}} Hierarchical Modelling in Two Stages, with Application to Meta-Analysis},
  author = {Lunn, David and Barrett, Jessica and Sweeting, Michael and Thompson, Simon},
  year = {2013},
  journal = {Journal of the Royal Statistical Society Series C},
  volume = {62},
  number = {4},
  pages = {551--572},
  doi = {10.1111/rssc.12007},
  keywords = {Abdominal aortic aneurysm,Bayesian hierarchical modelling,BUGS,Markov chain Monte Carlo methods,Random-effects meta-analysis},
  file = {/Users/amanderson/Zotero/storage/QYB4IHYS/Lunn et al. - 2013 - Fully Bayesian hierarchical modelling in two stage.pdf;/Users/amanderson/Zotero/storage/VZTANCUG/Lunn et al. - 2013 - Fully Bayesian hierarchical modelling in two stage.pdf;/Users/amanderson/Zotero/storage/U24D3CW8/rssc.html}
}

@article{ma_current_2019,
  title = {Current Methods for Quantifying Drug Synergism},
  author = {Ma, Jun and {Motsinger-Reif}, Alison},
  year = {2019},
  month = jul,
  journal = {Proteomics {$\&$} Bioinformatics: Current Research},
  volume = {1},
  number = {2},
  pages = {43--48},
  issn = {2641-7561},
  abstract = {The effectiveness of drug combinations for treatment of a variety of complex diseases is well established. "Drug cocktail" treatments are often prescribed to improve the overall efficacy, decrease toxicity, alter pharmacodynamics, etc in an overall treatment strategy. Specifically, if when combined, drugs interact in some way that causes the total effect to be greater than that predicted by their individual potencies, then drugs are considered synergistic. While there are established ways to quantify the impact of drug combinations clinically, it is an open challenge to quantitatively summarize a synergistic interaction. In this paper, we discuss an overview of the current statistical and mathematical methods for the study of drug combination effects, especially drug synergy quantification (where the interaction effect is not just detected, but quantified according to its magnitude). We first introduce two popular reference models for testing to null hypothesis of non-interaction for a combination, including the Bliss independence model and the Loewe additivity model. Then we discuss several methods for quantifying drug synergism. The advantages and disadvantages with these methods are also provided, and finally, we discuss important next directions in this area.},
  langid = {english},
  pmcid = {PMC7010330},
  pmid = {32043089},
  keywords = {Bliss,Combination Index,Drug combinations,Loewe,Synergy},
  file = {/Users/amanderson/Zotero/storage/U3P7KZR7/Ma and Motsinger-Reif - 2019 - Current Methods for Quantifying Drug Synergism.pdf}
}

@article{manderson_combining_2021,
  title = {Combining Chains of {{Bayesian}} Models with {{Markov}} Melding},
  author = {Manderson, Andrew A. and Goudie, Robert J. B.},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.11566 [stat]},
  eprint = {2111.11566},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {A challenge for practitioners of Bayesian inference is specifying a model that incorporates multiple relevant, heterogeneous data. It may be easier to instead specify distinct submodels for each source of data, then join the submodels together. We consider chains of submodels, where submodels directly relate to their neighbours via common quantities which may be parameters or deterministic functions thereof. We propose chained Markov melding, an extension of Markov melding, a generic method to combine chains of submodels into a joint model. One challenge we address is appropriately capturing the prior dependence between common quantities within a submodel, whilst also reconciling differences in priors for the same common quantity between two adjacent submodels. Estimating the posterior of the resulting overall joint model is also challenging, so we describe a sampler that uses the chain structure to incorporate information contained in the submodels in multiple stages, possibly in parallel. We demonstrate our methodology using two examples. The first example considers an ecological integrated population model, where multiple data are required to accurately estimate population immigration and reproduction rates. We also consider a joint longitudinal and time-to-event model with uncertain, submodel-derived event times. Chained Markov melding is a conceptually appealing approach to integrating submodels in these settings.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  note = {Comment: 32 pages, 14 figures},
  file = {/Users/amanderson/Zotero/storage/HKHYRY85/Manderson and Goudie - 2021 - Combining chains of Bayesian models with Markov me.pdf;/Users/amanderson/Zotero/storage/NRQEF9QE/2111.html}
}

@article{manderson_numerically_2021,
  title = {A Numerically Stable Algorithm for Integrating {{Bayesian}} Models Using {{Markov}} Melding},
  author = {Manderson, Andrew A. and Goudie, Robert J. B.},
  year = {2021},
  month = sep,
  journal = {arXiv:2001.08038 [stat]},
  eprint = {2001.08038},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {When statistical analyses consider multiple data sources, Markov melding provides a method for combining the source-specific Bayesian models. Markov melding joins together submodels that have a common quantity. One challenge is that the prior for this quantity can be implicit, and its prior density must be estimated. We show that error in this density estimate makes the two-stage Markov chain Monte Carlo sampler employed by Markov melding unstable and unreliable. We propose a robust two-stage algorithm that estimates the required prior marginal self-density ratios using weighted samples, dramatically improving accuracy in the tails of the distribution. The stabilised version of the algorithm is pragmatic and provides reliable inference. We demonstrate our approach using an evidence synthesis for inferring HIV prevalence, and an evidence synthesis of A/H1N1 influenza.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/NK3YXZC9/Manderson and Goudie - 2021 - A numerically stable algorithm for integrating Bay.pdf;/Users/amanderson/Zotero/storage/FABAUZY4/2001.html}
}

@article{massa_algebraic_2017,
  title = {Algebraic Representations of {{Gaussian Markov}} Combinations},
  author = {Massa, M. Sofia and Riccomagno, Eva},
  year = {2017},
  journal = {Bernoulli},
  volume = {23},
  number = {1},
  pages = {626--644},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  doi = {10.3150/15-BEJ759},
  file = {/Users/amanderson/Zotero/storage/B4FS8MNY/Massa and Riccomagno - 2017 - Algebraic representations of Gaussian Markov combi.pdf}
}

@incollection{massa_combining_2010,
  title = {Combining Statistical Models},
  booktitle = {Algebraic Methods in Statistics and Probability {{II}}},
  author = {Massa, M. Sofia and Lauritzen, Steffen L.},
  year = {2010},
  series = {Contemp. {{Math}}.},
  volume = {516},
  pages = {239--259},
  publisher = {{Amer. Math. Soc., Providence, RI}},
  doi = {10.1090/conm/516/10179},
  mrnumber = {2730753},
  keywords = {uncited-but-relevant},
  file = {/Users/amanderson/Zotero/storage/89W637RX/Massa and Lauritzen - 2010 - Combining statistical models.pdf}
}

@article{mauff_joint_2020,
  title = {Joint Models with Multiple Longitudinal Outcomes and a Time-to-Event Outcome: A Corrected Two-Stage Approach},
  shorttitle = {Joint Models with Multiple Longitudinal Outcomes and a Time-to-Event Outcome},
  author = {Mauff, Katya and Steyerberg, Ewout and Kardys, Isabella and Boersma, Eric and Rizopoulos, Dimitris},
  year = {2020},
  month = jul,
  journal = {Statistics and Computing},
  volume = {30},
  number = {4},
  pages = {999--1014},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09927-9},
  abstract = {Joint models for longitudinal and survival data have gained a lot of attention in recent years, with the development of myriad extensions to the basic model, including those which allow for multivariate longitudinal data, competing risks and recurrent events. Several software packages are now also available for their implementation. Although mathematically straightforward, the inclusion of multiple longitudinal outcomes in the joint model remains computationally difficult due to the large number of random effects required, which hampers the practical application of this extension. We present a novel approach that enables the fitting of such models with more realistic computational times. The idea behind the approach is to split the estimation of the joint model in two steps: estimating a multivariate mixed model for the longitudinal outcomes and then using the output from this model to fit the survival submodel. So-called two-stage approaches have previously been proposed and shown to be biased. Our approach differs from the standard version, in that we additionally propose the application of a correction factor, adjusting the estimates obtained such that they more closely resemble those we would expect to find with the multivariate joint model. This correction is based on importance sampling ideas. Simulation studies show that this corrected two-stage approach works satisfactorily, eliminating the bias while maintaining substantial improvement in computational time, even in more difficult settings.},
  langid = {english},
  keywords = {read},
  file = {/Users/amanderson/Zotero/storage/FQDMBAWX/Mauff et al. - 2020 - Joint models with multiple longitudinal outcomes a.pdf}
}

@article{meier_discrete_2003,
  title = {Discrete Proportional Hazards Models for Mismeasured Outcomes},
  author = {Meier, Amalia S. and Richardson, Barbra A. and Hughes, James P.},
  year = {2003},
  journal = {Biometrics},
  volume = {59},
  number = {4},
  pages = {947--954},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2003.00109.x},
  abstract = {Summary. Outcome mismeasurement can lead to biased estimation in several contexts. Magder and Hughes (1997, American Journal of Epidemiology146, 195\textendash 203) showed that failure to adjust for imperfect outcome measures in logistic regression analysis can conservatively bias estimation of covariate effects, even when the mismeasurement rate is the same across levels of the covariate. Other authors have addressed the need to account for mismeasurement in survival analysis in selected cases (Snapinn, 1998, Biometrics54, 209\textendash 218; Gelfand and Wang, 2000, Statistics in Medicine19, 1865\textendash 1879; Balasubramanian and Lagakos, 2001, Biometrics57, 1048\textendash 1058, 2003, Biometrika90, 171\textendash 182). We provide a general, more widely applicable, adjusted proportional hazards (APH) method for estimation of cumulative survival and hazard ratios in discrete time when the outcome is measured with error. We show that mismeasured failure status in a standard proportional hazards (PH) model can conservatively bias estimation of hazard ratios and that inference, in most practical situations, is more severely affected by poor specificity than by poor sensitivity. However, in simulations over a wide range of conditions, the APH method with correctly specified mismeasurement rates performs very well.},
  langid = {english},
  keywords = {Measurement error,Proportional hazards,read,Sensitivity,Specificity,Survival},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2003.00109.x},
  file = {/Users/amanderson/Zotero/storage/YFJ9DHBM/Meier et al. - 2003 - Discrete Proportional Hazards Models for Mismeasur.pdf;/Users/amanderson/Zotero/storage/EIYKBYU7/j.0006-341X.2003.00109.html}
}

@incollection{meng_trio_2014,
  title = {A Trio of Inference Problems That Could Win You a {{Nobel Prize}} in Statistics (If You Help Fund It)},
  booktitle = {Past, {{Present}}, and {{Future}} of {{Statistical Science}}},
  author = {Meng, Xiao-Li},
  editor = {Lin, Xihong and Genest, Christian and Banks, David L. and Molenberghs, Geert and Scott, David W. and Wang, Jane-Ling},
  year = {2014},
  month = mar,
  pages = {561--586},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/b16720-52},
  isbn = {978-0-429-17128-4},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/7KUP9XG5/Lin et al. - 2014 - A trio of inference problems that could win you a .pdf}
}

@inproceedings{neiswanger_asymptotically_2014,
  title = {Asymptotically {{Exact}}, {{Embarrassingly Parallel MCMC}}},
  booktitle = {Proceedings of the {{Thirtieth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Neiswanger, Willie and Wang, Chong and Xing, Eric P.},
  year = {2014},
  series = {{{UAI}}'14},
  pages = {623--632},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, United States}},
  isbn = {978-0-9749039-1-0}
}

@book{nelsen_introduction_2006,
  title = {An Introduction to Copulas},
  author = {Nelsen, Roger B.},
  year = {2006},
  edition = {Second},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28678-0},
  keywords = {Copulas (Mathematical statistics)},
  file = {/Users/amanderson/Zotero/storage/C83TRFIW/Nelsen - 2006 - An introduction to copulas.pdf}
}

@article{nicholson_interoperability_2021,
  title = {Interoperability of Statistical Models in Pandemic Preparedness: Principles and Reality},
  shorttitle = {Interoperability of Statistical Models in Pandemic Preparedness},
  author = {Nicholson, George and Blangiardo, Marta and Briers, Mark and Diggle, Peter J. and Fjelde, Tor Erlend and Ge, Hong and Goudie, Robert J. B. and Jersakova, Radka and King, Ruairidh E. and Lehmann, Brieuc C. L. and Mallon, Ann-Marie and Padellini, Tullia and Teh, Yee Whye and Holmes, Chris and Richardson, Sylvia},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.13730 [stat]},
  eprint = {2109.13730},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We present "interoperability" as a guiding framework for statistical modelling to assist policy makers asking multiple questions using diverse datasets in the face of an evolving pandemic response. Interoperability provides an important set of principles for future pandemic preparedness, through the joint design and deployment of adaptable systems of statistical models for disease surveillance using probabilistic reasoning. We illustrate this through case studies for inferring spatial-temporal coronavirus disease 2019 (COVID-19) prevalence and reproduction numbers in England.},
  archiveprefix = {arXiv},
  keywords = {62P10,Statistics - Applications,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/LDRNBYUD/Nicholson et al. - 2021 - Interoperability of statistical models in pandemic.pdf;/Users/amanderson/Zotero/storage/8Q4I9NL7/2109.html}
}

@inproceedings{niculescu-mizil_inductive_2012,
  title = {Inductive Transfer for {{Bayesian}} Network Structure Learning},
  booktitle = {Proceedings of {{ICML Workshop}} on {{Unsupervised}} and {{Transfer Learning}}},
  author = {{Niculescu-Mizil}, Alexandru and Caruana, Rich},
  year = {2012},
  month = jun,
  pages = {167--180},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {We study the multi-task Bayesian Network structure learning problem: given data for multiple related problems, learn a Bayesian Network structure for each of them, sharing information among the pro...},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/2QUTV3JL/Niculescu-Mizil and Caruana - 2012 - Inductive Transfer for Bayesian Network Structure .pdf;/Users/amanderson/Zotero/storage/QPI4KWW4/niculescu12a.html}
}

@misc{nimble_development_team_nimble_2019,
  title = {{{NIMBLE}}: {{MCMC}}, Particle Filtering, and Programmable Hierarchical Modeling},
  author = {{NIMBLE Development Team}},
  year = {2019},
  doi = {10.5281/zenodo.1211190},
  note = {R package manual version 0.9.0}
}

@article{nitzan_pulse_2014,
  title = {Pulse Oximetry: Fundamentals and Technology Update},
  shorttitle = {Pulse Oximetry},
  author = {Nitzan, Meir and Romem, Ayal and Koppel, Robert},
  year = {2014},
  month = jul,
  journal = {Medical Devices (Auckland, N.Z.)},
  volume = {7},
  pages = {231--239},
  issn = {1179-1470},
  doi = {10.2147/MDER.S47319},
  abstract = {Oxygen saturation in the arterial blood (SaO2) provides information on the adequacy of respiratory function. SaO2 can be assessed noninvasively by pulse oximetry, which is based on photoplethysmographic pulses in two wavelengths, generally in the red and infrared regions. The calibration of the measured photoplethysmographic signals is performed empirically for each type of commercial pulse-oximeter sensor, utilizing in vitro measurement of SaO2 in extracted arterial blood by means of co-oximetry. Due to the discrepancy between the measurement of SaO2 by pulse oximetry and the invasive technique, the former is denoted as SpO2. Manufacturers of pulse oximeters generally claim an accuracy of 2{$\%$}, evaluated by the standard deviation (SD) of the differences between SpO2 and SaO2, measured simultaneously in healthy subjects. However, an SD of 2{$\%$} reflects an expected error of 4{$\%$} (two SDs) or more in 5{$\%$} of the examinations, which is in accordance with an error of 3{$\%$}\textendash 4{$\%$}, reported in clinical studies. This level of accuracy is sufficient for the detection of a significant decline in respiratory function in patients, and pulse oximetry has been accepted as a reliable technique for that purpose. The accuracy of SpO2 measurement is insufficient in several situations, such as critically ill patients receiving supplemental oxygen, and can be hazardous if it leads to elevated values of oxygen partial pressure in blood. In particular, preterm newborns are vulnerable to retinopathy of prematurity induced by high oxygen concentration in the blood. The low accuracy of SpO2 measurement in critically ill patients and newborns can be attributed to the empirical calibration process, which is performed on healthy volunteers. Other limitations of pulse oximetry include the presence of dyshemoglobins, which has been addressed by multiwavelength pulse oximetry, as well as low perfusion and motion artifacts that are partially rectified by sophisticated algorithms and also by reflection pulse oximetry.},
  pmcid = {PMC4099100},
  pmid = {25031547},
  file = {/Users/amanderson/Zotero/storage/2ID3G7DQ/Nitzan et al. - 2014 - Pulse oximetry fundamentals and technology update.pdf}
}

@article{oh_considerations_2018,
  title = {Considerations for Analysis of Time-to-Event Outcomes Measured with Error: Bias and Correction with {{SIMEX}}},
  shorttitle = {Considerations for Analysis of Time-to-Event Outcomes Measured with Error},
  author = {Oh, Eric J. and Shepherd, Bryan E. and Lumley, Thomas and Shaw, Pamela A.},
  year = {2018},
  month = apr,
  journal = {Statistics in medicine},
  volume = {37},
  number = {8},
  pages = {1276--1289},
  issn = {0277-6715},
  doi = {10.1002/sim.7554},
  abstract = {For time-to-event outcomes, a rich literature exists on the bias introduced by covariate measurement error in regression models, such as the Cox model, and methods of analysis to address this bias. By comparison, less attention has been given to understanding the impact or addressing errors in the failure time outcome. For many diseases, the timing of an event of interest (such as progression-free survival or time to AIDS progression) can be difficult to assess or reliant on self-report and therefore prone to measurement error. For linear models, it is well known that random errors in the outcome variable do not bias regression estimates. With non-linear models, however, even random error or misclassification can introduce bias into estimated parameters. We compare the performance of two common regression models, the Cox and Weibull models, in the setting of measurement error in the failure time outcome. We introduce an extension of the SIMEX method to correct for bias in hazard ratio estimates from the Cox model and discuss other analysis options to address measurement error in the response. A formula to estimate the bias induced into the hazard ratio by classical measurement error in the event time for a log-linear survival model is presented. Detailed numerical studies are presented to examine the performance of the proposed SIMEX method under varying levels and parametric forms of the error in the outcome. We further illustrate the method with observational data on HIV outcomes from the Vanderbilt Comprehensive Care Clinic.},
  pmcid = {PMC5810403},
  pmid = {29193180},
  keywords = {accelerated failure time,Cox model,measurement error,read,SIMEX,survival analysis},
  file = {/Users/amanderson/Zotero/storage/8TKZG2R2/Oh et al. - 2018 - Considerations for analysis of time-to-event outco.pdf;/Users/amanderson/Zotero/storage/SSEHCERZ/Oh et al. - 2018 - Considerations for Analysis of Time-to-Event Outco.pdf;/Users/amanderson/Zotero/storage/4IQ5GI3G/sim.html}
}

@article{oh_raking_2021,
  title = {Raking and Regression Calibration: Methods to Address Bias from Correlated Covariate and Time-to-Event Error},
  shorttitle = {Raking and Regression Calibration},
  author = {Oh, Eric J. and Shepherd, Bryan E. and Lumley, Thomas and Shaw, Pamela A.},
  year = {2021},
  journal = {Statistics in Medicine},
  volume = {40},
  number = {3},
  pages = {631--649},
  issn = {1097-0258},
  doi = {10.1002/sim.8793},
  abstract = {Medical studies that depend on electronic health records (EHR) data are often subject to measurement error, as the data are not collected to support research questions under study. These data errors, if not accounted for in study analyses, can obscure or cause spurious associations between patient exposures and disease risk. Methodology to address covariate measurement error has been well developed; however, time-to-event error has also been shown to cause significant bias, but methods to address it are relatively underdeveloped. More generally, it is possible to observe errors in both the covariate and the time-to-event outcome that are correlated. We propose regression calibration (RC) estimators to simultaneously address correlated error in the covariates and the censored event time. Although RC can perform well in many settings with covariate measurement error, it is biased for nonlinear regression models, such as the Cox model. Thus, we additionally propose raking estimators which are consistent estimators of the parameter defined by the population estimating equation. Raking can improve upon RC in certain settings with failure-time data, require no explicit modeling of the error structure, and can be utilized under outcome-dependent sampling designs. We discuss features of the underlying estimation problem that affect the degree of improvement the raking estimator has over the RC approach. Detailed simulation studies are presented to examine the performance of the proposed estimators under varying levels of signal, error, and censoring. The methodology is illustrated on observational EHR data on HIV outcomes from the Vanderbilt Comprehensive Care Clinic.},
  langid = {english},
  keywords = {calibration,electronic health records,measurement error,misclassification,raking,survival analysis},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8793},
  file = {/Users/amanderson/Zotero/storage/87SZEKP8/Oh et al. - 2021 - Raking and regression calibration Methods to addr.pdf;/Users/amanderson/Zotero/storage/KTQKBV3G/sim.html}
}

@book{ohagan_uncertain_2006,
  ids = {ohagan:etal:06},
  title = {Uncertain Judgements: Eliciting Experts' Probabilities},
  author = {O'Hagan, A. and Buck, C.E. and Daneshkhah, A. and Eiser, J.R. and Garthwaite, P.H. and Jenkinson, D.J. and Oakley, J.E. and Rakow, T.},
  year = {2006},
  series = {Statistics in {{Practice}}},
  publisher = {{Wiley}},
  doi = {10.1002/0470033312},
  isbn = {978-0-470-03330-2},
  keywords = {Bayesian statistical decision theory,Distribution (Probability theory),Mathematical statistics,Probabilities,Statistics},
  file = {/Users/amanderson/Zotero/storage/C8CJ2XL5/O'Hagan - 2006 - Uncertain judgements eliciting experts' probabili.pdf}
}

@article{pandharipande_derivation_2009-1,
  title = {Derivation and Validation of {{Spo2}}/{{Fio2}} Ratio to Impute for {{Pao2}}/{{Fio2}} Ratio in the Respiratory Component of the {{Sequential Organ Failure Assessment}} Score},
  author = {Pandharipande, Pratik P. and Shintani, Ayumi K. and Hagerman, Heather E. and St Jacques, Paul J. and Rice, Todd W. and Sanders, Neal W. and Ware, Lorraine B. and Bernard, Gordon R. and Ely, E. Wesley},
  year = {2009},
  month = apr,
  journal = {Critical Care Medicine},
  volume = {37},
  number = {4},
  pages = {1317--1321},
  issn = {0090-3493},
  doi = {10.1097/CCM.0b013e31819cefa9},
  abstract = {Objective:{$~$}         The Sequential Organ Failure Assessment (SOFA) score is validated to measure severity of organ dysfunction in critically ill patients. However, in some practice settings, daily arterial blood gas data required to calculate the respiratory component of the SOFA score are often unavailable. The objectives of this study were to derive Spo2/Fio2 (SF) ratio correlations with the Pao2/Fio2 (PF) ratio to calculate the respiratory parameter of the SOFA score, and to validate the respiratory SOFA obtained using SF ratios against clinical outcomes.         Patients and Measurements:{$~$}         We obtained matched measurements of Spo2 and Pao2 from two populations: group 1\textemdash patients undergoing general anesthesia and group 2\textemdash patients from the acute respiratory distress syndrome network\textemdash low-vs. high-tidal volume for the acute respiratory management of acute respiratory distress syndrome database. Using a linear regression model, we first determined SF ratios corresponding to PF ratios of 100, 200, 300, and 400. Second, we evaluated the contribution of positive end-expiratory pressure (PEEP) on the relationship between SF and PF, for patients on PEEP in centimeters of water (cm H2O) of {$<$}8, 8-12, and {$>$}12. Third, we calculated the SOFA scores in a separate cohort of intensive care unit patients using the derived SF ratios and validated them against clinical outcomes.         Results:{$~$}         The total SOFA scores calculated using SF ratios and PF ratios were highly correlated (Spearman's rho 0.85, p {$<$} 0.001) in all patients and in the three stratified PEEP categories ({$<$}8 cm H2O, Spearman's rho 0.87, p {$<$} 0.001; PEEP 8-12 cm H20, Spearman's rho 0.85, p {$<$} 0.001; PEEP {$>$}12 cm H2O, Spearman's rho 0.85, p {$<$} 0.001). The respiratory SOFA scores based on SF ratios and PF ratios correlated similarly with intensive care unit length of stay and ventilator-free days, when validated in a cohort of critically ill patients.         Conclusion:{$~$}         The total and respiratory SOFA scores obtained with imputed SF values correlate with the corresponding SOFA score using PF ratios. Both the derived and original respiratory SOFA scores similarly predict outcomes.},
  langid = {american},
  file = {/Users/amanderson/Zotero/storage/8XH2DGET/Pandharipande et al. - 2009 - Derivation and validation of Spo2Fio2 ratio to im.pdf;/Users/amanderson/Zotero/storage/GQ8ZLLY2/Derivation_and_validation_of_Spo2_Fio2_ratio_to.21.html}
}

@article{parsons_bayesian_2021,
  title = {A {{Bayesian}} Hierarchical Modeling Approach to Combining Multiple Data Sources: {{A}} Case Study in Size Estimation},
  shorttitle = {A {{Bayesian}} Hierarchical Modeling Approach to Combining Multiple Data Sources},
  author = {Parsons, Jacob and Niu, Xiaoyue and Bao, Le},
  year = {2021},
  month = jul,
  journal = {arXiv:2012.05346 [stat]},
  eprint = {2012.05346},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {To combat the HIV/AIDS pandemic effectively, targeted interventions among certain key populations play a critical role. Examples of such key populations include sex workers, people who inject drugs, and men who have sex with men. While having accurate estimates for the size of these key populations is important, any attempt to directly contact or count members of these populations is difficult. As a result, indirect methods are used to produce size estimates. Multiple approaches for estimating the size of such populations have been suggested but often give conflicting results. It is therefore necessary to have a principled way to combine and reconcile these estimates. To this end, we present a Bayesian hierarchical model for estimating the size of key populations that combines multiple estimates from different sources of information. The proposed model makes use of multiple years of data and explicitly models the systematic error in the data sources used. We use the model to estimate the size of people who inject drugs in Ukraine. We evaluate the appropriateness of the model and compare the contribution of each data source to the final estimates.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/Users/amanderson/Zotero/storage/4GEXCITQ/Parsons et al. - 2021 - A Bayesian hierarchical modeling approach to combi.pdf;/Users/amanderson/Zotero/storage/FPAEVH49/2012.html}
}

@article{parsons_evaluating_2020-1,
  ids = {parsons_evaluating_2020},
  title = {Evaluating the Relative Contribution of Data Sources in a {{Bayesian}} Analysis with the Application of Estimating the Size of Hard to Reach Populations},
  author = {Parsons, Jacob and Niu, Xiaoyue and Bao, Le},
  year = {2020},
  month = sep,
  journal = {Statistical Communications in Infectious Diseases},
  volume = {12},
  number = {s1},
  eprint = {2009.02372},
  eprinttype = {arxiv},
  publisher = {{De Gruyter}},
  issn = {1948-4690},
  doi = {10.1515/scid-2019-0020},
  abstract = {Objectives When using multiple data sources in an analysis, it is important to understand the influence of each data source on the analysis and the consistency of the data sources with each other and the model. We suggest the use of a retrospective value of information framework in order to address such concerns. Methods Value of information methods can be computationally difficult. We illustrate the use of computational methods that allow these methods to be applied even in relatively complicated settings. In illustrating the proposed methods, we focus on an application in estimating the size of hard to reach populations. Specifically, we consider estimating the number of injection drug users in Ukraine by combining all available data sources spanning over half a decade and numerous sub-national areas in the Ukraine. This application is of interest to public health researchers as this hard to reach population that plays a large role in the spread of HIV. Results and conclusions We apply a Bayesian hierarchical model and evaluate the contribution of each data source in terms of absolute influence, expected influence, and level of surprise. Finally we apply value of information methods to inform suggestions on future data collection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Bayesian analysis,injecting drug user,size estimation,Statistics - Applications,uncited-but-relevant,value of information},
  file = {/Users/amanderson/Zotero/storage/3H3BFRK2/Parsons et al. - 2020 - Evaluating the relative contribution of data sourc.pdf;/Users/amanderson/Zotero/storage/4HR6LPL3/Parsons et al. - 2020 - Evaluating the relative contribution of data sourc.pdf;/Users/amanderson/Zotero/storage/NC8EAYR9/2009.html}
}

@article{pennell_bayesian_2010,
  title = {Bayesian Random-Effects Threshold Regression with Application to Survival Data with Nonproportional Hazards},
  author = {Pennell, Michael L. and Whitmore, G. A. and Ting Lee, Mei-Ling},
  year = {2010},
  month = jan,
  journal = {Biostatistics},
  volume = {11},
  number = {1},
  pages = {111--126},
  publisher = {{Oxford Academic}},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxp041},
  abstract = {Abstract.  In epidemiological and clinical studies, time-to-event data often violate the assumptions of Cox regression due to the presence of time-dependent cov},
  langid = {english},
  keywords = {read,uncited-but-relevant},
  file = {/Users/amanderson/Zotero/storage/4UXLC6DT/Pennell et al. - 2010 - Bayesian random-effects threshold regression with .pdf;/Users/amanderson/Zotero/storage/226AYGYI/226250.html}
}

@article{plummer_cuts_2015,
  title = {Cuts in {{Bayesian}} Graphical Models},
  author = {Plummer, Martyn},
  year = {2015},
  month = jan,
  journal = {Statistics and Computing},
  volume = {25},
  number = {1},
  pages = {37--43},
  issn = {1573-1375},
  doi = {10.1007/s11222-014-9503-z},
  abstract = {The cut function defined by the OpenBUGS software is described as a ``valve'' that prevents feedback in Bayesian graphical models. It is shown that the MCMC algorithm applied by OpenBUGS in the presence of a cut function does not converge to a well-defined limiting distribution. However, it may be improved by using tempered transitions. The cut algorithm is compared with multiple imputation as a gold standard in a simple example.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/GRNKTBEV/Plummer - 2015 - Cuts in Bayesian graphical models.pdf}
}

@misc{plummer_rjags_2019,
  title = {Rjags: {{Bayesian}} Graphical Models Using {{MCMC}}},
  author = {Plummer, Martyn},
  year = {2019},
  annotation = {Note: R package version 4-10},
  note = {R package version 4-10}
}

@article{presanis_synthesising_2014,
  title = {Synthesising Evidence to Estimate Pandemic (2009) {{A}}/{{H1N1}} Influenza Severity in 2009\textendash 2011},
  author = {Presanis, Anne M. and Pebody, Richard G. and Birrell, Paul J. and Tom, Brian D. M. and Green, Helen K. and Durnall, Hayley and Fleming, Douglas and De Angelis, Daniela},
  year = {2014},
  journal = {Annals of Applied Statistics},
  volume = {8},
  number = {4},
  pages = {2378--2403},
  publisher = {{The Institute of Mathematical Statistics}},
  doi = {10.1214/14-AOAS775},
  file = {/Users/amanderson/Zotero/storage/RF3C7SX2/Presanis et al. - 2014 - Synthesising evidence to estimate pandemic (2009) .pdf}
}

@book{rizopoulos_joint_2012,
  ids = {rizopoulos_joint_nodate},
  title = {Joint Models for Longitudinal and Time-to-Event Data: With Applications in {{R}}},
  shorttitle = {Joint Models for Longitudinal and Time-to-Event Data},
  author = {Rizopoulos, Dimitris},
  year = {2012},
  month = jun,
  publisher = {{CRC Press}},
  abstract = {In longitudinal studies it is often of interest to investigate how a marker that is repeatedly measured in time is associated with a time to an event of interest, e.g., prostate cancer studies where longitudinal PSA level measurements are collected in conjunction with the time-to-recurrence. Joint Models for Longitudinal and Time-to-Event Data: With Applications in R provides a full treatment of random effects joint models for longitudinal and time-to-event outcomes that can be utilized to analyze such data. The content is primarily explanatory, focusing on applications of joint modeling, but sufficient mathematical details are provided to facilitate understanding of the key features of these models.  All illustrations put forward can be implemented in the R programming language via the freely available package JM written by the author. All the R code used in the book is available at: http://jmr.r-forge.r-project.org/},
  googlebooks = {xotIpb2duaMC},
  isbn = {978-1-4398-7286-4},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Medical / Epidemiology,Science / Life Sciences / Biology},
  file = {/Users/amanderson/Zotero/storage/SQGYQTBT/Rizopoulos - Joint Models for Longitudinal and Time-to-Event Da.pdf}
}

@article{rosenberg_hazard_1995,
  title = {Hazard Function Estimation Using {{B-splines}}},
  author = {Rosenberg, Philip S.},
  year = {1995},
  journal = {Biometrics},
  volume = {51},
  number = {3},
  pages = {874--887},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2532989},
  abstract = {A flexible parametric procedure is given to model the hazard function as a linear combination of cubic B-splines and to obtain maximum likelihood estimates from censored survival data. The approach yields smooth estimates of the hazard and survivorship functions that are intermediate in structure between strongly parametric and non-parametric models. A simple method is described for selecting the number and location of knots. Simulation results show favorable root mean square error compared to non-parametric estimates for both the hazard and survivorship functions. Three methods are given to calculate confidence intervals based on the delta method, profile likelihood, and bootstrap, respectively. The procedure is applied to estimate hazard rates for acquired immunodeficiency syndrome (AIDS) following infection with human immunodeficiency virus (HIV). Spline methods can accommodate complex censoring mechanisms such as those that arise in the AIDS setting. To illustrate, HIV infection incidence is estimated for a cohort of hemophiliacs in which the dates of HIV infection are interval-censored and some subjects were born after the onset of the HIV epidemic.},
  file = {/Users/amanderson/Zotero/storage/89WK7NBP/Rosenberg - 1995 - Hazard Function Estimation Using B-Splines.pdf}
}

@article{royston_flexible_2001,
  title = {Flexible Parametric Alternatives to the {{Cox}} Model, and More},
  author = {Royston, Patrick},
  year = {2001},
  month = nov,
  journal = {The Stata Journal},
  volume = {1},
  number = {1},
  pages = {1--28},
  publisher = {{SAGE Publications}},
  issn = {1536-867X},
  doi = {10.1177/1536867X0100100101},
  abstract = {Since its introduction to a wondering public in 1972, the Cox proportional hazards regression model has become an overwhelmingly popular tool in the analysis of censored survival data. However, some features of the Cox model may cause problems for the analyst or an interpreter of the data. They include the restrictive assumption of proportional hazards for covariate effects, and ``loss'' (non-estimation) of the baseline hazard function induced by conditioning on event times. In medicine, the hazard function is often of fundamental interest since it represents an important aspect of the time course of the disease in question. In the present article, the Stata implementation of a class of flexible parametric survival models recently proposed by Royston and Parmar (2001) will be described. The models start by assuming either proportional hazards or proportional odds (user-selected option). The baseline distribution function is modeled by restricted cubic regression spline in log time, and parameter estimation is by maximum likelihood. Model selection and choice of knots for the spline function are discussed. Interval-censored data and models in which one or more covariates have nonproportional effects are also supported by the software. Examples based on a study of prognostic factors in breast cancer are given.},
  langid = {english},
  keywords = {hazard function,parametric survival analysis,proportional hazards,proportional odds,st0001},
  file = {/Users/amanderson/Zotero/storage/NMFLADUN/Royston - 2001 - Flexible Parametric Alternatives to the Cox Model,.pdf}
}

@article{royston_flexible_2002,
  title = {Flexible Parametric Proportional-Hazards and Proportional-Odds Models for Censored Survival Data, with Application to Prognostic Modelling and Estimation of Treatment Effects},
  author = {Royston, Patrick and Parmar, Mahesh K. B.},
  year = {2002},
  journal = {Statistics in Medicine},
  volume = {21},
  number = {15},
  pages = {2175--2197},
  issn = {1097-0258},
  doi = {10.1002/sim.1203},
  abstract = {Modelling of censored survival data is almost always done by Cox proportional-hazards regression. However, use of parametric models for such data may have some advantages. For example, non-proportional hazards, a potential difficulty with Cox models, may sometimes be handled in a simple way, and visualization of the hazard function is much easier. Extensions of the Weibull and log-logistic models are proposed in which natural cubic splines are used to smooth the baseline log cumulative hazard and log cumulative odds of failure functions. Further extensions to allow non-proportional effects of some or all of the covariates are introduced. A hypothesis test of the appropriateness of the scale chosen for covariate effects (such as of treatment) is proposed. The new models are applied to two data sets in cancer. The results throw interesting light on the behaviour of both the hazard function and the hazard ratio over time. The tools described here may be a step towards providing greater insight into the natural history of the disease and into possible underlying causes of clinical events. We illustrate these aspects by using the two examples in cancer. Copyright \textcopyright{} 2002 John Wiley {$\&$} Sons, Ltd.},
  langid = {english},
  keywords = {parametric models,proportional hazards,proportional odds,splines,survival analysis},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1203},
  file = {/Users/amanderson/Zotero/storage/KFPV6UPY/Royston and Parmar - 2002 - Flexible parametric proportional-hazards and propo.pdf;/Users/amanderson/Zotero/storage/BNHGUIDF/sim.html}
}

@article{rufo_bayesian_2012,
  title = {A {{Bayesian}} Approach to Aggregate Experts' Initial Information},
  author = {Rufo, Mar{\'i}a Jes{\'u}s and P{\'e}rez, Carlos J. and Mart{\'i}n, Jacinto},
  year = {2012},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {6},
  number = {none},
  pages = {2362--2382},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/12-EJS752},
  abstract = {This paper provides a Bayesian procedure to aggregate experts' information in a group decision making context. The belief of each expert is elicited as a multivariate prior distribution. Then, linear and logarithmic combination methods are used to represent a consensus distribution. Anyway, the choice of the appropriate strategy will depend on the decision maker's judgements. A significant task when using opinion pooling is to find the optimal weights. In order to carry it out, a criterion based on Kullback-Leibler divergence is proposed. Furthermore, based on the previous idea, an alternative procedure is presented when a solution cannot be found. The theoretical foundations are discussed in detail for each aggregation scheme. In particular, it is shown that a general unified method is achieved when they are applied to multivariate natural exponential families. Finally, two illustrative examples show that the proposed techniques can be easily applied in practice and their usefulness for decision making under the described situations.},
  keywords = {62C10,62H99,Bayesian analysis,group decision,Kullback-Leibler divergence,multivariate exponential families,opinion pooling},
  file = {/Users/amanderson/Zotero/storage/FLY5YNH7/Rufo et al. - 2012 - A Bayesian approach to aggregate experts initial .pdf;/Users/amanderson/Zotero/storage/M4JKFRY8/12-EJS752.html}
}

@article{rufo_log-linear_2012,
  title = {Log-Linear Pool to Combine Prior Distributions: {{A}} Suggestion for a Calibration-Based Approach},
  shorttitle = {Log-Linear Pool to Combine Prior Distributions},
  author = {Rufo, M. J. and Mart{\'i}n, J. and P{\'e}rez, C. J.},
  year = {2012},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {7},
  number = {2},
  pages = {411--438},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/12-BA714},
  abstract = {An important issue involved in group decision making is the suitable aggregation of experts' beliefs about a parameter of interest. Two widely used combination methods are linear and log-linear pools. Yet, a problem arises when the weights have to be selected. This paper provides a general decision-based procedure to obtain the weights in a log-linear pooled prior distribution. The process is based on Kullback-Leibler divergence, which is used as a calibration tool. No information about the parameter of interest is considered before dealing with the experts' beliefs. Then, a pooled prior distribution is achieved, for which the expected calibration is the best one in the Kullback-Leibler sense. In the absence of other information available to the decision-maker prior to getting experimental data, the methodology generally leads to selection of the most diffuse pooled prior. In most cases, a problem arises from the marginal distribution related to the noninformative prior distribution since it is improper. In these cases, an alternative procedure is proposed. Finally, two applications show how the proposed techniques can be easily applied in practice.},
  keywords = {Bayesian analysis,Kullback-Leibler divergence,Pooled distribution},
  file = {/Users/amanderson/Zotero/storage/22QRRZD5/Rufo et al. - 2012 - Log-Linear Pool to Combine Prior Distributions A .pdf;/Users/amanderson/Zotero/storage/L739IGTX/12-BA714.html}
}

@article{rutherford_use_2015,
  title = {The Use of Restricted Cubic Splines to Approximate Complex Hazard Functions in the Analysis of Time-to-Event Data: A Simulation Study},
  shorttitle = {The Use of Restricted Cubic Splines to Approximate Complex Hazard Functions in the Analysis of Time-to-Event Data},
  author = {Rutherford, Mark J. and Crowther, Michael J. and Lambert, Paul C.},
  year = {2015},
  month = mar,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {85},
  number = {4},
  pages = {777--793},
  publisher = {{Taylor {$\&$} Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949655.2013.845890},
  abstract = {If interest lies in reporting absolute measures of risk from time-to-event data then obtaining an appropriate approximation to the shape of the underlying hazard function is vital. It has previously been shown that restricted cubic splines can be used to approximate complex hazard functions in the context of time-to-event data. The degree of complexity for the spline functions is dictated by the number of knots that are defined. We highlight through the use of a motivating example that complex hazard function shapes are often required when analysing time-to-event data. Through the use of simulation, we show that provided a sufficient number of knots are used, the approximated hazard functions given by restricted cubic splines fit closely to the true function for a range of complex hazard shapes. The simulation results also highlight the insensitivity of the estimated relative effects (hazard ratios) to the correct specification of the baseline hazard.},
  keywords = {flexible parametric models,restricted cubic splines,simulation,survival analysis},
  annotation = {{$\_$}eprint: https://doi.org/10.1080/00949655.2013.845890},
  file = {/Users/amanderson/Zotero/storage/P2YWGTXK/Rutherford et al. - 2015 - The use of restricted cubic splines to approximate.pdf;/Users/amanderson/Zotero/storage/T29X88F8/00949655.2013.html}
}

@article{schaub_local_2006,
  title = {Local Population Dynamics and the Impact of Scale and Isolation: A Study on Different Little Owl Populations},
  shorttitle = {Local Population Dynamics and the Impact of Scale and Isolation},
  author = {Schaub, Michael and Ullrich, Bruno and Kn{\"o}tzsch, Gerhard and Albrecht, Patrick and Meisser, Christian},
  year = {2006},
  journal = {Oikos},
  volume = {115},
  number = {3},
  pages = {389--400},
  issn = {1600-0706},
  doi = {10.1111/j.2006.0030-1299.15374.x},
  abstract = {The understanding of how variation of demographic rates translates into variation of population growth is a central aim in population ecology. Besides stochastic and deterministic factors, the spatial extent and the isolation of a local population may have an impact on the contribution of the different demographic components. Using long-term demographic data we performed retrospective population analyses of four little owl (Athene noctua) populations with differential spatial extent and degree of isolation to assess the contribution of demographic rates to the variation of the growth rate ({$\lambda$}) of each local population and to the difference of {$\lambda$} among populations. In all populations variation of fecundity contributed least to variation of {$\lambda$}, and variation of adult survival contributed most to variation of {$\lambda$} in three of four populations. Between population comparisons revealed that differences mainly stem from differences of immigration and juvenile local survival. The relative importance of immigration to {$\lambda$} tended to decrease with increasing spatial extent and isolation of the local populations. None of the four local populations was self-sustainable. Because the local populations export and import individuals, they can be considered as open recruitment systems in which part of the recruited breeding birds are not produced locally. The spatial extent and the degree of isolation of a local population have an impact on local population dynamics; hence these factors need to be considered in studies about local population dynamics and for deriving conservation measures.},
  langid = {english},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2006.0030-1299.15374.x},
  file = {/Users/amanderson/Zotero/storage/8XVKMBMY/Schaub et al. - 2006 - Local population dynamics and the impact of scale .pdf;/Users/amanderson/Zotero/storage/PNG6H748/j.2006.0030-1299.15374.html;/Users/amanderson/Zotero/storage/RWRAPQJ7/j.2006.0030-1299.15374.html}
}

@article{scott_bayes_2016-1,
  ids = {scott:etal:16,scott_bayes_2016},
  title = {Bayes and Big Data: The Consensus {{Monte Carlo}} Algorithm},
  shorttitle = {Bayes and Big Data},
  author = {Scott, Steven L. and Blocker, Alexander W. and Bonassi, Fernando V. and Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  year = {2016},
  journal = {International Journal of Management Science and Engineering Management},
  volume = {11},
  pages = {78--88},
  file = {/Users/amanderson/Zotero/storage/HVQZ66KJ/Scott et al. - 2016 - Bayes and big data the consensus Monte Carlo algo.pdf;/Users/amanderson/Zotero/storage/QQ5KEILU/Scott et al. - 2016 - Bayes and Big Data The Consensus Monte Carlo Algo.pdf}
}

@article{seethala_early_2017,
  title = {Early Risk Factors and the Role of Fluid Administration in Developing Acute Respiratory Distress Syndrome in Septic Patients},
  author = {Seethala, Raghu R. and Hou, Peter C. and Aisiku, Imoigele P. and Frendl, Gyorgy and Park, Pauline K. and Mikkelsen, Mark E. and Chang, Steven Y. and Gajic, Ognjen and Sevransky, Jonathan},
  year = {2017},
  month = jan,
  journal = {Annals of Intensive Care},
  volume = {7},
  number = {1},
  pages = {11},
  issn = {2110-5820},
  doi = {10.1186/s13613-017-0233-1},
  abstract = {Sepsis is a major risk factor for acute respiratory distress syndrome (ARDS). However, there remains a paucity of literature examining risk factors for ARDS in septic patients early in their course. This study examined the role of early fluid administration and identified other risk factors within the first 6{$~$}h of hospital presentation associated with developing ARDS in septic patients.},
  keywords = {Acute lung injury,Acute respiratory distress syndrome,Fluid resuscitation,Pneumonia,Sepsis},
  file = {/Users/amanderson/Zotero/storage/SHBVBQ2G/Seethala et al. - 2017 - Early risk factors and the role of fluid administr.pdf;/Users/amanderson/Zotero/storage/A423Q3I5/s13613-017-0233-1.html}
}

@article{sharef_bayesian_2010,
  title = {Bayesian Adaptive {{B-spline}} Estimation in Proportional Hazards Frailty Models},
  author = {Sharef, Emmanuel and Strawderman, Robert L. and Ruppert, David and Cowen, Mark and Halasyamani, Lakshmi},
  year = {2010},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {4},
  number = {none},
  issn = {1935-7524},
  doi = {10.1214/10-EJS566},
  abstract = {Frailty models derived from the proportional hazards regression model are frequently used to analyze clustered right-censored survival data. We propose a semiparametric Bayesian methodology for this purpose, modeling both the unknown baseline hazard and density of the random effects using mixtures of B-splines. The posterior distributions for all regression coefficients and spline parameters are obtained using Markov Chain Monte Carlo (MCMC). The methodology permits the use of weighted mixtures of parametric and nonparametric components in modeling the hazard function and frailty distribution; in addition, the spline knots may also be selected adaptively using reversible-jump MCMC. Simulations indicate that the method produces smooth and accurate posterior hazard and frailty density estimates. The Bayesian approach not only produces point estimators that outperform existing approaches in certain circumstances, but also offers a wealth of information about the parameters of interest in the form of MCMC samples from the joint posterior probability distribution. We illustrate the adaptability of the method with data from a study of congestive heart failure.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/U6GF8DK8/Sharef et al. - 2010 - Bayesian adaptive B-spline estimation in proportio.pdf}
}

@article{snapinn_survival_1998,
  title = {Survival Analysis with Uncertain Endpoints},
  author = {Snapinn, Steven M.},
  year = {1998},
  journal = {Biometrics},
  volume = {54},
  number = {1},
  pages = {209--218},
  issn = {0006-341X},
  doi = {10.2307/2534008},
  abstract = {In some survival analysis applications, the endpoint of interest has a degree of uncertainty associated with it. These events are typically classified by the investigator or by an endpoint committee as true or false according to some decision rule, and the analysis proceeds using only the true endpoints. This procedure has two drawbacks: The cutpoint for the decision rule is somewhat arbitrary, and the information contained in the level of certainty is lost. This paper introduces a modification of the Cox regression model that allows all potential endpoints to be included in the analysis along with the level of certainty of each. Simulation results show this procedure to considerably increase the power of the standard procedure in a wide range of situations.},
  keywords = {read},
  file = {/Users/amanderson/Zotero/storage/ARRIW8RB/Snapinn - 1998 - Survival Analysis with Uncertain Endpoints.pdf}
}

@misc{soetaert_rootsolve_2020,
  title = {Rootsolve: Nonlinear Root Finding, Equilibrium and Steady-State Analysis of Ordinary Differential Equations},
  shorttitle = {Rootsolve},
  author = {Soetaert, Karline and Hindmarsh, Alan C. and Eisenstat, S. C. and Moler, Cleve and Dongarra, Jack and Saad, Youcef},
  year = {2020},
  month = apr,
  abstract = {Routines to find the root of nonlinear functions, and to perform steady-state and equilibrium analysis of ordinary differential equations (ODE). Includes routines that: (1) generate gradient and jacobian matrices (full and banded), (2) find roots of non-linear equations by the 'Newton-Raphson' method, (3) estimate steady-state conditions of a system of (differential) equations in full, banded or sparse form, using the 'Newton-Raphson' method, or by dynamically running, (4) solve the steady-state conditions for uni-and multicomponent 1-D, 2-D, and 3-D partial differential equations, that have been converted to ordinary differential equations by numerical differencing (using the method-of-lines approach). Includes fortran code.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {DifferentialEquations,NumericalMathematics},
  note = {R package version 1.8.2.1}
}

@article{srivastava_scalable_2018,
  ids = {srivastava:li:dunson:18},
  title = {Scalable {{Bayes}} via {{Barycenter}} in {{Wasserstein}} Space},
  author = {Srivastava, Sanvesh and Li, Cheng and Dunson, David B.},
  year = {2018},
  journal = {The Journal of Machine Learning Research},
  volume = {19},
  number = {1},
  pages = {312--346},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  acmid = {3291133},
  keywords = {Statistics - Methodology},
  note = {Comment: 43 pages, 7 figures, and 11 tables. The updated revision will appear in JMLR},
  file = {/Users/amanderson/Zotero/storage/XTI27VQG/Srivastava et al. - 2018 - Scalable Bayes via Barycenter in Wasserstein Space.pdf}
}

@misc{stan_development_team_rstan_2021,
  title = {{{RStan}}: The {{R}} Interface to {{Stan}}},
  author = {{Stan Development Team}},
  year = {2021},
  annotation = {Note: R package version 2.26},
  note = {R package version 2.26}
}

@article{sweeting_estimating_2010,
  title = {Estimating the Distribution of the Window Period for Recent {{HIV}} Infections: {{A}} Comparison of Statistical Methods},
  shorttitle = {Estimating the Distribution of the Window Period for Recent {{HIV}} Infections},
  author = {Sweeting, Michael J. and Angelis, Daniela De and Parry, John and Suligoi, Barbara},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {30},
  pages = {3194--3202},
  issn = {1097-0258},
  doi = {10.1002/sim.3941},
  abstract = {In the past few years a number of antibody biomarkers have been developed to distinguish between recent and established Human Immunodeficiency Virus (HIV) infection. Typically, a specific threshold/cut-off of the biomarker is chosen, values below which are indicative of recent infections. Such biomarkers have attracted considerable interest as the basis for incidence estimation using a cross-sectional sample. An estimate of HIV incidence can be obtained from the prevalence of recent infection, as measured in the sample, and knowledge of the time spent in the recent infection state, known as the window period. However, such calculations are based on a number of assumptions concerning the distribution of the window period. We compare two statistical methods for estimating the mean and distribution of a window period using data on repeated measurements of an antibody biomarker from a cohort of HIV seroconverters. The methods account for the interval-censored nature of both the date of seroconversion and the date of crossing a specific threshold. We illustrate the methods using repeated measurements of the Avidity Index (AI) and make recommendations about the choice of threshold for this biomarker so that the resulting window period satisfies the assumptions for incidence estimation. Copyright \textcopyright{} 2010 John Wiley {$\&$} Sons, Ltd.},
  langid = {english},
  keywords = {biomarker,HIV incidence,mixed-effects models,window period},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3941},
  file = {/Users/amanderson/Zotero/storage/IJLWTMAS/Sweeting et al. - 2010 - Estimating the distribution of the window period f.pdf;/Users/amanderson/Zotero/storage/4NX2PUY2/sim.html}
}

@article{szczesniak_dynamic_2020,
  title = {Dynamic Predictive Probabilities to Monitor Rapid Cystic Fibrosis Disease Progression},
  author = {Szczesniak, Rhonda D. and Su, Weiji and Brokamp, Cole and Keogh, Ruth H. and Pestian, John P. and Seid, Michael and Diggle, Peter J. and Clancy, John P.},
  year = {2020},
  journal = {Statistics in Medicine},
  volume = {39},
  number = {6},
  pages = {740--756},
  issn = {1097-0258},
  doi = {10.1002/sim.8443},
  abstract = {Cystic fibrosis (CF) is a progressive, genetic disease characterized by frequent, prolonged drops in lung function. Accurately predicting rapid underlying lung-function decline is essential for clinical decision support and timely intervention. Determining whether an individual is experiencing a period of rapid decline is complicated due to its heterogeneous timing and extent, and error component of the measured lung function. We construct individualized predictive probabilities for ``nowcasting'' rapid decline. We assume each patient's true longitudinal lung function, S(t), follows a nonlinear, nonstationary stochastic process, and accommodate between-patient heterogeneity through random effects. Corresponding lung-function decline at time t is defined as the rate of change, S{${'}$}(t). We predict S{${'}$}(t) conditional on observed covariate and measurement history by modeling a measured lung function as a noisy version of S(t). The method is applied to data on 30 879 US CF Registry patients. Results are contrasted with a currently employed decision rule using single-center data on 212 individuals. Rapid decline is identified earlier using predictive probabilities than the center's currently employed decision rule (mean difference: 0.65 years; 95{$\%$} confidence interval (CI): 0.41, 0.89). We constructed a bootstrapping algorithm to obtain CIs for predictive probabilities. We illustrate real-time implementation with R Shiny. Predictive accuracy is investigated using empirical simulations, which suggest this approach more accurately detects peak decline, compared with a uniform threshold of rapid decline. Median area under the ROC curve estimates (Q1-Q3) were 0.817 (0.814-0.822) and 0.745 (0.741-0.747), respectively, implying reasonable accuracy for both. This article demonstrates how individualized rate of change estimates can be coupled with probabilistic predictive inference and implementation for a useful medical-monitoring approach.},
  copyright = {\textcopyright{} 2019 The Authors. Statistics in Medicine published by John Wiley {$\&$} Sons, Ltd.},
  langid = {english},
  keywords = {longitudinal data analysis,medical monitoring,nonstationary process,nowcasting,predictive probability distributions,read},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8443},
  file = {/Users/amanderson/Zotero/storage/W4GHB2MX/Szczesniak et al. - 2020 - Dynamic predictive probabilities to monitor rapid .pdf;/Users/amanderson/Zotero/storage/C9R8T7VL/sim.html}
}

@article{the_ards_definition_task_force_acute_2012,
  title = {Acute Respiratory Distress Syndrome: The {{Berlin}} Definition},
  shorttitle = {Acute Respiratory Distress Syndrome},
  author = {{The ARDS Definition Task Force}},
  year = {2012},
  month = jun,
  journal = {JAMA},
  volume = {307},
  number = {23},
  pages = {2526--2533},
  issn = {0098-7484},
  doi = {10.1001/jama.2012.5669},
  abstract = {The acute respiratory distress syndrome (ARDS) was defined in 1994 by the American-European Consensus Conference (AECC); since then, issues regarding the reliability and validity of this definition have emerged. Using a consensus process, a panel of experts convened in 2011 (an initiative of the European Society of Intensive Care Medicine endorsed by the American Thoracic Society and the Society of Critical Care Medicine) developed the Berlin Definition, focusing on feasibility, reliability, validity, and objective evaluation of its performance. A draft definition proposed 3 mutually exclusive categories of ARDS based on degree of hypoxemia: mild (200 mm Hg{$~\&$}lt;{$~$}PaO2/FIO2{$~\leq~$}300 mm Hg), moderate (100 mm Hg{$~\&$}lt;{$~$}PaO2/FIO2{$~\leq~$}200 mm Hg), and severe (PaO2/FIO2{$~\leq~$}100 mm Hg) and 4 ancillary variables for severe ARDS: radiographic severity, respiratory system compliance ({$\leq$}40 mL/cm H2O), positive end-expiratory pressure ({$\geq$}10 cm H2O), and corrected expired volume per minute ({$\geq$}10 L/min). The draft Berlin Definition was empirically evaluated using patient-level meta-analysis of 4188 patients with ARDS from 4 multicenter clinical data sets and 269 patients with ARDS from 3 single-center data sets containing physiologic information. The 4 ancillary variables did not contribute to the predictive validity of severe ARDS for mortality and were removed from the definition. Using the Berlin Definition, stages of mild, moderate, and severe ARDS were associated with increased mortality (27{$\%$}; 95{$\%$} CI, 24{$\%$}-30{$\%$}; 32{$\%$}; 95{$\%$} CI, 29{$\%$}-34{$\%$}; and 45{$\%$}; 95{$\%$} CI, 42{$\%$}-48{$\%$}, respectively; P{$~\&$}lt;{$~$}.001) and increased median duration of mechanical ventilation in survivors (5 days; interquartile [IQR], 2-11; 7 days; IQR, 4-14; and 9 days; IQR, 5-17, respectively; P{$~\&$}lt;{$~$}.001). Compared with the AECC definition, the final Berlin Definition had better predictive validity for mortality, with an area under the receiver operating curve of 0.577 (95{$\%$} CI, 0.561-0.593) vs 0.536 (95{$\%$} CI, 0.520-0.553; P{$~\&$}lt;{$~$}.001). This updated and revised Berlin Definition for ARDS addresses a number of the limitations of the AECC definition. The approach of combining consensus discussions with empirical evaluation may serve as a model to create more accurate, evidence-based, critical illness syndrome definitions and to better inform clinical care, research, and health services planning.},
  file = {/Users/amanderson/Zotero/storage/2EPLXUET/The ARDS Definition Task Force - 2012 - Acute Respiratory Distress Syndrome The Berlin De.pdf;/Users/amanderson/Zotero/storage/FMMDDZT2/1160659.html}
}

@article{tierney_markov_1994,
  title = {Markov Chains for Exploring Posterior Distributions},
  author = {Tierney, Luke},
  year = {1994},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {22},
  number = {4},
  issn = {0090-5364},
  doi = {10.1214/aos/1176325750},
  file = {/Users/amanderson/Zotero/storage/58JM3HRB/Tierney - 1994 - Markov Chains for Exploring Posterior Distribution.pdf;/Users/amanderson/Zotero/storage/LN2DIKSU/1176325750 (3).pdf}
}

@article{tom:etal:10,
  ids = {tom_reuse_2010},
  title = {Reuse, Recycle, Reweigh: {{Combating}} Influenza through Efficient Sequential {{Bayesian}} Computation for Massive Data},
  author = {Tom, Jennifer A. and Sinsheimer, Janet S. and Suchard, Marc A.},
  year = {2010},
  journal = {The Annals of Applied Statistics},
  volume = {4},
  number = {4},
  pages = {1722--1748},
  publisher = {{The Institute of Mathematical Statistics}},
  doi = {10.1214/10-AOAS349},
  file = {/Users/amanderson/Zotero/storage/E7WDEIUD/Tom et al. - 2010 - Reuse, recycle, reweigh Combating influenza throu.pdf}
}

@article{tsiatis_joint_2004,
  ids = {tsiatis_joint_nodate},
  title = {Joint Modeling of Longitudinal and Time-to-Event Data: An Overview},
  shorttitle = {Joint Modeling of Longitudinal and Time-to-Event Data},
  author = {Tsiatis, Anastasios A. and Davidian, Marie},
  year = {2004},
  journal = {Statistica Sinica},
  volume = {14},
  number = {3},
  pages = {809--834},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {A common objective in longitudinal studies is to characterize the relationship between a longitudinal response process and a time-to-event. Considerable recent interest has focused on so-called joint models, where models for the event time distribution and longitudinal data are taken to depend on a common set of latent random effects. In the literature, precise statement of the underlying assumptions typically made for these models has been rare. We review the rationale for and development of joint models, offer insight into the structure of the likelihood for model parameters that clarifies the nature of common assumptions, and describe and contrast some of our recent proposals for implementation and inference.},
  file = {/Users/amanderson/Zotero/storage/87WBFMGQ/Tsiatis and Davidian - Joint Modeling of Longitudinal and Time-to-Event D.pdf;/Users/amanderson/Zotero/storage/VB6SQE6Z/Tsiatis and Davidian - 2004 - JOINT MODELING OF LONGITUDINAL AND TIME-TO-EVENT D.pdf}
}

@article{vehtari_rank-normalization_2020,
  title = {Rank-Normalization, Folding, and Localization: An Improved $\widehat{R}$ for Assessing Convergence of {{MCMC}}},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2020},
  journal = {Bayesian Analysis},
  eprint = {1903.08008},
  eprinttype = {arxiv},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic R\textasciicircum R\textasciicircum{$<$}math alttext="{$\$\backslash$}widehat{$\lbrace$}R{$\rbrace\$$}" overflow="scroll"{$>$} {$<$}mover accent="true"{$>$} {$<$}mrow{$>$} {$<$}mi{$>$}R{$<$}/mi{$>$} {$<$}/mrow{$>$} {$<$}mrow{$>$} {$<$}mo{$>$}\textasciicircum{$<$}/mo{$>$} {$<$}/mrow{$>$} {$<$}/mover{$>$} {$<$}/math{$>$} of Gelman and Rubin (1992) has serious flaws. Traditional R\textasciicircum R\textasciicircum{$<$}math alttext="{$\$\backslash$}widehat{$\lbrace$}R{$\rbrace\$$}" overflow="scroll"{$>$} {$<$}mover accent="true"{$>$} {$<$}mrow{$>$} {$<$}mi{$>$}R{$<$}/mi{$>$} {$<$}/mrow{$>$} {$<$}mrow{$>$} {$<$}mo{$>$}\textasciicircum{$<$}/mo{$>$} {$<$}/mrow{$>$} {$<$}/mover{$>$} {$<$}/math{$>$} will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {broken-with-latex,read,Statistics - Computation,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/3F4IM86A/Vehtari et al. - 2020 - Rank-Normalization, Folding, and Localization An .pdf;/Users/amanderson/Zotero/storage/9Q2D3EQC/Vehtari et al. - 2020 - Rank-normalization, folding, and localization An .pdf;/Users/amanderson/Zotero/storage/KS95FEQ4/1593828229.html}
}

@article{wang_integrative_2020,
  title = {Integrative Survival Analysis with Uncertain Event Times in Application to a Suicide Risk Study},
  author = {Wang, Wenjie and Aseltine, Robert and Chen, Kun and Yan, Jun},
  year = {2020},
  month = mar,
  journal = {Annals of Applied Statistics},
  volume = {14},
  number = {1},
  pages = {51--73},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/19-AOAS1287},
  abstract = {The concept of integrating data from disparate sources to accelerate scientific discovery has generated tremendous excitement in many fields. The potential benefits from data integration, however, may be compromised by the uncertainty due to incomplete/imperfect record linkage. Motivated by a suicide risk study, we propose an approach for analyzing survival data with uncertain event times arising from data integration. Specifically, in our problem deaths identified from the hospital discharge records together with reported suicidal deaths determined by the Office of Medical Examiner may still not include all the death events of patients, and the missing deaths can be recovered from a complete database of death records. Since the hospital discharge data can only be linked to the death record data by matching basic patient characteristics, a patient with a censored death time from the first dataset could be linked to multiple potential event records in the second dataset. We develop an integrative Cox proportional hazards regression in which the uncertainty in the matched event times is modeled probabilistically. The estimation procedure combines the ideas of profile likelihood and the expectation conditional maximization algorithm (ECM). Simulation studies demonstrate that under realistic settings of imperfect data linkage the proposed method outperforms several competing approaches including multiple imputation. A marginal screening analysis using the proposed integrative Cox model is performed to identify risk factors associated with death following suicide-related hospitalization in Connecticut. The identified diagnostics codes are consistent with existing literature and provide several new insights on suicide risk, prediction and prevention.},
  langid = {english},
  mrnumber = {MR4085083},
  zmnumber = {07200161},
  keywords = {Cox model,data linkage,ECM algorithm,integrative learning,read,suicide prevention},
  file = {/Users/amanderson/Zotero/storage/I68DWIUP/Wang et al. - 2020 - Integrative survival analysis with uncertain event.pdf;/Users/amanderson/Zotero/storage/RFQAFZII/1587002664.html}
}

@article{wang_shape-restricted_2021-1,
  ids = {wang_shape-restricted_2021},
  title = {Shape-Restricted Regression Splines with {{R}} Package Splines2},
  author = {Wang, Wenjie and Yan, Jun},
  year = {2021},
  month = aug,
  journal = {Journal of Data Science},
  volume = {19},
  number = {3},
  pages = {498--517},
  publisher = {{School of Statistics, Renmin University of China}},
  issn = {1680-743X, 1683-8602},
  doi = {10.6339/21-JDS1020},
  abstract = {Splines are important tools for the flexible modeling of curves and surfaces in regression analyses. Functions for constructing spline basis functions are available in R through the base package splines. When the curves to be modeled have known characteristics in monotonicity or curvature, more efficient statistical inferences are possible with shape-restricted splines. Such splines, however, are not available in the R package splines. The package splines2 provides easy-to-use shape-restricted spline basis functions, along with their derivatives and integrals which are important tools in many inference scenarios. It also provides additional splines and features that are not available in the splines package, such as periodic splines and generalized Bernstein polynomials. The usages of the functions are illustrated with shape-restricted regression, recurrent event data analysis, and extreme-value copulas.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/5I3TTGHF/Wang and Yan - 2021 - Shape-Restricted Regression Splines with R Package.pdf;/Users/amanderson/Zotero/storage/PD2HHZQX/Wang and Yan - 2021 - Shape-Restricted Regression Splines with R Package.pdf;/Users/amanderson/Zotero/storage/QUPGCJLV/jds1020_s001 (1).pdf;/Users/amanderson/Zotero/storage/3BHM4YVF/info.html;/Users/amanderson/Zotero/storage/879Q29GN/info.html}
}

@misc{wang_splines2_2021,
  title = {Splines2: Regression Spline Functions and Classes},
  shorttitle = {Splines2},
  author = {Wang, Wenjie and Yan, Jun},
  year = {2021},
  month = apr,
  abstract = {Constructs basis matrix of B-splines, M-splines, I-splines, convex splines (C-splines), periodic M-splines, natural cubic splines, generalized Bernstein polynomials, and their integrals (except C-splines) and derivatives of given order by close-form recursive formulas. It also contains a C++ head-only library integrated with Rcpp. See De Boor (1978) {$<$}doi:10.1002/zamm.19800600129{$>$}, Ramsay (1988) {$<$}doi:10.1214/ss/1177012761{$>$}, and Meyer (2008) {$<$}doi:10.1214/08-AOAS167{$>$} for more information about the spline basis.},
  copyright = {GPL ({$\geq$} 3)},
  note = {R package version 0.4.3}
}

@article{wang_suicide_2020,
  title = {Suicide Risk Modeling with Uncertain Diagnostic Records},
  author = {Wang, Wenjie and Luo, Chongliang and Aseltine, Robert H. and Wang, Fei and Yan, Jun and Chen, Kun},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.02597 [stat]},
  eprint = {2009.02597},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Motivated by the pressing need for suicide prevention through improving behavioral healthcare, we use medical claims data to study the risk of subsequent suicide attempts for patients who were hospitalized due to suicide attempts and later discharged. Understanding the risk behaviors of such patients at elevated suicide risk is an important step towards the goal of "Zero Suicide". An immediate and unconventional challenge is that the identification of suicide attempts from medical claims contains substantial uncertainty: almost 20{$\backslash\%$} of "suspected" suicide attempts are identified from diagnostic codes indicating external causes of injury and poisoning with undermined intent. It is thus of great interest to learn which of these undetermined events are more likely actual suicide attempts and how to properly utilize them in survival analysis with severe censoring. To tackle these interrelated problems, we develop an integrative Cox cure model with regularization to perform survival regression with uncertain events and a latent cure fraction. We apply the proposed approach to study the risk of subsequent suicide attempt after suicide-related hospitalization for adolescent and young adult population, using medical claims data from Connecticut. The identified risk factors are highly interpretable; more intriguingly, our method distinguishes the risk factors that are most helpful in assessing either susceptibility or timing of subsequent attempt. The predicted statuses of the uncertain attempts are further investigated, leading to several new insights on suicide event identification.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/amanderson/Zotero/storage/R3I4ZADD/Wang et al. - 2020 - Suicide Risk Modeling with Uncertain Diagnostic Re.pdf;/Users/amanderson/Zotero/storage/XHAWDLMI/2009.html}
}

@article{ye_semiparametric_2008,
  title = {Semiparametric Modeling of Longitudinal Measurements and Time-to-Event Data\textendash a Two-Stage Regression Calibration Approach},
  author = {Ye, Wen and Lin, Xihong and Taylor, Jeremy M. G.},
  year = {2008},
  journal = {Biometrics},
  volume = {64},
  number = {4},
  pages = {1238--1246},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2007.00983.x},
  abstract = {In this article we investigate regression calibration methods to jointly model longitudinal and survival data using a semiparametric longitudinal model and a proportional hazards model. In the longitudinal model, a biomarker is assumed to follow a semiparametric mixed model where covariate effects are modeled parametrically and subject-specific time profiles are modeled nonparametrially using a population smoothing spline and subject-specific random stochastic processes. The Cox model is assumed for survival data by including both the current measure and the rate of change of the underlying longitudinal trajectories as covariates, as motivated by a prostate cancer study application. We develop a two-stage semiparametric regression calibration (RC) method. Two variations of the RC method are considered, risk set regression calibration and a computationally simpler ordinary regression calibration. Simulation results show that the two-stage RC approach performs well in practice and effectively corrects the bias from the naive method. We apply the proposed methods to the analysis of a dataset for evaluating the effects of the longitudinal biomarker PSA on the recurrence of prostate cancer.},
  copyright = {\textcopyright{} 2008, The International Biometric Society},
  langid = {english},
  keywords = {Joint modeling,Longitudinal data,Semiparametric mixed models,Smoothing splines,Survival analysis},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2007.00983.x},
  file = {/Users/amanderson/Zotero/storage/V6KU8AY6/Ye et al. - 2008 - Semiparametric modeling of longitudinal measuremen.pdf;/Users/amanderson/Zotero/storage/4VIWS4UR/j.1541-0420.2007.00983.html}
}

@article{yu_semi-parametric_2009,
  title = {A Semi-Parametric Threshold Regression Analysis of Sexually Transmitted Infections in Adolescent Women},
  author = {Yu, Zhangsheng and Tu, Wanzhu and Lee, Mei-Ling Ting},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {24},
  pages = {3029--3042},
  issn = {1097-0258},
  doi = {10.1002/sim.3686},
  abstract = {Time-to-event analysis of sexually transmitted infection data is often complicated by the existence of nonproportional hazards and nonlinear independent variable effects. Methods without the proportional hazards assumption, such as threshold regression models, have been successfully used in many applications. This paper seeks to extend the existing threshold regression models to accommodate the nonlinear independent variable effects. Specifically, we incorporated penalized and regression splines to the threshold regression models for added modeling flexibility. Cross validation methods were used for the selection of the number of knots and for the determination of smoothing parameters. Variance estimates were proposed for inference purposes. Simulation results showed that the proposed methods were able to achieve nonparametric function and parametric coefficient estimates that are close to their true values. Simulation also demonstrated satisfactory performance of variance estimates. Using the proposed methods, we analyzed time from sexual debut to the first infection with Chlamydia trachomatis infection in a group of young women. Analysis shows that the lifetime number of sexual partners has a nonlinear effect on the risk of C. trachomatis infection and the infection risks were differential by ethnicity and age of sexual debut. Copyright \textcopyright{} 2009 John Wiley {$\&$} Sons, Ltd.},
  langid = {english},
  keywords = {cross validation,first hitting time model,nonproportional hazard,penalized spline,regression spline,survival analysis},
  annotation = {{$\_$}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3686},
  file = {/Users/amanderson/Zotero/storage/RCMUQ97R/Yu et al. - 2009 - A semi-parametric threshold regression analysis of.pdf;/Users/amanderson/Zotero/storage/BR2V2MLX/sim.html}
}

@article{zipkin_synthesizing_2018,
  title = {Synthesizing Multiple Data Types for Biological Conservation Using Integrated Population Models},
  author = {Zipkin, Elise F. and Saunders, Sarah P.},
  year = {2018},
  month = jan,
  journal = {Biological Conservation},
  volume = {217},
  pages = {240--250},
  issn = {00063207},
  doi = {10.1016/j.biocon.2017.10.017},
  abstract = {Assessing the impacts of ongoing climate and anthropogenic-induced change on wildlife populations requires understanding species distributions and abundances across large spatial and temporal scales. For threatened or declining populations, collecting sufficient broad-scale data is challenging as sample sizes tend to be low because many such species are rare and/or elusive. As a result, demographic data are often piecemeal, leading to difficulties in determining causes of population changes and developing strategies to mitigate the effects of environmental stressors. Thus, the population dynamics of threatened species across spatio-temporal extents is typically inferred through incomplete, independent, local-scale studies. Emerging integrative modeling approaches, such as integrated population models (IPMs), combine multiple data types into a single analysis and provide a foundation for overcoming problems of sparse or fragmentary data. In this paper, we demonstrate how IPMs can be successfully implemented by synthesizing the elements, advantages, and novel insights of this modeling approach. We highlight the latest developments in IPMs that are explicitly relevant to the ecology and conservation of threatened species, including capabilities to quantify the spatial scale of management, sourcesink dynamics, synchrony within metapopulations, and population density effects on demographic rates. Adoption of IPMs has led to improved detection of population declines, adaptation of targeted monitoring schemes, and refined management strategies. Continued methodological advancements of IPMs, such as incorporation of a wider set of data types (e.g., citizen science data) and coupled population-environment models, will allow for broader applicability within ecological and conservation sciences.},
  langid = {english},
  file = {/Users/amanderson/Zotero/storage/KXSQPIPH/Zipkin and Saunders - 2018 - Synthesizing multiple data types for biological co.pdf}
}



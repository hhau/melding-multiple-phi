<!-- This needs to go in the introduction of the thesis -->
- Note that for the acceptance probabilities to be interpretable as probabilities, they must be bounded between zero and 1. 
- To avoid clutter in the mathematics, we do not explicitly write $\alpha(\phi^{*}, \phi) = \text{min}\left(1, \frac{\pd (\phi^{*} \mid Y)}{\pd(\phi \mid Y)}\right)$ for all acceptance probability calculations.
- This never matters in practice, as acceptance probabilities are only ever compared with samples from a $\text{U}(0, 1)$ random variable, and hence if they happen to exceed 1, they were to be accepted anyway.

# Literature review + analysis 

- Massa papers (Again)
    - See if they have a good graph theory based notation
    - Think in terms of adjacency matrices / adjacencies
- Data fusion? they have some nice words for this: (data|information) (fusion|combination|integration)
- Network meta analysis
- https://arxiv.org/pdf/1810.05575.pdf ? - too specific / technical
- https://dl.acm.org/citation.cfm?id=1512927
    - "Target" here has an interesting meaning
- Probability propagation / belief propagation
    - https://link.springer.com/article/10.1007/BF01531015
    - http://www.stats.ox.ac.uk/~steffen/teaching/gm09/propag.pdf

- Ecologists really want to do this kind of thing:
    - https://esajournals.onlinelibrary.wiley.com/doi/toc/10.1002/(ISSN)1939-9170.SF-Data-Integration
    - https://www.stat.colostate.edu/~hooten/papers/pdf/Williams_etal_Ecology_2017.pdf
        - Too hard with the PDE term
    - https://www.stat.colostate.edu/~hooten/papers/pdf/Pepin_etal_EcolLetters_2017.pdf
        - Maybe? Meld over $\phi = (\theta, \sigma^2)$ if the DAG is to be believed
        - This is a good melding example, but doesn't help with the multiple phi thing
        - Might still be useful?
    - https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecy.2710
        - Model integration seems to be taking off
    - https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecy.2711
        - this has 5 data sources, but looks quite complicated, it would be real neat if we could do this properly? (nah, relies on some proprietary software)
    - https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecy.2714
        - Could be split into three models? Little different than I was thinking
        - Has data and BUGS code
    - https://link.springer.com/chapter/10.1007/978-1-4939-0977-3_9
    - https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/11-1881.1
    - https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0052.1
        - This is an interesting critique of one specific, complex model. It is somewhat dated (conjugate analyses are less prevalent nowadays) but it still rightly critiques complex models that do not have a grounded research question in mind.
    - https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.1710
        - yet another use of the term _data fusion_

## Integrated population models

There is a huge number of examples in Ecology of _integrated population models_.
_Integrated_, in that they integrate more than one source of data and an associated model, with the overall focus being on estimating the dynamics of a _population_ (birth, death, reproduction, and migration rates) and temporal abundance (how many of them are there at a point in time.)

- https://projecteuclid.org/euclid.ba/1551949261
    - If we decide to go to BA?

- https://www.sciencedirect.com/science/article/pii/S0006320717305141?via%3Dihub
    - Figure 1 in here is _exactly_ what I was thinking of in Section \ref{arbitrary-graphs-of-models}
        - The process they describe is:
            1. Specify submodels for the data sets (ensuring they have some parameters in common)
            2. Form the joint model by multiplying the individual submodels together
            3. Estimate the join model 
        - This is in effect, melding for multiple parameters using product of experts for the pooled prior.
        - The table gives us a bunch of examples to go and look at?
    - The paper Figure 1 is based off: https://link.springer.com/article/10.1007%2Fs10336-010-0632-7
        - _Sixth, the joint likelihood of integrated population models might be developed in such a way that it explicitly accounts for the dependence among datasets, such that the assumption of independence can be relaxed._
            -  I don't think Melding specifically does this (just makes the inter model dependence explicit)
        - _Seventh, the integrated population models could be modified in such a way that integral (i.e. continuous) population models (Ellner and Rees 2006) instead of stage- or age structured (i.e. categorical) population models are used to define the link between population size and demography. This would allow the quantifying of the impact of individual continuous traits on population dynamics._ 
            - Melding can do this, but their gripe seems to be applications specific
        - _Finally, another avenue is to explore additional data types that contain demographic information and that may be linked to a population model._
            - This is application specific.
- https://onlinelibrary.wiley.com/doi/epdf/10.1111/oik.01924
    - Another very similar example
    - The fit this jointly, and note that convergence is no good for some quantities, they also note an imbalance in information quantity between data sets.
    - BUGS code: http://www.oikosjournal.org/sites/oikosjournal.org/files/appendix/oik-01924.pdf - somewhat complex.
- https://link.springer.com/content/pdf/10.1007/s13253-018-00349-9.pdf 
    - this is a statisical look at estimating IPMs using SMC, dealing with the fact that they often have latent discrete parameters
    - `4.3. IMPROVING SMC EFFICIENCY FOR INTEGRATED MODELS` is effectively an SMC version of the melding multi-stage sampler, but with tempering
    - Also a good source of examples, as they are 'pre-translated' into statistical language

- https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13110
    - Another recent, review on computational methods in integrated population models 
    - It is interesting that they see the advantage of integrated modelling as being able to better design experiments? (as well as consider more sources of information)

- https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-985X.2008.00582.x

- https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.13538
    - This example seems simpler, but can't get at code + data?

<!-- This should probably be in the introduction not the examples -->
- _Q_: What are IPMs and why do ecologists use them?
- Integrated population models (IPMs) allow ecologists to combine data on a population's abundance (how many), trajectory (birth / death / immigration / emigration rates), demography (juvenile / adult / sex), and spatial location (where).
- The combination of these data allows ecologist to infer quantities that would be unidentifiable using a single source of data.
- Additionally, IPMs make the model specification task is easier; data are modelled separately from one another, and the submodels are then combined to form the IPM.
- This process is equivalent to Markov melding using product-of-experts to form the pooled prior.
 


## More thoughts on examples

- Seems to be a popular idea in Ecology, so useful as examples, but will not necessarily have the greatest impact (they are already aware they can do such a thing).
- Why reframe these models in this particular framework?
    - There is value in unifying disparate modelling frameworks. Different applied fields seem to do similar things but with different names. Advantages, practical tips, and patterns of thought are more easily shared with a common mathematical framework.
        - Justifying this requires an example from another field.
    - Having a mathematical framework may also make it easier to spot sensible extensions to models
        - Really? Maybe writing down submodels in a common notation makes it easier to spot where additional information could be incorporated?
    - (_somewhat specific to IPMs_): IPMs are currently limited to models where interacting with the joint distribution directly is possible; the sequential sampler is designed to circumvent this restriction, i.e. to enable inference in settings where evaluating the joint distribution is infeasible.
        - As the authors of one particular review article note, many ecologists are technically inclined to implement their own IPMs (although they can be done in BUGS?), tools making this easier would be valuable.
        - Finke/King/Beskos/Dellaportas SMC example still requires evaluating the joint.
            - It might be sensible to steal then notation from here


# Melded model





# The pooled prior and pooling weights



## General case

The logarithmic pooled prior for $\Nm$ models in a chain is
\input{tex-input/multiple-phi/0051-pooled-prior-overall-general.tex}

\newpage

# Model ordering and commutativity 


1. _Is this any different from melding $\pd_{1}(\phi_{1 \cap 2}, \psi_{1}, Y_{1})$ and $\pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3}, \psi_{2}, Y_{2})$ together first, then melding the result with $\pd_{3}(\phi_{2 \cap 3}, \psi_{3}, Y_{3})$?_
    1. If it is different, why? When are they the same, and when not?
        - @massa:lauritzen:10 note that in the case of Markov combination, this is not always true.
    1. The previous two questions assume that melding $\pd_{1}$ with $\pd_{2}$ first, then the result with $\pd_{3}$ is commutative, i.e. melding $\pd_{2}$ with $\pd_{3}$ first, then the result with $\pd_{1}$ should be identical.
        - Ignoring practical reasons why this may not be the case.
2. _Does applying this operator to 2 $\Nm = 3$ chains of models (base case) result in the same thing as applying it to $\Nm = 6$ models at once?_
    - This will depend on the relationship between the two groups of models? Assume it's a chain and check?
    - We can probably just generalise the result from (1.) / the original melding operator?
    - If we distinguish the new operator from the old one, then we should verify this.

## Applying the original Markov Melding operator twice

### Definitions

- Denote the original melding operator with $\circledast$, and its output  
\input{tex-input/noncommutativity/0005-def-usual-melded-model.tex}
where $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2}) = g^{12}(\pd_{1}(\phi_{1 \cap 2}), \pd_{2}(\phi_{1 \cap 2}))$ for some pooling function $g^{12}$.
- Denote the entire parameter space of $\pd_{\text{meld}}^{12}$ as $\Theta^{12} = (\phi_{1 \cap 2}, \phi_{2 \cap 3}, \psi_{1}, \psi_{2}, Y_{1}, Y_{2})$ .
- Any marginal distribution of $\pd_{\text{meld}}^{12}$ can be derived in the usual way, for example 
\input{tex-input/noncommutativity/0006-example-melded-marginal-definition.tex}
- Define $\pd_{\text{meld}}^{(12)3}$ as 
\input{tex-input/noncommutativity/0007-iterated-application-melding.tex}
so that the parentheses in the superscript indicate the order in which the melding operator is applied.
- Analogously, define $\pd_{\text{pool}}^{(12)3}(\phi_{2 \cap 3}) = g^{(12)3}(\pd_{\text{meld}}^{12}(\phi_{2 \cap 3}), \pd_{3}(\phi_{2 \cap 3}))$ for a potentially different choice of pooling function $g^{(12)3}$.

### $(\pd_{1} \circledast \pd_{2}) \circledast \pd_{3}$: Step one

In step one, $\pd_{1} \circledast \pd_{2}$ produces the usual melded model:
\input{tex-input/noncommutativity/0010-usual-melded-model.tex}
In the $\Nm = 2$ context the second model would be expressed in terms of $\psi_{2}' = (\psi_{2}, \phi_{2 \cap 3})$. 
We are intentionally keeping these quantities distinct, to clarify the next application of the original melding operator. 

### $(\pd_{1} \circledast \pd_{2}) \circledast \pd_{3}$: Step two

Next we compute $\pd_{\text{meld}}^{(12)3} = \pd_{\text{meld}}^{12} \circledast \pd_{3}$:
\input{tex-input/noncommutativity/0011-double-melded-model.tex}
with $\pd_{\text{pool}}^{(12)3}(\phi_{2 \cap 3}) = g^{(12)3}(\pd_{\text{meld}}^{12}(\phi_{2 \cap 3}), \pd_{3}(\phi_{2 \cap 3}))$.
The difficult term here is $\pd_{\text{meld}}^{12}(\phi_{2 \cap 3})$. Define $\Omega = (\phi_{1 \cap 2}, \phi_{2 \cap 3}, \psi_{1}, \psi_{2}, Y_{1}, Y_{2})$, then
\input{tex-input/noncommutativity/0012-strange-marginal-definition.tex}
In words, this is the marginal distribution for $\phi_{2 \cap 3}$ under the melded model $\pd_{1} \circledast \pd_{2}$.
Expanding Equation \eqref{eqn:double-melded-model} gives:
\input{tex-input/noncommutativity/0013-expanded-double-melded-model.tex}

### Is this the same as the proposed operator?

For this to be equal to the model defined in Equation \eqref{eqn:melded-model} (ignoring all normalising constants for clarity), both the following equalities must hold:

1. $\pd_{2}(\phi_{1 \cap 2}) \pd_{\text{meld}}^{12}(\phi_{2 \cap 3}) = \pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3})$.
    - This can only be true if $\phi_{1 \cap 2}$ and $\phi_{2 \cap 3}$ are independent a priori in $\pd_{2}$.
    - If they are independent then $\pd_{2}(\phi_{1 \cap 2}, \phi_{2 \cap 3}) = \pd_{2}(\phi_{1 \cap 2}) \pd_{2}(\phi_{2 \cap 3})$, and $\pd_{\text{meld}}^{12}(\phi_{2 \cap 3}) = \pd_{2}(\phi_{2 \cap 3})$ for the equality to hold.
        - This is true when $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2}) = \pd_{2}(\phi_{1 \cap 2})$, which is dictatorial pooling.
        - To verify:
        \input{tex-input/noncommutativity/0015-verify-dictatorial-pooling.tex}
        thus $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2}) = \pd_{2}(\phi_{1 \cap 2})$ in order for final integral to produce the desired marginal: $\pd_{2}(\phi_{2 \cap 3})$.

2. $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2}) \pd_{\text{pool}}^{(12)3}(\phi_{2 \cap 3}) = \pd_{\text{pool}}(\phi_{1 \cap 2}, \phi_{2 \cap 3})$
    - All the $g$'s (pooling functions) must be logarithmic if we form the joint pooled prior (RHS) via logarithmic pooling
        - Even then, we need independence in the second model for this to be true.
    - _On term matching_: If we form the joint pooled prior via the strange form of linear pooling we define, then the joint has 4 terms. 
    - As for the left hand side:
        - $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2})$ has 1 term (logarithmic) or 2 terms (linear).
        - If $\pd_{\text{pool}}^{(12)3}(\phi_{2 \cap 3})$ uses our form of logarithmic pooling, then it has 1 or 2 terms if $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2})$ is logarithmic or linear respectively (due to the interaction with $\pd_{\text{meld}}^{12}(\phi_{2 \cap 3})$).
        - Alternatively, if $\pd_{\text{pool}}^{(12)3}(\phi_{2 \cap 3})$ uses our form of linear pooling, then it has 4 or 8 terms (using the same reasoning).
        - So if we use linear pooling to form the joint pooled prior (RHS), then we must use logarithmic pooling for $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2})$ and linear for $\pd_{\text{pool}}^{(12)3}(\phi_{2 \cap 3})$ to result in the correct number of terms. This alone will not guarantee that the quantities are equal, but at least it may be possible.

So in general, applying the original melding operator twice does not result in the same model as \eqref{eqn:melded-model}.

### Is the original commutative? Does $(\pd_{1} \circledast \pd_{2}) \circledast \pd_{3} = \pd_{1} \circledast (\pd_{2} \circledast \pd_{3})$?

By carefully considering the indices in Equation \eqref{eqn:expanded-double-melded-model}, we find that the original melding operator is only commutative if 
\input{tex-input/noncommutativity/0016-commutativity-condition.tex}
which implies the following equalities
\input{tex-input/noncommutativity/0014-orig-melding-commutative-equalities.tex}
Showing one of the equalities in Equation \eqref{eqn:orig-melding-commutative-equalities-1} and \eqref{eqn:orig-melding-commutative-equalities-2} implies its partner equality is also true. 
Consider the first equality
\input{tex-input/noncommutativity/0017-pooling-equality.tex}
Assume that $g^{12}$ and $g^{1(23)}$ are both linear or logarithmic pooling functions.
For Equation \eqref{eqn:pooling-equality} to be true, $\pd_{2}(\phi_{1 \cap 2}) = \pd_{\text{meld}}^{23}(\phi_{1 \cap 2})$, which is the same result we require in Equation \eqref{eqn:orig-melding-commutative-equalities-2}.
We have already shown that this is only true when using dictatorial pooling.

### Conditional commutativity

- _Question_: Can we show that the original operator has a weaker form of commutativity if there exists a part of $\pd_{2}$, which we can temporarily call $A$, that renders $\phi_{1 \cap 2} \indep \phi_{2 \cap 3} \mid A$?
- _TODO_: Need to make explicit how this argument goes through the pooled prior terms in Equation \eqref{eqn:expanded-double-melded-model}
    - I think that Section \ref{is-the-original-commutative-does-pd_1-circledast-pd_2-circledast-pd_3-pd_1-circledast-pd_2-circledast-pd_3} shows that this is really the same question.

Assume there exists an $A \subset \psi_{2}$, such that $\phi_{1 \cap 2} \indep \phi_{2 \cap 3} \mid A$, and $A' = \psi_{2} \setminus{A}$. 
    <!-- - $A$ is sometimes known as the _d-separator_ of $\phi_{1 \cap 2}$ and $\phi_{2 \cap 3}$. -->
The original melding operator is conditionally commutative, which is to say $(\pd_{1}(\cdot) \circledast \pd_{2}(\cdot \mid A)) \circledast \pd_{3}(\cdot) = \pd_{1}(\cdot) \circledast (\pd_{2}(\cdot \mid A) \circledast \pd_{3}(\cdot))$.
To show this, it suffices to to show that 
\input{tex-input/noncommutativity/00201-conditional-commutativity-target-equality.tex} 
Symmetry in the indices implies that it is sufficient to show that $\pd_{\text{meld}}^{12}(\phi_{2 \cap 3} \mid A) = \pd_{2}(\phi_{2 \cap 3} \mid A)$
\input{tex-input/noncommutativity/0020-conditional-commutativity.tex}
Noting that $\int \pd_{1}(\phi_{1 \cap 2}, \psi_{1}, Y_{1}) \text{d}Y_{1} = \pd_{1}(\phi_{1 \cap 2}, \psi_{1})$ by definition, we can immediately integrate out $Y_{1}, \psi_{1}, Y_{2}$, and $A'$, leaving
\input{tex-input/noncommutativity/0021-conditional-commutativity-two.tex}
where we use the conditional independence property to pull $\pd_{2}(\phi_{2 \cap 3} \mid A)$ out of the integral, and the last step assumes that $\pd_{\text{pool}}^{12}(\phi_{1 \cap 2} \mid A)$ is normalised.

### Summary

1. Original operator targets something different from the proposed operator.
1. Original operator is generally noncommutative.

Exceptions include: 

- If $\phi_{1 \cap 2} \indep \phi_{2 \cap 3}$ in $\pd_{2}$, then both operators target the same thing, and the original operator is commutative.
- If we use dictatorial pooling, i.e. set $\pd_{\text{pool}}^{23}(\phi_{1 \cap 2}) = \pd_{2}(\phi_{1 \cap 2})$ and/or $\pd_{\text{pool}}^{12}(\phi_{2 \cap 3}) = \pd_{2}(\phi_{2 \cap 3})$, then the original operator is commutative, but still targets something different from the proposed operator.
- If there exists an $A \subset \psi_{2}$ such that $\phi_{1 \cap 2} \indep \phi_{2 \cap 3} \mid A$, then the original operator has a weaker form of commutativity $(\pd_{1}(\cdot) \circledast \pd_{2}(\cdot \mid A)) \circledast \pd_{3}(\cdot) = \pd_{1}(\cdot) \circledast (\pd_{2}(\cdot \mid A) \circledast \pd_{3}(\cdot))$.

#### Example {-}

\input{tex-input/noncommutativity/0030-example-cond-comm.tex}

# Multi-stage sampler 1: starting with $\modelindex = 1$

Beginning with the general case, the overall target is
\input{tex-input/multi-stage-sampler/0010-melded-posterior.tex}

## Stage one

Target
\input{tex-input/multi-stage-sampler/0020-stage-one-target.tex}
This is targeted with a single MCMC proposal $\q(\phi_{1 \cap 2}^{*}, \psi_{1}^{*} \mid \phi_{1 \cap 2}, \psi_{1})$, leading to an acceptance probability of
\input{tex-input/multi-stage-sampler/0021-stage-one-acceptance-probability.tex}


## Stage two

Target
\input{tex-input/multi-stage-sampler/0030-stage-two-target.tex}
using Gibbs updates (_Should Gibbs updates use the asterisk?_)
\input{tex-input/multi-stage-sampler/0031-stage-two-gibbs-updates.tex}
These updates have acceptance probabilities 
\input{tex-input/multi-stage-sampler/0032-stage-two-acceptance-probabilities.tex}

Formally, one can update the $\psi_{1}$ samples from stage one by keeping track of the indices[^indices].

[^indices]: I need to formally think about and define the indices, we are going to refine / resample them in strange ways over many steps.

## Stage $\Nm - 1$

Target
\input{tex-input/multi-stage-sampler/0040-stage-Mminus1-target.tex}
using Gibbs updates
\input{tex-input/multi-stage-sampler/0041-stage-Mminus1-gibbs-updates.tex}
results in acceptance probabilities of
\input{tex-input/multi-stage-sampler/0042-stage-Mminus1-acceptance-probabilities.tex}

## Stage $\Nm$

- Include the $\Nm$th model, which we already have samples of $\phi_{\Nm - 1}$ for.

Target
\input{tex-input/multi-stage-sampler/0050-stage-M-target.tex}
with Gibbs updates
\input{tex-input/multi-stage-sampler/0051-stage-M-gibbs-updates.tex}
with acceptance probabilities
\input{tex-input/multi-stage-sampler/0052-stage-M-acceptance-probabilities.tex}

## Stage $\Nm + 1$

Target
\input{tex-input/multi-stage-sampler/0060-stage-Mplus1-target.tex}
with a one step update
\input{tex-input/multi-stage-sampler/0061-stage-Mplus1-update.tex}
which has an acceptance probability of
\input{tex-input/multi-stage-sampler/0062-stage-Mplus1-acceptance-probability.tex}

- This could be combined with stage $\Nm$, but pedagogically it is nice to have it as a separate stage.

\newpage

# Multi-stage sampler 2: Split and recombine

## Meet in the middle

_Rewrite for $\Nm = 3$ models, pick a consistent name_.

\input{tex-input/dc-sampler/0000-tikz-target.tex}

Choose a specific model $k$.
Recursively use this split and recombine method, or the aforementioned multi-stage sampler, to generate samples from the first set of models $\textcolor{mymidblue}{A} = \{1, \ldots, k - 1\}$, with melded joint distribution 
    $$\pd_{\text{meld}, \textcolor{mymidblue}{A}}(\phi_{1 \cap 2}, \ldots, \phi_{k - 1 \cap k}, \psi_{1}, \ldots, \psi_{k - 1}, Y_{1}, \ldots, Y_{k - 1})$$ 
and the second set $\textcolor{myredhighlight}{B} = \{k + 1, \ldots, \Nm\}$, with melded joint distribution 
    $$\pd_{\text{meld}, \textcolor{myredhighlight}{B}}(\phi_{k \cap k + 1}, \ldots, \phi_{\Nm - 1 \cap \Nm}, \psi_{k + 1}, \ldots, \psi_{\Nm}, Y_{k + 1}, \ldots, Y_{\Nm}).$$

The last step of the multi-stage sampler then targets the overall melded posterior
\input{tex-input/dc-sampler/0010-dc-final-target.tex}
with Gibbs updates
\input{tex-input/dc-sampler/0011-dc-gibbs-updates.tex}
and acceptance probabilities
\input{tex-input/dc-sampler/0012-dc-acceptance-probabilities.tex}

# Examples

## Integrated population model -  Little Owls





### Submodels





### Sampling

<!-- Make this explicit,  -->



### Results and discussion





\newpage

# Arbitrary graphs of models

- Notation: Choose model $\pd_{a}$ for initial target?
- The all models connected to $a$ are $b_{i}$ for $i = 1, \ldots, N_{b}$?
    - This quickly breaks down if things reconnect? Also what would the next layer be called $c_{i, j}$? for $j = 1, \ldots, N_{j, b}$?
- Lets look an an example DAG, we can also introduce the arrow notation for noninvertible link functions, Figure \ref{fig:example-model-graph}.

\input{tex-input/arbitrary-graphs/0010-example-model-graph.tex}

- We can come up with a sampling strategy for this, partially determined by the arrows
    - $s_{1} = \{\pd_{4}\}, s_{1}' = \{\pd_{2}\}$
        - These can occur in parallel
    - $s_{2} = \{\pd_{3}, \pd_{4}\}$
        - This can occur whilst $s_{1}$ is ongoing.
    - $s_{3} = \{\pd_{1}, \pd_{2}, \pd_{3}, \pd_{4}\}$
    - $s_{4} = \{\pd_{1}, \pd_{2}, \pd_{3}, \pd_{4}, \pd_{5}\}$

- We should the redraw the DAG such that things that could be done in parallel are on an equal level.

\input{tex-input/arbitrary-graphs/0011-example-model-graph.tex}


# Conclusion

- There are a lot of unknown self-density ratios, including many $\pd_{\modelindex}(\phi_{\modelindex - 1, \text{nu}}, \phi_{\modelindex, \text{nu}}) \mathop{/} \pd_{\modelindex}(\phi_{\modelindex - 1, \text{de}}, \phi_{\modelindex, \text{de}})$ terms.
    - In the case where $\phi_{\modelindex - 1}$ and $\phi_{\modelindex}$ are both one dimensional, this is already a four-dimensional problem.
    - We can reframe this as an advantage, as estimating $\pd_{\text{pool}}(\phi_{1 \cap 2}, \ldots,\phi_{\Nm - 1})$ is potentially a series of lower dimensional estimation problems, which is easier than one estimation problem in a larger dimension?



